{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip3 install transformers zstandard jsonlines peft wandb bitsandbytes lion-pytorch -q\n","!pip3 install accelerate datasets sentencepiece langchain torch_xla[tpuvm] -q\n","!pip uninstall tensorflow -y #that's the meme part"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["get_ipython().kernel.do_shutdown(True)\n","### for good measures restart kernel"]},{"cell_type":"markdown","metadata":{},"source":["**Tokens?**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!huggingface-cli login --token <hf_read_token> #for downloading gated models\n","# import wandb\n","# wandb.login()"]},{"cell_type":"markdown","metadata":{},"source":["**Sharding Module for different Architechture**"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:43:54.450080Z","iopub.status.busy":"2024-02-29T08:43:54.449856Z","iopub.status.idle":"2024-02-29T08:43:54.461349Z","shell.execute_reply":"2024-02-29T08:43:54.460731Z","shell.execute_reply.started":"2024-02-29T08:43:54.450051Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting spmd_util.py\n"]}],"source":["%%writefile spmd_util.py\n","import torch\n","import torch.nn as nn\n","import re\n","import torch_xla.experimental.xla_sharding as xs\n","import torch_xla.core.xla_model as xm\n","from transformers import (\n","    GPTNeoXConfig, T5Config, LlamaConfig, GPT2Config, MistralConfig, Qwen2Config, MixtralConfig, PhiConfig,GemmaConfig\n",")\n","\n","# ends with $ to prevent sharding lora parameters\n","\n","\n","T5_RULES = (\n","    # embeddings\n","    (\"shared$\", (\"mp\", \"fsdp\")),\n","    (\"embed_tokens$\", (\"mp\", \"fsdp\")),\n","    \n","    # attention\n","    (\"q$\", (\"fsdp\", \"mp\")),\n","    (\"k$\", (\"fsdp\", \"mp\")),\n","    (\"v$\", (\"fsdp\", \"mp\")),\n","    (\"o$\", (\"mp\", \"fsdp\")),\n","\n","    # mlp\n","    (\"w$\", (\"fsdp\", \"mp\")),\n","    (\"wi_0$\", (\"fsdp\", \"mp\")),\n","    (\"wi_1$\", (\"fsdp\", \"mp\")),\n","    (\"wo$\", (\"mp\", \"fsdp\")),\n","\n","    # seq2seq lm head\n","    (\"lm_head\", (\"fsdp\", \"mp\")),\n",")\n","\n","QWEN_RULES = (\n","    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n","    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n","    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n","    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n","    (\"lm_head\", (\"fsdp\", \"mp\")),\n","    )\n","GPT2_RULES = (\n","    # embeddings\n","    (\"wte\", (\"mp\", \"fsdp\")), \n","    (\"wpe\", (\"mp\", \"fsdp\")),\n","    \n","    # attention\n","    (\"c_attn\", (\"fsdp\", \"mp\")),\n","    (\"c_proj\", (\"mp\", \"fsdp\")),\n","    \n","    # mlp\n","    (\"c_fc\", (\"fsdp\", \"mp\")), \n","    (\"c_proj\", (\"mp\", \"fsdp\")),\n","    \n","    # output \n","    (\"ln_f\", (\"fsdp\", \"mp\")),\n",")\n","MISTRAL_RULES = (\n","    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n","    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n","    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n","    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n","    (\"lm_head\", (\"fsdp\", \"mp\")),\n","    )\n","\n","\n","PHI_RULES = (\n","    ### (regex) linear modules, (list[sharding methods]) )\n","    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n","    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n","    (\"self_attn\\\\.dense\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.fc2\", (\"mp\", \"fsdp\")),  \n","    (\"mlp\\\\.fc1\", (\"fsdp\", \"mp\")),\n","    (\"lm_head\", (\"fsdp\", \"mp\")),\n","    \n",")\n","\n","LLAMA_RULES = (\n","    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n","    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n","    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n","    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n","    (\"lm_head\", (\"fsdp\", \"mp\")),\n","    )\n","\n","GPTNEOX_RULES = (\n","    # embeddings\n","    (\"gpt_neox\\\\.embed_in\", (\"mp\", \"fsdp\")),\n","    # atention\n","    (\"attention\\\\.query_key_value$\", (\"fsdp\", \"mp\")),\n","    (\"attention\\\\.dense$\", (\"mp\", \"fsdp\")),\n","    # mlp\n","    (\"mlp\\\\.dense_h_to_4h$\", (\"fsdp\", \"mp\")),\n","    (\"mlp\\\\.dense_4h_to_h$\", (\"mp\", \"fsdp\")),\n","    # output\n","    (\"embed_out\", (\"fsdp\", \"mp\")),\n",")\n","\n","\n","\n","MIXTRAL_RULES = (\n","    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n","    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n","    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n","    (\"w1\", (\"fsdp\", \"mp\")),\n","    (\"w2\", (\"mp\", \"fsdp\")),\n","    (\"w3\", (\"fsdp\", \"mp\")),\n","    (\"gate\", (\"mp\", \"fsdp\")),\n","    (\"lm_head\", (\"fsdp\", \"mp\")),\n","    )\n","\n","GEMMA_RULES = (\n","    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n","    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n","    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n","    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n","    (\"lm_head\", (\"fsdp\", \"mp\")),\n",")\n","    \n","ALL_RULES = [\n","    (GPTNeoXConfig, GPTNEOX_RULES),\n","    (T5Config, T5_RULES),\n","    (LlamaConfig, LLAMA_RULES),\n","    (GPT2Config, GPT2_RULES),\n","    (MistralConfig, MISTRAL_RULES),\n","    (Qwen2Config, QWEN_RULES),\n","    (MixtralConfig, MIXTRAL_RULES),\n","    (PhiConfig,PHI_RULES),\n","    (GemmaConfig,GEMMA_RULES),\n","]\n","\n","def find_rule(model):\n","    for config, rule in ALL_RULES:\n","        if model.config.__class__ == config:\n","            return rule\n","    raise Exception(\"unsupported model to partitioning\")\n","\n","strkey2id = {\n","    \"dp\": 0,\n","    \"fsdp\": 1,\n","    \"mp\": 2\n","}\n","\n","def partition_module(model, mesh, device=xm.xla_device(), verbose=False):\n","    partition_specs = find_rule(model)\n","    rule = [(k, tuple([strkey2id[x] for x in v])) for k, v in partition_specs]\n","        \n","    # print(rule)\n","\n","    for name, module in model.named_modules():\n","        module.to(device)\n","        # print(name, module.__class__.__name__)\n","        if isinstance(module, (nn.Embedding, nn.Linear)):\n","            for rule_pattern, spec in rule:\n","                if re.findall(rule_pattern, name):\n","                    if verbose:\n","                        print(\"match\", rule_pattern, name)\n","                    \n","                    xs.mark_sharding(module.weight, mesh, spec)\n","                    break\n","        \n","def partition_module_dp(model, mesh, device=xm.xla_device(), verbose=True):\n","    spec = (1, 2)\n","\n","    for name, module in model.named_modules():\n","        module.to(device)\n","        if isinstance(module, (nn.Embedding, nn.Linear)):\n","            xs.mark_sharding(module.weight, mesh, spec)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["**Required Libs**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:04.153806Z","iopub.status.busy":"2024-02-29T08:44:04.153497Z","iopub.status.idle":"2024-02-29T08:44:09.108646Z","shell.execute_reply":"2024-02-29T08:44:09.107014Z","shell.execute_reply.started":"2024-02-29T08:44:04.153780Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import datasets\n","import torch.optim as optim\n","import torch_xla.debug.profiler as xp\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\n","import torch_xla.distributed.parallel_loader as pl\n","import torch_xla.test.test_utils as test_utils\n","import torch\n","import torch.nn as nn\n","import re\n","import torch_xla.experimental.xla_sharding as xs\n","import torch_xla.core.xla_model as xm\n","from transformers import (\n","    GPTNeoXConfig, T5Config, LlamaConfig, AutoTokenizer, AutoModelForCausalLM, MistralConfig, Qwen2Config, GPT2Config, DataCollatorWithPadding, AutoConfig, AutoModelForSequenceClassification\n",") # You can use any of models with those configs (even flan T5 xxl!). Other models are not supported.\n","\n","from transformers import logging as hf_logging\n","import torch.nn.functional as F\n","import torch_xla.runtime as xr\n","\n","xr.use_spmd()\n","\n","import torch_xla.experimental.xla_sharding as xs # \"experimental\" prefix always means you're gonna have a good time LMAO\n","from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n","from torch_xla.experimental.xla_sharding import Mesh\n","\n","from peft import LoraConfig, TaskType, get_peft_model # If we wanna use peft. Quantazation requiers GPU though. You'll have to download already quantazed models\n","from spmd_util import partition_module                # You could experiment with using already quantazed models like 4bit/Llama-2-7b-Chat-GPTQ if you're feeling funny\n","from datasets import Dataset, load_dataset, concatenate_datasets\n","from dataclasses import dataclass\n","from tqdm import tqdm\n","\n","import transformers\n","import datasets\n","import pandas as pd\n","import numpy as np\n","from datasets import Dataset\n","from torch.utils.data import Dataset as TorchDataset\n","import torch.utils\n","from torch_xla.utils.checkpoint import checkpoint\n","try:\n","    !export USE_TORCH=True #If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\n","    os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n","    os.environ.pop('TPU_PROCESS_ADDRESSES')\n","    os.environ.pop('CLOUD_TPU_TASK_ID')\n","    hf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\n","except:\n","    pass\n"]},{"cell_type":"markdown","metadata":{},"source":["**Configuration**"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:09.111498Z","iopub.status.busy":"2024-02-29T08:44:09.110728Z","iopub.status.idle":"2024-02-29T08:44:09.116089Z","shell.execute_reply":"2024-02-29T08:44:09.115239Z","shell.execute_reply.started":"2024-02-29T08:44:09.111464Z"},"trusted":true},"outputs":[],"source":["MAX_INPUT=1024\n","MODEL = \"abideen/gemma-7b-openhermes\" #You should be able to use 7B model with no changes! There should be enough HBM\n","SAVED_MODEL = \"fhai50032/Gemma-Unaligned\"\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:16.939058Z","iopub.status.busy":"2024-02-29T08:44:16.938665Z","iopub.status.idle":"2024-02-29T08:44:17.868368Z","shell.execute_reply":"2024-02-29T08:44:17.867377Z","shell.execute_reply.started":"2024-02-29T08:44:16.939026Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens :\n"," {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']} \n","\n","\n"]}],"source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","if 'pad_token' not in tokenizer.special_tokens_map:\n","  tokenizer.pad_token=tokenizer.eos_token\n","\n","\n","print(f\"Tokens :\\n {tokenizer.special_tokens_map} \\n\\n\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:21.271079Z","iopub.status.busy":"2024-02-29T08:44:21.270023Z","iopub.status.idle":"2024-02-29T08:44:21.279047Z","shell.execute_reply":"2024-02-29T08:44:21.278123Z","shell.execute_reply.started":"2024-02-29T08:44:21.271039Z"},"trusted":true},"outputs":[],"source":["class ConversationDataset(TorchDataset):\n","    def __init__(self, tokenizer, max_length=1024, dataset=None):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        messages = self.dataset[idx][\"QAs\"]\n","        text = \"\"\n","        for message in messages:\n","            role = message[\"from\"]\n","            if role == \"system\":\n","                text += f\"<|im_start|>system\\n{message['value']}<|im_end|>\\n\"\n","            if role in [\"human\",\"user\"]:\n","                text += f\"<|im_start|>user\\n{message['value']}<|im_end|>\\n\"\n","            if role == \"function-call\":\n","                text += f\"<|im_start|>call\\n{message['value']}<|im_end|>\\n\"\n","            if role == \"function-response\":\n","                text += f\"<|im_start|>function\\n{message['value']}<|im_end|>\\n\"\n","            if role in [\"gpt\",\"assistant\"]:\n","                text += f\"<|im_start|>assistant\\n{message['value']}{self.tokenizer.eos_token}\"\n","        input_ids = self.tokenizer(text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding=\"max_length\", return_attention_mask=True, return_tensors=\"pt\")\n","        return {\n","            \"input_ids\": input_ids[\"input_ids\"].squeeze(0),\n","            \"labels\": input_ids[\"input_ids\"].squeeze(0),\n","            \"attention_mask\":input_ids[\"attention_mask\"].squeeze(0),\n","        }"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:23.988961Z","iopub.status.busy":"2024-02-29T08:44:23.988526Z","iopub.status.idle":"2024-02-29T08:44:25.872432Z","shell.execute_reply":"2024-02-29T08:44:25.871750Z","shell.execute_reply.started":"2024-02-29T08:44:23.988927Z"},"trusted":true},"outputs":[],"source":["train_dataset=\"NobodyExistsOnTheInternet/ToxicQAFinal\"\n","test_dataset=\"NobodyExistsOnTheInternet/ToxicQAFinal\"\n","\n","train_data = load_dataset(train_dataset, split=\"train\").shuffle(seed=69)\n","val = (load_dataset(test_dataset, split=\"train[:640]\")).shuffle(seed=420)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:27.104591Z","iopub.status.busy":"2024-02-29T08:44:27.104163Z","iopub.status.idle":"2024-02-29T08:44:27.112426Z","shell.execute_reply":"2024-02-29T08:44:27.111804Z","shell.execute_reply.started":"2024-02-29T08:44:27.104539Z"},"trusted":true},"outputs":[{"data":{"text/plain":["6866"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["len(train_data)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:30.130826Z","iopub.status.busy":"2024-02-29T08:44:30.130506Z","iopub.status.idle":"2024-02-29T08:44:30.136084Z","shell.execute_reply":"2024-02-29T08:44:30.135490Z","shell.execute_reply.started":"2024-02-29T08:44:30.130799Z"},"trusted":true},"outputs":[],"source":["FLAGS = {'MAX_INPUT': MAX_INPUT,\n","         'LOGGING_STEPS': 1,\n","         'NUM_EPOCHS': 1,\n","         'PAUSE_STEPS':1000,\n","         'MAX_STEPS': -1,#Ooverides num epochs\n","         'BATCH_SIZE': 2, #Making batch_size lower then 8 will result in slower training, but will allow for larger models\\context. Fortunately, we have 128GBs. Setting higher batch_size doesn't seem to improve time.\n","          'LEN_TRAIN_DATA': len(train_data),\n","         'VAL_STEPS': 20,\n","         'VAL_BATCH': 4,\n","#         'GRAD_ACCUMULATION':2,\n","#          'MAX_GRAD_CLIP':1.0,\n","        'LEARNING_RATE':2e-5,\n","         'WARMUP_RATIO':0.1,\n","         'OPTIMIZER':'adamw', # default = 'adamw'  options->  ['adamw','adamw8bit','adafactor','lion']           \n","         'SCHEDULAR':'linear', # default= 'cosine'     options:-> ['linear','cosine']\n","         'WEIGHT_DECAY':0.01,\n","         'TRAIN_DATASET':train_dataset,\n","         \"TEST_DATASET\":test_dataset,\n","         'WANDB':True,\n","        'PROJECT':'Xlake-Coder',\n","        } # Indian pun :) "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:34.121740Z","iopub.status.busy":"2024-02-29T08:44:34.121408Z","iopub.status.idle":"2024-02-29T08:44:34.126880Z","shell.execute_reply":"2024-02-29T08:44:34.126246Z","shell.execute_reply.started":"2024-02-29T08:44:34.121713Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'MAX_INPUT': 1024,\n"," 'LOGGING_STEPS': 1,\n"," 'NUM_EPOCHS': 1,\n"," 'PAUSE_STEPS': 100,\n"," 'MAX_STEPS': -1,\n"," 'BATCH_SIZE': 2,\n"," 'LEN_TRAIN_DATA': 6866,\n"," 'VAL_STEPS': 20,\n"," 'VAL_BATCH': 4,\n"," 'LEARNING_RATE': 2e-05,\n"," 'WARMUP_RATIO': 0.1,\n"," 'OPTIMIZER': 'adamw',\n"," 'SCHEDULAR': 'linear',\n"," 'WEIGHT_DECAY': 0.01,\n"," 'TRAIN_DATASET': 'NobodyExistsOnTheInternet/ToxicQAFinal',\n"," 'TEST_DATASET': 'NobodyExistsOnTheInternet/ToxicQAFinal',\n"," 'WANDB': True,\n"," 'PROJECT': 'Xlake-Coder'}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["FLAGS"]},{"cell_type":"markdown","metadata":{},"source":["**Quantization When??**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from transformers import BitsAndBytesConfig\n","\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True,\n","#     bnb_4bit_use_double_quant=True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_compute_dtype=torch.bfloat16,\n","#     llm_int8_has_fp16_weight=False,\n","        \n","# )\n","# model = AutoModelForCausalLM.from_pretrained(MODEL,torch_dtype=torch.bfloat16,quantization_config=bnb_config,\n","#     trust_remote_code=True,\n","#     low_cpu_mem_usage=True) "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:37.569081Z","iopub.status.busy":"2024-02-29T08:44:37.568732Z","iopub.status.idle":"2024-02-29T08:44:42.171091Z","shell.execute_reply":"2024-02-29T08:44:42.170417Z","shell.execute_reply.started":"2024-02-29T08:44:37.569051Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\n"]}],"source":["model = AutoModelForCausalLM.from_pretrained(MODEL,torch_dtype=torch.bfloat16) \n","model._set_gradient_checkpointing(enable=True, gradient_checkpointing_func=checkpoint)\n","### use only bf16 or atleast set compute type to bf16 "]},{"cell_type":"markdown","metadata":{},"source":["**LoRA Applicable**"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:44.631788Z","iopub.status.busy":"2024-02-29T08:44:44.631468Z","iopub.status.idle":"2024-02-29T08:44:49.349528Z","shell.execute_reply":"2024-02-29T08:44:49.348777Z","shell.execute_reply.started":"2024-02-29T08:44:44.631761Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n","  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"]},{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n","trainable params: 150,011,904 || all params: 8,687,692,800 || trainable%: 1.7267174087923551\n"]}],"source":["ls=LoraConfig(\n","    r = 48, # Lora Rank ,I would prefer 8-32 for smaller models like 7b\n","    target_modules = ['q_proj', 'down_proj', 'up_proj', 'o_proj', 'v_proj', 'gate_proj', 'k_proj'],\n","    lora_alpha = 16, #weight_scaling\n","    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimize\n","    # modules_to_save = [\"lm_head\", \"embed_tokens\"] ## if you use new chat formats or embedding tokens\n",")\n","model = get_peft_model(model, ls)\n","model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{},"source":["**Data-Distributer**"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:52.906598Z","iopub.status.busy":"2024-02-29T08:44:52.906017Z","iopub.status.idle":"2024-02-29T08:44:52.913549Z","shell.execute_reply":"2024-02-29T08:44:52.912939Z","shell.execute_reply.started":"2024-02-29T08:44:52.906562Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Max Steps:- 430  , Each Step has 16 inputs\n"]}],"source":["train_data = ConversationDataset(tokenizer, dataset=train_data, max_length=1024)\n","val = ConversationDataset(tokenizer, dataset=val)\n","train_sampler = torch.utils.data.distributed.DistributedSampler(\n","    train_data, num_replicas=8, rank=xm.get_ordinal(), shuffle=True)\n","training_loader = torch.utils.data.DataLoader(train_data, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=train_sampler)\n","val_sampler = torch.utils.data.distributed.DistributedSampler(\n","    val, num_replicas=8, rank=xm.get_ordinal(), shuffle=True)\n","testing_loader = torch.utils.data.DataLoader(val, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=val_sampler)\n","\n","print(f\"Max Steps:- {len(training_loader)}  , Each Step has {8*FLAGS['BATCH_SIZE']} inputs\")\n","\n","FLAGS['STEPS']=len(training_loader)\n","FLAGS['BATCH_DATA']=FLAGS['BATCH_SIZE']*8 ## 8 CORES ON TPU \n","# print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(val[0]['input_ids'])\n","for i in testing_loader:\n","    print(i['input_ids'])\n","    break\n","print(tokenizer.decode(val[0]['input_ids']))"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:57.462890Z","iopub.status.busy":"2024-02-29T08:44:57.462566Z","iopub.status.idle":"2024-02-29T08:44:57.468968Z","shell.execute_reply":"2024-02-29T08:44:57.468232Z","shell.execute_reply.started":"2024-02-29T08:44:57.462863Z"},"trusted":true},"outputs":[],"source":["def get_nb_trainable_parameters(model):\n","        r\"\"\"\n","        Returns the number of trainable parameters and number of all parameters in the model.\n","        \"\"\"\n","        trainable_params = 0\n","        all_param = 0\n","        for _, param in model.named_parameters():\n","            num_params = param.numel()\n","            # if using DS Zero 3 and the weights are initialized empty\n","            if num_params == 0 and hasattr(param, \"ds_numel\"):\n","                num_params = param.ds_numel\n","\n","            # Due to the design of 4bit linear layers from bitsandbytes\n","            # one needs to multiply the number of parameters by 2 to get\n","            # the correct number of parameters\n","            if param.__class__.__name__ == \"Params4bit\":\n","                num_params = num_params * 2\n","\n","            all_param += num_params\n","            if param.requires_grad:\n","                trainable_params += num_params\n","\n","        return trainable_params, all_param\n","def print_trainable_parameters(model):\n","        \"\"\"\n","        Prints the number of trainable parameters in the model.\n","        \"\"\"\n","        trainable_params, all_param = get_nb_trainable_parameters(model)\n","        \n","        print(\n","            f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} || trainable%: {100 * trainable_params / all_param}\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print_trainable_parameters(model)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:45:00.968977Z","iopub.status.busy":"2024-02-29T08:45:00.968642Z","iopub.status.idle":"2024-02-29T08:45:26.316300Z","shell.execute_reply":"2024-02-29T08:45:26.315190Z","shell.execute_reply.started":"2024-02-29T08:45:00.968948Z"},"trusted":true},"outputs":[],"source":["config = AutoConfig.from_pretrained(MODEL)\n","num_devices = xr.global_runtime_device_count()\n","mesh_shape = (1, num_devices, 1)\n","device_ids = np.array(range(num_devices))\n","mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n","partition_module(model, mesh) # After this, the model is sharded between cores but still has the same API as if it was on single device. Neat."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["FLAGS"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:45:34.901746Z","iopub.status.busy":"2024-02-29T08:45:34.901138Z","iopub.status.idle":"2024-02-29T08:45:36.417079Z","shell.execute_reply":"2024-02-29T08:45:36.415928Z","shell.execute_reply.started":"2024-02-29T08:45:34.901690Z"},"trusted":true},"outputs":[],"source":["!export XLA_USE_BF16=1\n","import torch.nn as nn\n","import wandb\n","__wandb__=FLAGS['WANDB']\n","from random import randrange\n","from transformers import AdamW,Adafactor\n","from lion_pytorch import Lion #LION best used for large batch size ~ 4096+ similar convergence as adam but faster\n","from transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\n","# from bitsandbytes.optim import AdamW8bit \n","val_step=0\n","device = xm.xla_device()\n","\n","\n","def evaluate_loss(outputs,labels,pad_id=tokenizer.pad_token_id):\n","  epsilon=1e-8\n","  logits=outputs.logits\n","  logits = logits[..., :-1, :].contiguous()\n","  labels = labels[..., 1:].contiguous()\n","  log_probs = -nn.functional.log_softmax(logits, dim=-1)\n","  if labels.dim() == log_probs.dim() - 1:\n","    labels = labels.unsqueeze(-1)\n","  padding_mask = labels.eq(pad_id)\n","  labels = torch.clamp(labels, min=0)\n","  nll_loss = log_probs.gather(dim=-1, index=labels)\n","  smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)\n","  nll_loss.masked_fill_(padding_mask, 0.0)\n","  smoothed_loss.masked_fill_(padding_mask, 0.0)\n","  num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n","  nll_loss = nll_loss.sum() / num_active_elements\n","  smoothed_loss = smoothed_loss.sum() / (num_active_elements * log_probs.shape[-1])\n","  return (1-epsilon)*nll_loss + epsilon*smoothed_loss\n","\n","\n","\n","def train(FLAGS):\n","\n","    \n","    ### Configuring Training\n","    global val_step\n","    update_params= filter(lambda p: p.requires_grad, model.parameters())\n","    num_iterations = FLAGS[\"NUM_EPOCHS\"] * FLAGS['STEPS']  #    // FLAGS['GRAD_ACCUMULATION'])\n","    warmup_steps = int(num_iterations * FLAGS['WARMUP_RATIO'])\n","    \n","    if __wandb__:\n","        wandb.init(project=FLAGS['PROJECT'],config=FLAGS)\n","        wandb.define_metric(\"Validation_loss\", step_metric=\"val_step\")\n","        wandb.define_metric(\"Learning_rate\",step_metric=\"train_step\")\n","        wandb.define_metric(\"train_loss\",step_metric=\"train_step\")\n","    \n","    ### Optimizers\n","    \n","    if (FLAGS['OPTIMIZER']).lower()=='adamw':\n","        optimizer = AdamW(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'],no_deprecation_warning=True)\n","    elif (FLAGS['OPTIMIZER']).lower()=='lion':\n","        optimizer = Lion(update_params, lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'])\n","    elif (FLAGS['OPTIMIZER']).lower()=='adafactor':\n","        optimizer = Adafactor(update_params,lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'],scale_parameter=True,relative_step=False)\n","    else:\n","#         optimizer = AdamW8bit(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'])\n","        optimizer = AdamW(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'],no_deprecation_warning=True)\n","\n","    for param_group in optimizer.param_groups:\n","        if len(param_group[\"params\"]) > 0:\n","            print(param_group[\"params\"][0].device)\n","            break\n","    \n","    \n","    ### Schedulars\n","    \n","    if (FLAGS['SCHEDULAR']).lower()=='linear':\n","        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n","    else:\n","        scheduler = get_cosine_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n","        \n","        \n","    \n","    \n","    ### Training Loop\n","    val_step=0\n","    check=False #for brakes\n","    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n","        if check:\n","            break\n","        model.train()\n","        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n","        for step, batch in enumerate(training_loader):\n","            \n","            input_ids, labels,attention_mask = batch[\"input_ids\"].to(device),  batch[\"labels\"].to(device),batch['attention_mask'].to(device)\n","            xs.mark_sharding(input_ids, mesh, (0, 1))  ### earlier:-> (0,1) according to pytorch-xla , input/dataloaders must be sharded across ('data',None) \n","            xs.mark_sharding( labels,   mesh, (0, 1))  ###\n","            xs.mark_sharding(  attention_mask,    mesh, (0, 1))###\n","            outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n","            loss = evaluate_loss(outputs,labels)\n","            \n","#           loss = loss / (FLAGS['GRAD_ACCUMULATION'] + scheduler.get_last_lr()[0]) # my touch for grad_norm\n","\n","\n","\n","            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n","                xm.master_print(f'loss: {loss.item()}, time: {test_utils.now()}, step: {step+1}')\n","            if __wandb__:\n","                wandb.log({\n","                'Learning_rate': optimizer.param_groups[0]['lr'],\n","                'train_loss': loss.item(),\n","                'train_step': step + 1 + ((epoch-1) * FLAGS[\"STEPS\"]),\n","                        })\n","            del input_ids , attention_mask \n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            xm.mark_step()\n","            optimizer.zero_grad()\n","            del loss \n","                            \n","            if (step+1)% FLAGS['VAL_STEPS'] == 0:\n","                end_index=FLAGS[\"VAL_BATCH\"]\n","                with torch.no_grad():\n","                    total_loss = 0\n","                    total_step = 0\n","                    for stepx, batchx in enumerate(testing_loader):\n","                        input_ids = batchx[\"input_ids\"].to(device)\n","                        labels = batchx[\"labels\"].to(device)\n","                        attention_mask = batchx[\"attention_mask\"].to(device)\n","                        xs.mark_sharding(input_ids, mesh, (0, 1))\n","                        xs.mark_sharding(labels, mesh, (0, 1))\n","                        xs.mark_sharding( attention_mask,    mesh, (0, 1))\n","                        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n","                        loss = evaluate_loss(outputs,labels)\n","                        total_loss += loss.item()\n","                        total_step +=1\n","                        xm.master_print('----- Time -> {} ----- Validation Batch -> {} ----  Validation Loss -> {:.4f}'.format(test_utils.now(), total_step , loss.item()))\n","                        if __wandb__:\n","                            val_step+=1\n","                            wandb.log({\n","                                'Validation_loss': loss.item(),\n","                                'val_step':val_step,\n","                                    })\n","                        if (stepx+1)%end_index==0:\n","                            break\n","                        \n","                    average_loss=total_loss/total_step\n","                    xm.master_print('----- Time -> {} ----- Validation Batch Size -> {} ----  Validation Loss -> {:.7f}'.format(test_utils.now(), total_step , average_loss))\n","\n","            if (step+1)% FLAGS['PAUSE_STEPS']==0:\n","                inp=input('want to continue training after {} steps'.format(step+1))\n","                check = bool(\"no\" in inp.lower())\n","                if check:\n","                    break\n","                else:\n","                    pass\n","            \n","        \n","        \n","        \n","        \n","          "]},{"cell_type":"markdown","metadata":{},"source":["**12 Mins to Train on 4k**"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:45:42.554589Z","iopub.status.busy":"2024-02-29T08:45:42.554111Z","iopub.status.idle":"2024-02-29T09:09:54.002353Z","shell.execute_reply":"2024-02-29T09:09:54.001612Z","shell.execute_reply.started":"2024-02-29T08:45:42.554550Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.16.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240229_084547-bhjgc3g4</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/fhai50032/Xlake-Coder/runs/bhjgc3g4' target=\"_blank\">apricot-cloud-8</a></strong> to <a href='https://wandb.ai/fhai50032/Xlake-Coder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/fhai50032/Xlake-Coder' target=\"_blank\">https://wandb.ai/fhai50032/Xlake-Coder</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/fhai50032/Xlake-Coder/runs/bhjgc3g4' target=\"_blank\">https://wandb.ai/fhai50032/Xlake-Coder/runs/bhjgc3g4</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["xla:0\n","Epoch 1 train begin 08:45:48\n","loss: 3.703125238418579, time: 08:46:33, step: 1\n","loss: 3.781250238418579, time: 08:48:48, step: 2\n","loss: 3.750000238418579, time: 08:50:31, step: 3\n","loss: 3.250000238418579, time: 08:50:32, step: 4\n","loss: 3.343750238418579, time: 08:50:33, step: 5\n","loss: 3.218750238418579, time: 08:50:35, step: 6\n","loss: 3.593750238418579, time: 08:50:36, step: 7\n","loss: 3.625000238418579, time: 08:50:38, step: 8\n","loss: 3.328125238418579, time: 08:50:39, step: 9\n","loss: 3.281250238418579, time: 08:50:41, step: 10\n","loss: 3.500000238418579, time: 08:50:42, step: 11\n","loss: 3.046875238418579, time: 08:50:44, step: 12\n","loss: 3.296875238418579, time: 08:50:45, step: 13\n","loss: 3.421875238418579, time: 08:50:47, step: 14\n","loss: 2.937500238418579, time: 08:50:48, step: 15\n","loss: 2.968750238418579, time: 08:50:50, step: 16\n","loss: 2.859375238418579, time: 08:50:51, step: 17\n","loss: 3.156250238418579, time: 08:50:53, step: 18\n","loss: 3.343750238418579, time: 08:50:54, step: 19\n","loss: 3.000000238418579, time: 08:50:56, step: 20\n","----- Time -> 08:50:57 ----- Validation Batch -> 1 ----  Validation Loss -> 3.2344\n","----- Time -> 08:51:36 ----- Validation Batch -> 2 ----  Validation Loss -> 3.1719\n","----- Time -> 08:52:22 ----- Validation Batch -> 3 ----  Validation Loss -> 3.0781\n","----- Time -> 08:53:14 ----- Validation Batch -> 4 ----  Validation Loss -> 3.3438\n","----- Time -> 08:53:14 ----- Validation Batch Size -> 4 ----  Validation Loss -> 3.2070315\n","loss: 3.046875238418579, time: 08:54:13, step: 21\n","loss: 2.781250238418579, time: 08:56:26, step: 22\n","loss: 3.156250238418579, time: 08:56:30, step: 23\n","loss: 3.234375238418579, time: 08:56:31, step: 24\n","loss: 2.843750238418579, time: 08:56:33, step: 25\n","loss: 3.296875238418579, time: 08:56:34, step: 26\n","loss: 3.031250238418579, time: 08:56:36, step: 27\n","loss: 2.796875238418579, time: 08:56:37, step: 28\n","loss: 2.734375238418579, time: 08:56:38, step: 29\n","loss: 2.703125238418579, time: 08:56:40, step: 30\n","loss: 2.703125238418579, time: 08:56:41, step: 31\n","loss: 2.921875238418579, time: 08:56:43, step: 32\n","loss: 2.593750238418579, time: 08:56:44, step: 33\n","loss: 2.593750238418579, time: 08:56:46, step: 34\n","loss: 2.468750238418579, time: 08:56:47, step: 35\n","loss: 2.937500238418579, time: 08:56:49, step: 36\n","loss: 2.437500238418579, time: 08:56:50, step: 37\n","loss: 2.515625238418579, time: 08:56:52, step: 38\n","loss: 2.734375238418579, time: 08:56:53, step: 39\n","loss: 2.296875238418579, time: 08:56:55, step: 40\n","----- Time -> 08:56:56 ----- Validation Batch -> 1 ----  Validation Loss -> 2.4219\n","----- Time -> 08:56:58 ----- Validation Batch -> 2 ----  Validation Loss -> 2.4063\n","----- Time -> 08:56:59 ----- Validation Batch -> 3 ----  Validation Loss -> 2.5000\n","----- Time -> 08:57:00 ----- Validation Batch -> 4 ----  Validation Loss -> 2.5156\n","----- Time -> 08:57:00 ----- Validation Batch Size -> 4 ----  Validation Loss -> 2.4609377\n","loss: 2.500000238418579, time: 08:57:01, step: 41\n","loss: 2.437500238418579, time: 08:57:05, step: 42\n","loss: 2.140625238418579, time: 08:57:09, step: 43\n","loss: 2.296875238418579, time: 08:57:11, step: 44\n","loss: 2.265625238418579, time: 08:57:12, step: 45\n","loss: 2.046875238418579, time: 08:57:14, step: 46\n","loss: 2.046875238418579, time: 08:57:15, step: 47\n","loss: 2.187500238418579, time: 08:57:17, step: 48\n","loss: 1.953125238418579, time: 08:57:18, step: 49\n","loss: 2.062500238418579, time: 08:57:20, step: 50\n","loss: 2.109375238418579, time: 08:57:21, step: 51\n","loss: 1.937500238418579, time: 08:57:23, step: 52\n","loss: 1.968750238418579, time: 08:57:24, step: 53\n","loss: 2.140625238418579, time: 08:57:26, step: 54\n","loss: 2.109375238418579, time: 08:57:27, step: 55\n","loss: 1.914062738418579, time: 08:57:29, step: 56\n","loss: 1.968750238418579, time: 08:57:30, step: 57\n","loss: 2.187500238418579, time: 08:57:32, step: 58\n","loss: 2.046875238418579, time: 08:57:33, step: 59\n","loss: 2.125000238418579, time: 08:57:35, step: 60\n","----- Time -> 08:57:36 ----- Validation Batch -> 1 ----  Validation Loss -> 2.0000\n","----- Time -> 08:57:38 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9688\n","----- Time -> 08:57:39 ----- Validation Batch -> 3 ----  Validation Loss -> 2.0469\n","----- Time -> 08:57:40 ----- Validation Batch -> 4 ----  Validation Loss -> 1.9844\n","----- Time -> 08:57:40 ----- Validation Batch Size -> 4 ----  Validation Loss -> 2.0000002\n","loss: 2.093750238418579, time: 08:57:42, step: 61\n","loss: 1.984375238418579, time: 08:57:46, step: 62\n","loss: 1.945312738418579, time: 08:57:50, step: 63\n","loss: 1.929687738418579, time: 08:57:51, step: 64\n","loss: 1.859375238418579, time: 08:57:53, step: 65\n","loss: 1.750000238418579, time: 08:57:54, step: 66\n","loss: 1.984375238418579, time: 08:57:56, step: 67\n","loss: 1.984375238418579, time: 08:57:57, step: 68\n","loss: 1.7500003576278687, time: 08:57:59, step: 69\n","loss: 1.859375238418579, time: 08:58:00, step: 70\n","loss: 1.804687738418579, time: 08:58:02, step: 71\n","loss: 1.710937738418579, time: 08:58:03, step: 72\n","loss: 1.734375238418579, time: 08:58:05, step: 73\n","loss: 1.929687738418579, time: 08:58:06, step: 74\n","loss: 1.929687738418579, time: 08:58:08, step: 75\n","loss: 1.648437738418579, time: 08:58:09, step: 76\n","loss: 1.664062738418579, time: 08:58:11, step: 77\n","loss: 2.140625238418579, time: 08:58:12, step: 78\n","loss: 1.679687738418579, time: 08:58:14, step: 79\n","loss: 1.593750238418579, time: 08:58:15, step: 80\n","----- Time -> 08:58:17 ----- Validation Batch -> 1 ----  Validation Loss -> 1.7656\n","----- Time -> 08:58:18 ----- Validation Batch -> 2 ----  Validation Loss -> 1.7813\n","----- Time -> 08:58:19 ----- Validation Batch -> 3 ----  Validation Loss -> 1.7969\n","----- Time -> 08:58:20 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7031\n","----- Time -> 08:58:20 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.7617190\n","loss: 1.718750238418579, time: 08:58:22, step: 81\n","loss: 1.742187738418579, time: 08:58:26, step: 82\n","loss: 1.781250238418579, time: 08:58:30, step: 83\n","loss: 1.812500238418579, time: 08:58:31, step: 84\n","loss: 1.546875238418579, time: 08:58:33, step: 85\n","loss: 1.664062738418579, time: 08:58:34, step: 86\n","loss: 1.671875238418579, time: 08:58:36, step: 87\n","loss: 1.593750238418579, time: 08:58:37, step: 88\n","loss: 1.531250238418579, time: 08:58:39, step: 89\n","loss: 1.687500238418579, time: 08:58:40, step: 90\n","loss: 1.4296878576278687, time: 08:58:42, step: 91\n","loss: 1.671875238418579, time: 08:58:43, step: 92\n","loss: 1.6250003576278687, time: 08:58:45, step: 93\n","loss: 1.585937738418579, time: 08:58:46, step: 94\n","loss: 1.609375238418579, time: 08:58:48, step: 95\n","loss: 1.726562738418579, time: 08:58:49, step: 96\n","loss: 1.593750238418579, time: 08:58:51, step: 97\n","loss: 1.617187738418579, time: 08:58:52, step: 98\n","loss: 1.828125238418579, time: 08:58:54, step: 99\n","loss: 1.4843753576278687, time: 08:58:55, step: 100\n","----- Time -> 08:58:57 ----- Validation Batch -> 1 ----  Validation Loss -> 1.6406\n","----- Time -> 08:58:58 ----- Validation Batch -> 2 ----  Validation Loss -> 1.6484\n","----- Time -> 08:59:00 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6953\n","----- Time -> 08:59:01 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5938\n","----- Time -> 08:59:01 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.6445315\n"]},{"name":"stdout","output_type":"stream","text":["want to continue training after 100 steps yes\n"]},{"name":"stdout","output_type":"stream","text":["loss: 1.562500238418579, time: 08:59:11, step: 101\n","loss: 1.710937738418579, time: 08:59:15, step: 102\n","loss: 1.4843753576278687, time: 08:59:19, step: 103\n","loss: 1.6562503576278687, time: 08:59:20, step: 104\n","loss: 1.648437738418579, time: 08:59:22, step: 105\n","loss: 1.562500238418579, time: 08:59:23, step: 106\n","loss: 1.625000238418579, time: 08:59:25, step: 107\n","loss: 1.476562738418579, time: 08:59:26, step: 108\n","loss: 1.523437738418579, time: 08:59:28, step: 109\n","loss: 1.484375238418579, time: 08:59:29, step: 110\n","loss: 1.585937738418579, time: 08:59:31, step: 111\n","loss: 1.6328128576278687, time: 08:59:32, step: 112\n","loss: 1.6171878576278687, time: 08:59:34, step: 113\n","loss: 1.6328128576278687, time: 08:59:35, step: 114\n","loss: 1.632812738418579, time: 08:59:37, step: 115\n","loss: 1.5156253576278687, time: 08:59:38, step: 116\n","loss: 1.6718753576278687, time: 08:59:40, step: 117\n","loss: 1.4687503576278687, time: 08:59:41, step: 118\n","loss: 1.5625003576278687, time: 08:59:43, step: 119\n","loss: 1.539062738418579, time: 08:59:44, step: 120\n","----- Time -> 08:59:46 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5781\n","----- Time -> 08:59:47 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5859\n","----- Time -> 08:59:49 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6250\n","----- Time -> 08:59:50 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5234\n","----- Time -> 08:59:50 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.5781253\n","loss: 1.5546878576278687, time: 08:59:51, step: 121\n","loss: 1.5156253576278687, time: 08:59:56, step: 122\n","loss: 1.4062503576278687, time: 09:00:00, step: 123\n","loss: 1.6328128576278687, time: 09:00:01, step: 124\n","loss: 1.4531253576278687, time: 09:00:02, step: 125\n","loss: 1.4453128576278687, time: 09:00:04, step: 126\n","loss: 1.5000003576278687, time: 09:00:06, step: 127\n","loss: 1.4453128576278687, time: 09:00:07, step: 128\n","loss: 1.6093753576278687, time: 09:00:08, step: 129\n","loss: 1.3750003576278687, time: 09:00:10, step: 130\n","loss: 1.335937738418579, time: 09:00:12, step: 131\n","loss: 1.625000238418579, time: 09:00:13, step: 132\n","loss: 1.3906253576278687, time: 09:00:14, step: 133\n","loss: 1.4843753576278687, time: 09:00:16, step: 134\n","loss: 1.507812738418579, time: 09:00:17, step: 135\n","loss: 1.4375003576278687, time: 09:00:19, step: 136\n","loss: 1.4531253576278687, time: 09:00:20, step: 137\n","loss: 1.585937738418579, time: 09:00:22, step: 138\n","loss: 1.5000003576278687, time: 09:00:23, step: 139\n","loss: 1.3281253576278687, time: 09:00:25, step: 140\n","----- Time -> 09:00:26 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5391\n","----- Time -> 09:00:28 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5625\n","----- Time -> 09:00:29 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5781\n","----- Time -> 09:00:31 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5000\n","----- Time -> 09:00:31 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.5449221\n","loss: 1.4375003576278687, time: 09:00:32, step: 141\n","loss: 1.5390628576278687, time: 09:00:36, step: 142\n","loss: 1.539062738418579, time: 09:00:40, step: 143\n","loss: 1.3046878576278687, time: 09:00:42, step: 144\n","loss: 1.5625003576278687, time: 09:00:43, step: 145\n","loss: 1.4531253576278687, time: 09:00:45, step: 146\n","loss: 1.5312503576278687, time: 09:00:46, step: 147\n","loss: 1.4687503576278687, time: 09:00:48, step: 148\n","loss: 1.593750238418579, time: 09:00:49, step: 149\n","loss: 1.4843753576278687, time: 09:00:51, step: 150\n","loss: 1.429687738418579, time: 09:00:52, step: 151\n","loss: 1.4218753576278687, time: 09:00:54, step: 152\n","loss: 1.4453128576278687, time: 09:00:55, step: 153\n","loss: 1.3437503576278687, time: 09:00:57, step: 154\n","loss: 1.5156253576278687, time: 09:00:58, step: 155\n","loss: 1.476562738418579, time: 09:01:00, step: 156\n","loss: 1.593750238418579, time: 09:01:01, step: 157\n","loss: 1.3906253576278687, time: 09:01:03, step: 158\n","loss: 1.3593753576278687, time: 09:01:04, step: 159\n","loss: 1.4921878576278687, time: 09:01:06, step: 160\n","----- Time -> 09:01:07 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5156\n","----- Time -> 09:01:08 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5156\n","----- Time -> 09:01:10 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5391\n","----- Time -> 09:01:11 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4766\n","----- Time -> 09:01:11 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.5117190\n","loss: 1.4218753576278687, time: 09:01:13, step: 161\n","loss: 1.3593753576278687, time: 09:01:17, step: 162\n","loss: 1.382812738418579, time: 09:01:21, step: 163\n","loss: 1.406250238418579, time: 09:01:22, step: 164\n","loss: 1.593750238418579, time: 09:01:24, step: 165\n","loss: 1.3750003576278687, time: 09:01:25, step: 166\n","loss: 1.4843753576278687, time: 09:01:27, step: 167\n","loss: 1.2656253576278687, time: 09:01:28, step: 168\n","loss: 1.4453128576278687, time: 09:01:30, step: 169\n","loss: 1.4609378576278687, time: 09:01:31, step: 170\n","loss: 1.4609378576278687, time: 09:01:33, step: 171\n","loss: 1.507812738418579, time: 09:01:34, step: 172\n","loss: 1.398437738418579, time: 09:01:36, step: 173\n","loss: 1.593750238418579, time: 09:01:37, step: 174\n","loss: 1.585937738418579, time: 09:01:39, step: 175\n","loss: 1.4765628576278687, time: 09:01:40, step: 176\n","loss: 1.312500238418579, time: 09:01:42, step: 177\n","loss: 1.554687738418579, time: 09:01:43, step: 178\n","loss: 1.4218753576278687, time: 09:01:45, step: 179\n","loss: 1.640625238418579, time: 09:01:46, step: 180\n","----- Time -> 09:01:48 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4922\n","----- Time -> 09:01:49 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5156\n","----- Time -> 09:01:50 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5156\n","----- Time -> 09:01:52 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4688\n","----- Time -> 09:01:52 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4980472\n","loss: 1.5703128576278687, time: 09:01:53, step: 181\n","loss: 1.3984378576278687, time: 09:01:57, step: 182\n","loss: 1.570312738418579, time: 09:02:01, step: 183\n","loss: 1.2578128576278687, time: 09:02:03, step: 184\n","loss: 1.4062503576278687, time: 09:02:04, step: 185\n","loss: 1.4453128576278687, time: 09:02:06, step: 186\n","loss: 1.523437738418579, time: 09:02:07, step: 187\n","loss: 1.500000238418579, time: 09:02:09, step: 188\n","loss: 1.414062738418579, time: 09:02:10, step: 189\n","loss: 1.281250238418579, time: 09:02:12, step: 190\n","loss: 1.382812738418579, time: 09:02:13, step: 191\n","loss: 1.500000238418579, time: 09:02:15, step: 192\n","loss: 1.648437738418579, time: 09:02:16, step: 193\n","loss: 1.531250238418579, time: 09:02:18, step: 194\n","loss: 1.375000238418579, time: 09:02:19, step: 195\n","loss: 1.3359378576278687, time: 09:02:21, step: 196\n","loss: 1.335937738418579, time: 09:02:22, step: 197\n","loss: 1.4062503576278687, time: 09:02:24, step: 198\n","loss: 1.437500238418579, time: 09:02:25, step: 199\n","loss: 1.382812738418579, time: 09:02:27, step: 200\n","----- Time -> 09:02:28 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4688\n","----- Time -> 09:02:30 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5000\n","----- Time -> 09:02:31 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5234\n","----- Time -> 09:02:32 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4375\n","----- Time -> 09:02:32 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4824221\n"]},{"name":"stdout","output_type":"stream","text":["want to continue training after 200 steps yes\n"]},{"name":"stdout","output_type":"stream","text":["loss: 1.492187738418579, time: 09:02:38, step: 201\n","loss: 1.429687738418579, time: 09:02:42, step: 202\n","loss: 1.476562738418579, time: 09:02:46, step: 203\n","loss: 1.468750238418579, time: 09:02:48, step: 204\n","loss: 1.468750238418579, time: 09:02:49, step: 205\n","loss: 1.390625238418579, time: 09:02:50, step: 206\n","loss: 1.453125238418579, time: 09:02:52, step: 207\n","loss: 1.359375238418579, time: 09:02:54, step: 208\n","loss: 1.351562738418579, time: 09:02:55, step: 209\n","loss: 1.390625238418579, time: 09:02:56, step: 210\n","loss: 1.476562738418579, time: 09:02:58, step: 211\n","loss: 1.3984378576278687, time: 09:03:00, step: 212\n","loss: 1.2578128576278687, time: 09:03:01, step: 213\n","loss: 1.4296878576278687, time: 09:03:02, step: 214\n","loss: 1.421875238418579, time: 09:03:04, step: 215\n","loss: 1.406250238418579, time: 09:03:06, step: 216\n","loss: 1.617187738418579, time: 09:03:07, step: 217\n","loss: 1.546875238418579, time: 09:03:09, step: 218\n","loss: 1.445312738418579, time: 09:03:10, step: 219\n","loss: 1.453125238418579, time: 09:03:12, step: 220\n","----- Time -> 09:03:13 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4609\n","----- Time -> 09:03:14 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4766\n","----- Time -> 09:03:16 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5000\n","----- Time -> 09:03:17 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4141\n","----- Time -> 09:03:17 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4628909\n","loss: 1.500000238418579, time: 09:03:18, step: 221\n","loss: 1.429687738418579, time: 09:03:23, step: 222\n","loss: 1.2500003576278687, time: 09:03:27, step: 223\n","loss: 1.265625238418579, time: 09:03:28, step: 224\n","loss: 1.398437738418579, time: 09:03:30, step: 225\n","loss: 1.453125238418579, time: 09:03:31, step: 226\n","loss: 1.531250238418579, time: 09:03:33, step: 227\n","loss: 1.273437738418579, time: 09:03:34, step: 228\n","loss: 1.351562738418579, time: 09:03:36, step: 229\n","loss: 1.367187738418579, time: 09:03:37, step: 230\n","loss: 1.320312738418579, time: 09:03:39, step: 231\n","loss: 1.429687738418579, time: 09:03:40, step: 232\n","loss: 1.382812738418579, time: 09:03:42, step: 233\n","loss: 1.335937738418579, time: 09:03:43, step: 234\n","loss: 1.453125238418579, time: 09:03:45, step: 235\n","loss: 1.523437738418579, time: 09:03:46, step: 236\n","loss: 1.421875238418579, time: 09:03:48, step: 237\n","loss: 1.335937738418579, time: 09:03:49, step: 238\n","loss: 1.437500238418579, time: 09:03:51, step: 239\n","loss: 1.437500238418579, time: 09:03:52, step: 240\n","----- Time -> 09:03:54 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n","----- Time -> 09:03:55 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4688\n","----- Time -> 09:03:56 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4922\n","----- Time -> 09:03:58 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4063\n","----- Time -> 09:03:58 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4550784\n","loss: 1.460937738418579, time: 09:03:59, step: 241\n","loss: 1.414062738418579, time: 09:04:03, step: 242\n","loss: 1.382812738418579, time: 09:04:07, step: 243\n","loss: 1.546875238418579, time: 09:04:09, step: 244\n","loss: 1.406250238418579, time: 09:04:10, step: 245\n","loss: 1.335937738418579, time: 09:04:12, step: 246\n","loss: 1.203125238418579, time: 09:04:13, step: 247\n","loss: 1.375000238418579, time: 09:04:15, step: 248\n","loss: 1.367187738418579, time: 09:04:16, step: 249\n","loss: 1.601562738418579, time: 09:04:18, step: 250\n","loss: 1.328125238418579, time: 09:04:19, step: 251\n","loss: 1.414062738418579, time: 09:04:21, step: 252\n","loss: 1.382812738418579, time: 09:04:22, step: 253\n","loss: 1.500000238418579, time: 09:04:24, step: 254\n","loss: 1.437500238418579, time: 09:04:25, step: 255\n","loss: 1.523437738418579, time: 09:04:27, step: 256\n","loss: 1.507812738418579, time: 09:04:28, step: 257\n","loss: 1.226562738418579, time: 09:04:30, step: 258\n","loss: 1.156250238418579, time: 09:04:31, step: 259\n","loss: 1.390625238418579, time: 09:04:33, step: 260\n","----- Time -> 09:04:34 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n","----- Time -> 09:04:36 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4609\n","----- Time -> 09:04:37 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4844\n","----- Time -> 09:04:38 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3984\n","----- Time -> 09:04:38 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4492190\n","loss: 1.320312738418579, time: 09:04:40, step: 261\n","loss: 1.343750238418579, time: 09:04:44, step: 262\n","loss: 1.328125238418579, time: 09:04:48, step: 263\n","loss: 1.257812738418579, time: 09:04:50, step: 264\n","loss: 1.406250238418579, time: 09:04:51, step: 265\n","loss: 1.468750238418579, time: 09:04:53, step: 266\n","loss: 1.414062738418579, time: 09:04:54, step: 267\n","loss: 1.375000238418579, time: 09:04:56, step: 268\n","loss: 1.429687738418579, time: 09:04:57, step: 269\n","loss: 1.429687738418579, time: 09:04:59, step: 270\n","loss: 1.406250238418579, time: 09:05:00, step: 271\n","loss: 1.273437738418579, time: 09:05:02, step: 272\n","loss: 1.453125238418579, time: 09:05:03, step: 273\n","loss: 1.351562738418579, time: 09:05:05, step: 274\n","loss: 1.351562738418579, time: 09:05:06, step: 275\n","loss: 1.453125238418579, time: 09:05:07, step: 276\n","loss: 1.445312738418579, time: 09:05:09, step: 277\n","loss: 1.507812738418579, time: 09:05:10, step: 278\n","loss: 1.414062738418579, time: 09:05:12, step: 279\n","loss: 1.437500238418579, time: 09:05:13, step: 280\n","----- Time -> 09:05:15 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n","----- Time -> 09:05:16 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4453\n","----- Time -> 09:05:18 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4766\n","----- Time -> 09:05:19 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3828\n","----- Time -> 09:05:19 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4394534\n","loss: 1.437500238418579, time: 09:05:20, step: 281\n","loss: 1.539062738418579, time: 09:05:25, step: 282\n","loss: 1.585937738418579, time: 09:05:29, step: 283\n","loss: 1.484375238418579, time: 09:05:30, step: 284\n","loss: 1.328125238418579, time: 09:05:32, step: 285\n","loss: 1.421875238418579, time: 09:05:33, step: 286\n","loss: 1.351562738418579, time: 09:05:35, step: 287\n","loss: 1.335937738418579, time: 09:05:36, step: 288\n","loss: 1.296875238418579, time: 09:05:38, step: 289\n","loss: 1.328125238418579, time: 09:05:39, step: 290\n","loss: 1.429687738418579, time: 09:05:41, step: 291\n","loss: 1.382812738418579, time: 09:05:42, step: 292\n","loss: 1.304687738418579, time: 09:05:43, step: 293\n","loss: 1.367187738418579, time: 09:05:45, step: 294\n","loss: 1.375000238418579, time: 09:05:46, step: 295\n","loss: 1.507812738418579, time: 09:05:48, step: 296\n","loss: 1.367187738418579, time: 09:05:49, step: 297\n","loss: 1.304687738418579, time: 09:05:51, step: 298\n","loss: 1.320312738418579, time: 09:05:52, step: 299\n","loss: 1.375000238418579, time: 09:05:54, step: 300\n","----- Time -> 09:05:55 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4609\n","----- Time -> 09:05:57 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4453\n","----- Time -> 09:05:58 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4844\n","----- Time -> 09:06:00 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3906\n","----- Time -> 09:06:00 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4453127\n"]},{"name":"stdout","output_type":"stream","text":["want to continue training after 300 steps yes\n"]},{"name":"stdout","output_type":"stream","text":["loss: 1.375000238418579, time: 09:06:21, step: 301\n","loss: 1.335937738418579, time: 09:06:25, step: 302\n","loss: 1.421875238418579, time: 09:06:29, step: 303\n","loss: 1.304687738418579, time: 09:06:30, step: 304\n","loss: 1.460937738418579, time: 09:06:32, step: 305\n","loss: 1.500000238418579, time: 09:06:33, step: 306\n","loss: 1.351562738418579, time: 09:06:35, step: 307\n","loss: 1.414062738418579, time: 09:06:37, step: 308\n","loss: 1.492187738418579, time: 09:06:38, step: 309\n","loss: 1.406250238418579, time: 09:06:40, step: 310\n","loss: 1.312500238418579, time: 09:06:41, step: 311\n","loss: 1.414062738418579, time: 09:06:43, step: 312\n","loss: 1.335937738418579, time: 09:06:44, step: 313\n","loss: 1.328125238418579, time: 09:06:46, step: 314\n","loss: 1.437500238418579, time: 09:06:47, step: 315\n","loss: 1.351562738418579, time: 09:06:49, step: 316\n","loss: 1.578125238418579, time: 09:06:50, step: 317\n","loss: 1.367187738418579, time: 09:06:52, step: 318\n","loss: 1.382812738418579, time: 09:06:53, step: 319\n","loss: 1.445312738418579, time: 09:06:55, step: 320\n","----- Time -> 09:06:56 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n","----- Time -> 09:06:58 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4453\n","----- Time -> 09:06:59 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4766\n","----- Time -> 09:07:00 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3984\n","----- Time -> 09:07:00 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4433596\n","loss: 1.382812738418579, time: 09:07:02, step: 321\n","loss: 1.367187738418579, time: 09:07:06, step: 322\n","loss: 1.296875238418579, time: 09:07:10, step: 323\n","loss: 1.242187738418579, time: 09:07:12, step: 324\n","loss: 1.351562738418579, time: 09:07:13, step: 325\n","loss: 1.398437738418579, time: 09:07:15, step: 326\n","loss: 1.320312738418579, time: 09:07:16, step: 327\n","loss: 1.343750238418579, time: 09:07:18, step: 328\n","loss: 1.343750238418579, time: 09:07:19, step: 329\n","loss: 1.351562738418579, time: 09:07:21, step: 330\n","loss: 1.468750238418579, time: 09:07:22, step: 331\n","loss: 1.296875238418579, time: 09:07:24, step: 332\n","loss: 1.257812738418579, time: 09:07:25, step: 333\n","loss: 1.546875238418579, time: 09:07:27, step: 334\n","loss: 1.398437738418579, time: 09:07:28, step: 335\n","loss: 1.593750238418579, time: 09:07:30, step: 336\n","loss: 1.445312738418579, time: 09:07:31, step: 337\n","loss: 1.367187738418579, time: 09:07:33, step: 338\n","loss: 1.375000238418579, time: 09:07:34, step: 339\n","loss: 1.359375238418579, time: 09:07:36, step: 340\n","----- Time -> 09:07:37 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n","----- Time -> 09:07:38 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4609\n","----- Time -> 09:07:40 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4766\n","----- Time -> 09:07:41 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3984\n","----- Time -> 09:07:41 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4472659\n","loss: 1.164062738418579, time: 09:07:43, step: 341\n","loss: 1.476562738418579, time: 09:07:47, step: 342\n","loss: 1.210937738418579, time: 09:07:51, step: 343\n","loss: 1.351562738418579, time: 09:07:52, step: 344\n","loss: 1.375000238418579, time: 09:07:54, step: 345\n","loss: 1.398437738418579, time: 09:07:55, step: 346\n","loss: 1.281250238418579, time: 09:07:57, step: 347\n","loss: 1.289062738418579, time: 09:07:58, step: 348\n","loss: 1.328125238418579, time: 09:08:00, step: 349\n","loss: 1.187500238418579, time: 09:08:01, step: 350\n","loss: 1.289062738418579, time: 09:08:03, step: 351\n","loss: 1.476562738418579, time: 09:08:04, step: 352\n","loss: 1.296875238418579, time: 09:08:06, step: 353\n","loss: 1.367187738418579, time: 09:08:07, step: 354\n","loss: 1.359375238418579, time: 09:08:09, step: 355\n","loss: 1.406250238418579, time: 09:08:10, step: 356\n","loss: 1.382812738418579, time: 09:08:12, step: 357\n","loss: 1.421875238418579, time: 09:08:13, step: 358\n","loss: 1.320312738418579, time: 09:08:15, step: 359\n","loss: 1.335937738418579, time: 09:08:16, step: 360\n","----- Time -> 09:08:18 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n","----- Time -> 09:08:19 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4453\n","----- Time -> 09:08:20 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4688\n","----- Time -> 09:08:22 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3984\n","----- Time -> 09:08:22 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4414065\n","loss: 1.210937738418579, time: 09:08:23, step: 361\n","loss: 1.312500238418579, time: 09:08:27, step: 362\n","loss: 1.414062738418579, time: 09:08:31, step: 363\n","loss: 1.398437738418579, time: 09:08:33, step: 364\n","loss: 1.406250238418579, time: 09:08:34, step: 365\n","loss: 1.320312738418579, time: 09:08:36, step: 366\n","loss: 1.406250238418579, time: 09:08:37, step: 367\n","loss: 1.359375238418579, time: 09:08:39, step: 368\n","loss: 1.281250238418579, time: 09:08:40, step: 369\n","loss: 1.328125238418579, time: 09:08:42, step: 370\n","loss: 1.195312738418579, time: 09:08:43, step: 371\n","loss: 1.281250238418579, time: 09:08:45, step: 372\n","loss: 1.164062738418579, time: 09:08:46, step: 373\n","loss: 1.375000238418579, time: 09:08:48, step: 374\n","loss: 1.281250238418579, time: 09:08:49, step: 375\n","loss: 1.570312738418579, time: 09:08:51, step: 376\n","loss: 1.359375238418579, time: 09:08:52, step: 377\n","loss: 1.445312738418579, time: 09:08:54, step: 378\n","loss: 1.312500238418579, time: 09:08:55, step: 379\n","loss: 1.265625238418579, time: 09:08:57, step: 380\n","----- Time -> 09:08:58 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4609\n","----- Time -> 09:08:59 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4297\n","----- Time -> 09:09:01 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4766\n","----- Time -> 09:09:02 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3906\n","----- Time -> 09:09:02 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4394534\n","loss: 1.437500238418579, time: 09:09:04, step: 381\n","loss: 1.296875238418579, time: 09:09:08, step: 382\n","loss: 1.382812738418579, time: 09:09:12, step: 383\n","loss: 1.515625238418579, time: 09:09:13, step: 384\n","loss: 1.398437738418579, time: 09:09:15, step: 385\n","loss: 1.359375238418579, time: 09:09:16, step: 386\n","loss: 1.367187738418579, time: 09:09:18, step: 387\n","loss: 1.304687738418579, time: 09:09:19, step: 388\n","loss: 1.367187738418579, time: 09:09:21, step: 389\n","loss: 1.406250238418579, time: 09:09:22, step: 390\n","loss: 1.445312738418579, time: 09:09:24, step: 391\n","loss: 1.304687738418579, time: 09:09:25, step: 392\n","loss: 1.382812738418579, time: 09:09:27, step: 393\n","loss: 1.250000238418579, time: 09:09:28, step: 394\n","loss: 1.375000238418579, time: 09:09:30, step: 395\n","loss: 1.289062738418579, time: 09:09:31, step: 396\n","loss: 1.429687738418579, time: 09:09:33, step: 397\n","loss: 1.460937738418579, time: 09:09:34, step: 398\n","loss: 1.343750238418579, time: 09:09:36, step: 399\n","loss: 1.242187738418579, time: 09:09:37, step: 400\n","----- Time -> 09:09:39 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n","----- Time -> 09:09:40 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4531\n","----- Time -> 09:09:42 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4688\n","----- Time -> 09:09:43 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3828\n","----- Time -> 09:09:43 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4394534\n"]},{"name":"stdout","output_type":"stream","text":["want to continue training after 400 steps no\n"]},{"name":"stderr","output_type":"stream","text":["wandb: WARNING No program path found when generating artifact job source for a non-colab notebook run. See https://docs.wandb.ai/guides/launch/create-job\n","wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"]},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning_rate</td><td>▁▂▄▆███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>Validation_loss</td><td>█▇▅▅▃▄▂▃▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇█▇▅▃▃▃▃▂▂▂▂▂▁▂▂▂▁▁▂▂▁▂▂▂▁▂▁▂▂▁▁▂▁▂▁▂▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning_rate</td><td>0.0</td></tr><tr><td>Validation_loss</td><td>1.38281</td></tr><tr><td>train_loss</td><td>1.24219</td></tr><tr><td>train_step</td><td>400</td></tr><tr><td>val_step</td><td>80</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">apricot-cloud-8</strong> at: <a href='https://wandb.ai/fhai50032/Xlake-Coder/runs/bhjgc3g4' target=\"_blank\">https://wandb.ai/fhai50032/Xlake-Coder/runs/bhjgc3g4</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240229_084547-bhjgc3g4/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["train(FLAGS)\n","if FLAGS['WANDB']:\n","    wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T09:09:58.342510Z","iopub.status.busy":"2024-02-29T09:09:58.342138Z","iopub.status.idle":"2024-02-29T09:12:07.731514Z","shell.execute_reply":"2024-02-29T09:12:07.730373Z","shell.execute_reply.started":"2024-02-29T09:09:58.342478Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading the model on CPU\n","Loaded model on cpu in 129.38400077819824 seconds \n"]}],"source":["import time\n","print('Loading the model on CPU')\n","START=time.time()\n","model = model.cpu()\n","print(f\"Loaded model on cpu in {time.time()-START} seconds \")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T09:12:12.417989Z","iopub.status.busy":"2024-02-29T09:12:12.417594Z","iopub.status.idle":"2024-02-29T09:12:22.131094Z","shell.execute_reply":"2024-02-29T09:12:22.130135Z","shell.execute_reply.started":"2024-02-29T09:12:12.417958Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"name":"stderr","output_type":"stream","text":["README.md: 100%|██████████| 5.18k/5.18k [00:00<00:00, 18.4MB/s]\n","adapter_model.safetensors: 100%|██████████| 300M/300M [00:05<00:00, 54.2MB/s] \n","tokenizer.json: 100%|██████████| 17.5M/17.5M [00:00<00:00, 42.9MB/s]\n"]},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/fhai50032/Gemma-Unaligned/commit/6df55c71cc91b149a6a7da601c23e01672715801', commit_message='Upload tokenizer', commit_description='', oid='6df55c71cc91b149a6a7da601c23e01672715801', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import login\n","login(\"hf_token\") ##\n","model.push_to_hub(\n","    SAVED_MODEL, \n","    tokenizer=tokenizer,\n","    safe_serialization=True,\n","    private=True,\n","    create_pr=True,\n","    max_shard_size=\"3GB\", \n","    )\n","tokenizer.push_to_hub(\n","    SAVED_MODEL,\n","    private=True, \n","    \n","    )"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
