{"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install transformers zstandard jsonlines peft wandb bitsandbytes -q\n!pip3 install accelerate datasets sentencepiece langchain torch_xla[tpuvm] -q\n!pip uninstall -y tensorflow\n!pip install tensorflow-cpu -q\n!git clone https://github.com/IsNoobgrammer/Pytorch-Optimizers optims","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_ipython().kernel.do_shutdown(True)\n### for good measures restart kernel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tokens?**","metadata":{}},{"cell_type":"code","source":"!huggingface-cli login --token <hf_read_token> #for downloading gated models\n# import wandb\n# wandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sharding Module for different Architechture**","metadata":{}},{"cell_type":"code","source":"%%writefile spmd_util.py\nimport math\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional\nfrom collections import defaultdict\nimport torch\nimport torch.nn as nn\nimport re\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n    GPTNeoXConfig, T5Config, LlamaConfig, GPT2Config, MistralConfig, Qwen2Config, MixtralConfig, PhiConfig,GemmaConfig\n)\n\n# ends with $ to prevent sharding lora parameters\n\n\nT5_RULES = (\n    # embeddings\n    (\"shared$\", (\"mp\", \"fsdp\")),\n    (\"embed_tokens$\", (\"mp\", \"fsdp\")),\n    \n    # attention\n    (\"q$\", (\"fsdp\", \"mp\")),\n    (\"k$\", (\"fsdp\", \"mp\")),\n    (\"v$\", (\"fsdp\", \"mp\")),\n    (\"o$\", (\"mp\", \"fsdp\")),\n\n    # mlp\n    (\"w$\", (\"fsdp\", \"mp\")),\n    (\"wi_0$\", (\"fsdp\", \"mp\")),\n    (\"wi_1$\", (\"fsdp\", \"mp\")),\n    (\"wo$\", (\"mp\", \"fsdp\")),\n\n    # seq2seq lm head\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n)\n\nQWEN_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\nGPT2_RULES = (\n    # embeddings\n    (\"wte\", (\"mp\", \"fsdp\")), \n    (\"wpe\", (\"mp\", \"fsdp\")),\n    \n    # attention\n    (\"c_attn\", (\"fsdp\", \"mp\")),\n    (\"c_proj\", (\"mp\", \"fsdp\")),\n    \n    # mlp\n    (\"c_fc\", (\"fsdp\", \"mp\")), \n    (\"c_proj\", (\"mp\", \"fsdp\")),\n    \n    # output \n    (\"ln_f\", (\"fsdp\", \"mp\")),\n)\nMISTRAL_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.(gate_proj|up_proj)\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\n\n\nPHI_RULES = (\n    ### (regex) linear modules, (list[sharding methods]) )\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.dense\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.fc2\", (\"mp\", \"fsdp\")),  \n    (\"mlp\\\\.fc1\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    \n)\n\nLLAMA_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\n\nGPTNEOX_RULES = (\n    # embeddings\n    (\"gpt_neox\\\\.embed_in\", (\"mp\", \"fsdp\")),\n    # atention\n    (\"attention\\\\.query_key_value$\", (\"fsdp\", \"mp\")),\n    (\"attention\\\\.dense$\", (\"mp\", \"fsdp\")),\n    # mlp\n    (\"mlp\\\\.dense_h_to_4h$\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.dense_4h_to_h$\", (\"mp\", \"fsdp\")),\n    # output\n    (\"embed_out\", (\"fsdp\", \"mp\")),\n)\n\n\n\nMIXTRAL_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"w1\", (\"fsdp\", \"mp\")),\n    (\"w2\", (\"mp\", \"fsdp\")),\n    (\"w3\", (\"fsdp\", \"mp\")),\n    (\"gate\", (\"mp\", \"fsdp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\n\nGEMMA_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n)\n    \nALL_RULES = [\n    (GPTNeoXConfig, GPTNEOX_RULES),\n    (T5Config, T5_RULES),\n    (LlamaConfig, LLAMA_RULES),\n    (GPT2Config, GPT2_RULES),\n    (MistralConfig, MISTRAL_RULES),\n    (Qwen2Config, QWEN_RULES),\n    (MixtralConfig, MIXTRAL_RULES),\n    (PhiConfig,PHI_RULES),\n    (GemmaConfig,GEMMA_RULES),\n]\n\n\ndef find_rule(model):\n    for config, rule in ALL_RULES:\n        x1=(str(config).split(\".\"))[-1]\n        x2=(str(model.config.__class__).split(\".\"))[-1]\n#         print(x1,x2)\n        if x1.lower()==x2.lower():\n            return rule\n    raise Exception(\"unsupported model to partitioning\")\n\nstrkey2id = {\n    \"dp\": 0, ## usefull for sharding inputs\n    \"fsdp\": 1, ## Pytorch-Xla (2D-sharding) axis to shard data (mostly mesh shape will be (8,1)) data will be sharded 8 way \n    \"mp\": 2 ## axis to shard model model will be sharded one way \n               ## Recommened checking Pytorch-tpu/transfomers on github (xla-fork of transformers)\n}\n\ndef partition_module(model, mesh, device=xm.xla_device(), verbose=False):\n    partition_specs = find_rule(model)\n    rule = [(k, tuple([strkey2id[x] for x in v])) for k, v in partition_specs]\n        \n    # print(rule)\n\n    for name, module in model.named_modules():\n        module.to(device)\n#         print(name, module.__class__.__name__)\n        if isinstance(module, (nn.Embedding, nn.Linear)):\n            for rule_pattern, spec in rule:\n                if re.findall(rule_pattern, name.lower())  : # and (\"lora\" not in name.lower()):\n                    if verbose:\n                        print(\"match\", rule_pattern, name)\n                    \n                    xs.mark_sharding(module.weight, mesh, spec)\n                    break\n","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:43:54.450080Z","iopub.status.busy":"2024-02-29T08:43:54.449856Z","iopub.status.idle":"2024-02-29T08:43:54.461349Z","shell.execute_reply":"2024-02-29T08:43:54.460731Z","shell.execute_reply.started":"2024-02-29T08:43:54.450051Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"Overwriting spmd_util.py\n"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Required Libs**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport datasets\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch\nimport torch.nn as nn\nimport torch_xla.test.test_utils as test_utils\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n AutoTokenizer, AutoModelForCausalLM, set_seed, DataCollatorWithPadding, AutoConfig \n)\n\nfrom transformers import logging as hf_logging\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nfrom torch_xla.experimental.xla_sharding import Mesh\n\nfrom peft import LoraConfig, TaskType, get_peft_model \nfrom datasets import  load_dataset, concatenate_datasets\nfrom tqdm import tqdm\n\nfrom torch.utils.data import Dataset as TorchDataset\nfrom torch_xla.utils.checkpoint import checkpoint\n\ntry:\n    !export USE_TORCH=True #If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\n    os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n    os.environ.pop('TPU_PROCESS_ADDRESSES')\n    os.environ.pop('CLOUD_TPU_TASK_ID')\n    hf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\nexcept:\n    pass","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:04.153806Z","iopub.status.busy":"2024-02-29T08:44:04.153497Z","iopub.status.idle":"2024-02-29T08:44:09.108646Z","shell.execute_reply":"2024-02-29T08:44:09.107014Z","shell.execute_reply.started":"2024-02-29T08:44:04.153780Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","output_type":"stream","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\n  from .autonotebook import tqdm as notebook_tqdm\n"}]},{"cell_type":"markdown","source":"**Configuration**","metadata":{}},{"cell_type":"code","source":"MAX_INPUT=4096 #128*32\nMODEL = \"fhai50032/RolePlayLake-7B\" #You should be able to use 7B model with no changes! There should be enough HBM\nSAVED_MODEL = \"fhai50032/RP-check-TPU\"\n# !export XLA_TENSOR_ALLOCATOR_MAXSIZE=1000000","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:09.111498Z","iopub.status.busy":"2024-02-29T08:44:09.110728Z","iopub.status.idle":"2024-02-29T08:44:09.116089Z","shell.execute_reply":"2024-02-29T08:44:09.115239Z","shell.execute_reply.started":"2024-02-29T08:44:09.111464Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nif 'pad_token' not in tokenizer.special_tokens_map:\n  tokenizer.pad_token=tokenizer.eos_token\n\n\nprint(f\"Tokens :\\n {tokenizer.special_tokens_map} \\n\\n\")","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:16.939058Z","iopub.status.busy":"2024-02-29T08:44:16.938665Z","iopub.status.idle":"2024-02-29T08:44:17.868368Z","shell.execute_reply":"2024-02-29T08:44:17.867377Z","shell.execute_reply.started":"2024-02-29T08:44:16.939026Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"Tokens :\n\n {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']} \n\n\n\n\n"}]},{"cell_type":"code","source":"class ConversationDataset(TorchDataset):\n    def __init__(self, tokenizer, max_length=1024, dataset=None):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        messages = self.dataset[idx][\"conversations\"]\n        text = \"\"\n        for message in messages:\n            role = message[\"from\"]\n            if role == \"system\":\n                text += f\"<|im_start|>system\\n{message['value']}<|im_end|>\\n\"\n            if role in [\"human\",\"user\"]:\n                text += f\"<|im_start|>user\\n{message['value']}<|im_end|>\\n\"\n            if role == \"function-call\":\n                text += f\"<|im_start|>call\\n{message['value']}<|im_end|>\\n\"\n            if role == \"function-response\":\n                text += f\"<|im_start|>function\\n{message['value']}<|im_end|>\\n\"\n            if role in [\"gpt\",\"assistant\"]:\n                text += f\"<|im_start|>assistant\\n{message['value']}\"\n        input_ids = self.tokenizer(text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding=\"max_length\", return_attention_mask=True, return_tensors=\"pt\")\n        return {\n            \"input_ids\": input_ids[\"input_ids\"].squeeze(0),\n            \"labels\": input_ids[\"input_ids\"].squeeze(0),\n            \"attention_mask\":input_ids[\"attention_mask\"].squeeze(0),\n        }","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:21.271079Z","iopub.status.busy":"2024-02-29T08:44:21.270023Z","iopub.status.idle":"2024-02-29T08:44:21.279047Z","shell.execute_reply":"2024-02-29T08:44:21.278123Z","shell.execute_reply.started":"2024-02-29T08:44:21.271039Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_dataset=\"fhai50032/magicoder-oss-instruct-sharegpt-75k\"\ntest_dataset=\"fhai50032/magicoder-oss-instruct-sharegpt-75k\"\n\ntrain_data = load_dataset(train_dataset, split=\"train\").shuffle(seed=69)\nval = (load_dataset(test_dataset, split=\"train[:640]\")).shuffle(seed=420)","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:23.988961Z","iopub.status.busy":"2024-02-29T08:44:23.988526Z","iopub.status.idle":"2024-02-29T08:44:25.872432Z","shell.execute_reply":"2024-02-29T08:44:25.871750Z","shell.execute_reply.started":"2024-02-29T08:44:23.988927Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:27.104591Z","iopub.status.busy":"2024-02-29T08:44:27.104163Z","iopub.status.idle":"2024-02-29T08:44:27.112426Z","shell.execute_reply":"2024-02-29T08:44:27.111804Z","shell.execute_reply.started":"2024-02-29T08:44:27.104539Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":["6866"]},"metadata":{}}]},{"cell_type":"code","source":"FLAGS = {'MAX_INPUT': MAX_INPUT,\n         'LOGGING_STEPS': 1,\n         'NUM_EPOCHS': 1,\n         'PAUSE_STEPS':1000, # asks to exit training after x steps , #todo checkpoint saving me no lazy\n         'MAX_STEPS': -1,#Ooverides num epochs\n         'BATCH_SIZE': 2, #Making batch_size lower then 8 will result in slower training, but will allow for larger models\\context. Fortunately, we have 128GBs. Setting higher batch_size doesn't seem to improve time.\n          'LEN_TRAIN_DATA': len(train_data),\n         'VAL_STEPS': 20,\n         'VAL_BATCH': 5,\n         'GRAD_ACCUMULATION_STEP':1,\n         'MAX_GRAD_CLIP':1,\n        'LEARNING_RATE':6e-5,\n         'WARMUP_RATIO':0.01,\n         'OPTIMIZER':'SM3', # default = 'adamw'  options->  ['adamw','SM3','came','adafactor','lion']           \n         'SCHEDULAR':'cosine', # default= 'cosine'     options:-> ['linear','cosine']\n         'WEIGHT_DECAY':0.1,\n         'TRAIN_DATASET':train_dataset,\n         \"TEST_DATASET\":test_dataset,\n         'WANDB':True,\n        'PROJECT':'RP-Coder',\n        } # Indian pun :) ","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:30.130826Z","iopub.status.busy":"2024-02-29T08:44:30.130506Z","iopub.status.idle":"2024-02-29T08:44:30.136084Z","shell.execute_reply":"2024-02-29T08:44:30.135490Z","shell.execute_reply.started":"2024-02-29T08:44:30.130799Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"FLAGS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Quantization When??**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(MODEL,torch_dtype=torch.bfloat16) \n# model._set_gradient_checkpointing(enable=True, gradient_checkpointing_func=checkpoint) \n# gradient checkpointing is not properly setup needs to do barieer optimization","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:37.569081Z","iopub.status.busy":"2024-02-29T08:44:37.568732Z","iopub.status.idle":"2024-02-29T08:44:42.171091Z","shell.execute_reply":"2024-02-29T08:44:42.170417Z","shell.execute_reply.started":"2024-02-29T08:44:37.569051Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","output_type":"stream","text":"Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\n"}]},{"cell_type":"markdown","source":"**LoRA Applicable**","metadata":{}},{"cell_type":"code","source":"ls=LoraConfig(\n    r = 48, # Lora Rank ,I would prefer 8-32 for smaller models like 7b\n    target_modules = ['q_proj', 'down_proj', 'up_proj', 'o_proj', 'v_proj', 'gate_proj', 'k_proj'],\n    lora_alpha = 16, #weight_scaling\n    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimize\n    # modules_to_save = [\"lm_head\", \"embed_tokens\"] ## if you use new chat formats or embedding tokens\n)\nmodel = get_peft_model(model, ls)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:44.631788Z","iopub.status.busy":"2024-02-29T08:44:44.631468Z","iopub.status.idle":"2024-02-29T08:44:49.349528Z","shell.execute_reply":"2024-02-29T08:44:49.348777Z","shell.execute_reply.started":"2024-02-29T08:44:44.631761Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","output_type":"stream","text":"/usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"},{"name":"stdout","output_type":"stream","text":"/usr/local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n\ntrainable params: 150,011,904 || all params: 8,687,692,800 || trainable%: 1.7267174087923551\n"}]},{"cell_type":"markdown","source":"**Data-Distributer**","metadata":{}},{"cell_type":"code","source":"train_data = ConversationDataset(tokenizer, dataset=train_data, max_length=FLAGS['MAX_INPUT'])\nval = ConversationDataset(tokenizer, dataset=val)\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(\n    train_data, num_replicas=8, rank=xm.get_ordinal(), shuffle=True,drop_last=True)\ntraining_loader = torch.utils.data.DataLoader(train_data, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=train_sampler)\nval_sampler = torch.utils.data.distributed.DistributedSampler(\n    val, num_replicas=8, rank=xm.get_ordinal(), shuffle=True,drop_last=True)\ntesting_loader = torch.utils.data.DataLoader(val, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=val_sampler)\n\nprint(f\"Max Steps:- {len(training_loader)}  , eFFECTIVE bATCH size {8*FLAGS['BATCH_SIZE']} Input\")\nprint(f\"Val Size:- {len(testing_loader)}  , eFFECTIvE bATCH size {8*FLAGS['BATCH_SIZE']} Input\")\nFLAGS['STEPS']=len(training_loader)\nFLAGS['BATCH_DATA']=FLAGS['BATCH_SIZE']*8 ## 8 CORES ON TPU \n# print(device)","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:52.906598Z","iopub.status.busy":"2024-02-29T08:44:52.906017Z","iopub.status.idle":"2024-02-29T08:44:52.913549Z","shell.execute_reply":"2024-02-29T08:44:52.912939Z","shell.execute_reply.started":"2024-02-29T08:44:52.906562Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","output_type":"stream","text":"Max Steps:- 430  , Each Step has 16 inputs\n"}]},{"cell_type":"code","source":"print(val[0]['input_ids'])\nc=0\nfor step, batch in enumerate(training_loader):\n\n    print(step)\n    print(tokenizer.decode(batch['input_ids'][0]))\n    break\n# print(tokenizer.decode(val[0]['input_ids']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_nb_trainable_parameters(model):\n        r\"\"\"\n        Returns the number of trainable parameters and number of all parameters in the model.\n        \"\"\"\n        trainable_params = 0\n        all_param = 0\n        for _, param in model.named_parameters():\n            num_params = param.numel()\n            # if using DS Zero 3 and the weights are initialized empty\n            if num_params == 0 and hasattr(param, \"ds_numel\"):\n                num_params = param.ds_numel\n\n            # Due to the design of 4bit linear layers from bitsandbytes\n            # one needs to multiply the number of parameters by 2 to get\n            # the correct number of parameters\n            if param.__class__.__name__ == \"Params4bit\":\n                num_params = num_params * 2\n\n            all_param += num_params\n            if param.requires_grad:\n                trainable_params += num_params\n\n        return trainable_params, all_param\ndef print_trainable_parameters(model):\n        \"\"\"\n        Prints the number of trainable parameters in the model.\n        \"\"\"\n        trainable_params, all_param = get_nb_trainable_parameters(model)\n        \n        print(\n            f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} || trainable%: {100 * trainable_params / all_param}\"\n        )","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:44:57.462890Z","iopub.status.busy":"2024-02-29T08:44:57.462566Z","iopub.status.idle":"2024-02-29T08:44:57.468968Z","shell.execute_reply":"2024-02-29T08:44:57.468232Z","shell.execute_reply.started":"2024-02-29T08:44:57.462863Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print_trainable_parameters(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spmd_util import partition_module\nimport torch_xla.distributed.parallel_loader as pl\n\ndevice = xm.xla_device()\nmodel = model.to(device)\nfrom torch_xla.distributed.fsdp.utils import apply_xla_patch_to_nn_linear  \nmodel = apply_xla_patch_to_nn_linear(model, xs.xla_patched_nn_linear_forward)  #for patching linear layer to use einsum instead of matmul\nnum_devices = xr.global_runtime_device_count()\nmodel_axis=1\ndata_axis=num_devices//model_axis\nmesh_shape = (1,data_axis, model_axis )\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp','fsdp', 'mp'))\npartition_module(model, mesh)\ntraining_loader = pl.MpDeviceLoader(training_loader, device)\ntesting_loader = pl.MpDeviceLoader(testing_loader, device)\nmesh_shape","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:45:00.968977Z","iopub.status.busy":"2024-02-29T08:45:00.968642Z","iopub.status.idle":"2024-02-29T08:45:26.316300Z","shell.execute_reply":"2024-02-29T08:45:26.315190Z","shell.execute_reply.started":"2024-02-29T08:45:00.968948Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"FLAGS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export XLA_USE_BF16=1\nimport torch.nn as nn\nimport wandb\n__wandb__=FLAGS['WANDB']\nfrom torch_xla.amp.syncfree import AdamW\nfrom transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\nfrom optims.optim import SM3, CAME , Adafactor\n# from random import randrange\n# from bitsandbytes.optim import AdamW8bit \n# from torchdistx.optimizers import AnyPrecisionAdamW\n\nval_step=0\n\n\n\n\ndef evaluate_loss(outputs,labels,pad_id=tokenizer.pad_token_id):\n  epsilon=1e-8\n  logits=outputs.logits\n  logits = logits[..., :-1, :].contiguous()\n  labels = labels[..., 1:].contiguous()\n  log_probs = -nn.functional.log_softmax(logits, dim=-1)\n  if labels.dim() == log_probs.dim() - 1:\n    labels = labels.unsqueeze(-1)\n  padding_mask = labels.eq(pad_id)\n  labels = torch.clamp(labels, min=0)\n  nll_loss = log_probs.gather(dim=-1, index=labels)\n  smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.bfloat16)\n  nll_loss.masked_fill_(padding_mask, 0.0)\n  smoothed_loss.masked_fill_(padding_mask, 0.0)\n  num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n  nll_loss = nll_loss.sum() / num_active_elements\n  smoothed_loss = smoothed_loss.sum() / (num_active_elements * log_probs.shape[-1])\n  del labels,logits,padding_mask\n  return (1-epsilon)*nll_loss + epsilon*smoothed_loss\n\n\n\ndef train(FLAGS):\n\n    \n    ### Configuring Training\n    global val_step\n    update_params= filter(lambda p: p.requires_grad, model.parameters())\n    num_iterations = int((FLAGS[\"NUM_EPOCHS\"] * FLAGS['STEPS'] ) // FLAGS['GRAD_ACCUMULATION_STEP'])\n    warmup_steps = int(num_iterations * FLAGS['WARMUP_RATIO'])\n    \n    if __wandb__:\n        wandb.init(project=FLAGS['PROJECT'],config=FLAGS)\n        wandb.define_metric(\"Validation_loss\", step_metric=\"val_step\")\n        wandb.define_metric(\"Learning_rate\",step_metric=\"train_step\")\n        wandb.define_metric(\"train_loss\",step_metric=\"train_step\")\n    \n    ### Optimizers\n    \n    if (FLAGS['OPTIMIZER']).lower()=='adamw':\n        optimizer = AdamW(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'])\n    elif (FLAGS['OPTIMIZER']).lower()=='lion':\n        optimizer = Lion(update_params, lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'])\n    elif (FLAGS['OPTIMIZER']).lower()=='adafactor':\n        optimizer = Adafactor(update_params,lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'],scale_parameter=False,relative_step=False)\n    elif (FLAGS['OPTIMIZER']).lower()=='came':\n        optimizer = CAME(model.parameters(),lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'],betas=(0.9, 0.999, 0.9999),eps=(1e-30, 1e-16))\n    else:\n#         optimizer = Lilith(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'])\n        optimizer=SM3(update_params,lr=FLAGS['LEARNING_RATE'])\n\n    for param_group in optimizer.param_groups:\n        if len(param_group[\"params\"]) > 0:\n            print(param_group[\"params\"][0].device)\n            break\n    \n    \n    ### Schedulars\n    \n    if (FLAGS['SCHEDULAR']).lower()=='linear':\n        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n    else:\n        scheduler = get_cosine_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n        \n        \n    \n    \n    ### Training Loop\n    val_step=0\n    check=False #for brakes\n    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n        if check:\n            break\n        model.train()\n        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n        for step, batch in enumerate(training_loader):\n            input_ids, labels,attention_mask = batch[\"input_ids\"].to(device),  batch[\"labels\"].to(device),batch['attention_mask'].to(device)\n            xs.mark_sharding(input_ids, mesh, (0,1))  ### earlier:-> (0,1) according to pytorch-xla , input/dataloaders must be sharded across ('data',None) \n            xs.mark_sharding( labels,  mesh, (0,1))  ###\n            xs.mark_sharding(  attention_mask,  mesh, (0, 1)) ###\n            outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n            loss = evaluate_loss(outputs,labels)\n            \n\n            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n                xm.master_print(f'loss: {loss.detach().cpu().item()}, time: {test_utils.now()}, step: {step+1}')\n            if __wandb__:\n                wandb.log({\n                'Learning_rate': optimizer.param_groups[0]['lr'],\n                'train_loss':  loss.detach().cpu().item(),\n                'train_step': step + 1 + ((epoch-1) * FLAGS[\"STEPS\"]),\n                        })\n                \n                \n                \n                \n            del input_ids , attention_mask \n            loss.backward()\n            del outputs,loss\n            \n            \n            \n            \n            if (step+1) % FLAGS['GRAD_ACCUMULATION_STEP'] == 0:\n                torch.nn.utils.clip_grad_norm_(update_params, max_norm=FLAGS['MAX_GRAD_CLIP']*8)\n                scheduler.step()\n                xm.optimizer_step(optimizer,pin_layout=True,barrier=True) ## performs xm.reduce_gradient() , optimizer.step() , xm.mark_step()\n                optimizer.zero_grad()\n            \n            \n            \n            \n            \n            if (step+1)% FLAGS['VAL_STEPS'] == 0:\n                end_index=FLAGS[\"VAL_BATCH\"]\n                model.eval()\n                with torch.no_grad():\n                    total_loss = 0\n                    total_step = 0\n                    for stepx, batchx in enumerate(testing_loader):\n                        input_ids = batchx[\"input_ids\"].to(device)\n                        labels = batchx[\"labels\"].to(device)\n                        attention_mask = batchx[\"attention_mask\"].to(device)\n                        xs.mark_sharding(input_ids, mesh, (0, None))\n                        xs.mark_sharding(labels, mesh, (0, None))\n                        xs.mark_sharding( attention_mask,    mesh, (0, None))\n                        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n                        loss = evaluate_loss(outputs,labels)\n                        total_loss += loss.item()\n                        total_step +=1\n                        xm.master_print('----- Time -> {} ----- Validation Batch -> {} ----  Validation Loss -> {:.4f}'.format(test_utils.now(), total_step , loss.item()))\n                        if __wandb__:\n                            val_step+=1\n                            wandb.log({\n                                'Validation_loss': loss.item(),\n                                'val_step':val_step,\n                                    })\n                        if (stepx+1)%end_index==0:\n                            break\n                    model.train()    \n                    average_loss=total_loss/total_step\n                    xm.master_print('----- Time -> {} ----- Validation Batch Size -> {} ----  Validation Loss -> {:.7f}'.format(test_utils.now(), total_step , average_loss))\n\n            if (step+1)% FLAGS['PAUSE_STEPS']==0:\n                inp=input('want to continue training after {} steps'.format(step+1))\n                check = bool(\"no\" in inp.lower())\n                if check:\n                    break\n                else:\n                    pass\n            \n        \n        \n        \n        \n          ","metadata":{"execution":{"iopub.execute_input":"2024-02-29T08:45:34.901746Z","iopub.status.busy":"2024-02-29T08:45:34.901138Z","iopub.status.idle":"2024-02-29T08:45:36.417079Z","shell.execute_reply":"2024-02-29T08:45:36.415928Z","shell.execute_reply.started":"2024-02-29T08:45:34.901690Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**12 Mins to Train on 4k**","metadata":{}},{"cell_type":"code","source":"train(FLAGS)\nif FLAGS['WANDB']:\n    wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nprint('Loading the model on CPU')\nSTART=time.time()\nmodel = model.cpu()\nprint(f\"Loaded model on cpu in {time.time()-START} seconds \")","metadata":{"execution":{"iopub.execute_input":"2024-02-29T09:09:58.342510Z","iopub.status.busy":"2024-02-29T09:09:58.342138Z","iopub.status.idle":"2024-02-29T09:12:07.731514Z","shell.execute_reply":"2024-02-29T09:12:07.730373Z","shell.execute_reply.started":"2024-02-29T09:09:58.342478Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"Loading the model on CPU\n\nLoaded model on cpu in 129.38400077819824 seconds \n"}]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"hf_token\") ##\nmodel.push_to_hub(\n    SAVED_MODEL, \n    tokenizer=tokenizer,\n    safe_serialization=True,\n    create_pr=True,\n    max_shard_size=\"3GB\", \n    )\ntokenizer.push_to_hub(\n    SAVED_MODEL,\n    \n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
