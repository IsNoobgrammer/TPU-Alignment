{"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install transformers zstandard jsonlines peft wandb bitsandbytes lion-pytorch -q\n!pip3 install accelerate datasets sentencepiece langchain torch_xla[tpuvm] -q\n!pip uninstall tensorflow -y #that's the meme part","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_ipython().kernel.do_shutdown(True)\n### for good measures restart kernel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tokens?**","metadata":{}},{"cell_type":"code","source":"!huggingface-cli login --token <hf_read_token> #for downloading gated models\n# import wandb\n# wandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sharding Module for different Architechture**","metadata":{}},{"cell_type":"code","source":"%%writefile spmd_util.py\nimport torch\nimport torch.nn as nn\nimport re\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n    GPTNeoXConfig, T5Config, LlamaConfig, GPT2Config, MistralConfig, Qwen2Config, MixtralConfig, PhiConfig,GemmaConfig\n)\n\n# ends with $ to prevent sharding lora parameters\n\n\nT5_RULES = (\n    # embeddings\n    (\"shared$\", (\"mp\", \"fsdp\")),\n    (\"embed_tokens$\", (\"mp\", \"fsdp\")),\n    \n    # attention\n    (\"q$\", (\"fsdp\", \"mp\")),\n    (\"k$\", (\"fsdp\", \"mp\")),\n    (\"v$\", (\"fsdp\", \"mp\")),\n    (\"o$\", (\"mp\", \"fsdp\")),\n\n    # mlp\n    (\"w$\", (\"fsdp\", \"mp\")),\n    (\"wi_0$\", (\"fsdp\", \"mp\")),\n    (\"wi_1$\", (\"fsdp\", \"mp\")),\n    (\"wo$\", (\"mp\", \"fsdp\")),\n\n    # seq2seq lm head\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n)\n\nQWEN_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\nGPT2_RULES = (\n    # embeddings\n    (\"wte\", (\"mp\", \"fsdp\")), \n    (\"wpe\", (\"mp\", \"fsdp\")),\n    \n    # attention\n    (\"c_attn\", (\"fsdp\", \"mp\")),\n    (\"c_proj\", (\"mp\", \"fsdp\")),\n    \n    # mlp\n    (\"c_fc\", (\"fsdp\", \"mp\")), \n    (\"c_proj\", (\"mp\", \"fsdp\")),\n    \n    # output \n    (\"ln_f\", (\"fsdp\", \"mp\")),\n)\nMISTRAL_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\n\n\nPHI_RULES = (\n    ### (regex) linear modules, (list[sharding methods]) )\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.dense\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.fc2\", (\"mp\", \"fsdp\")),  \n    (\"mlp\\\\.fc1\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    \n)\n\nLLAMA_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\n\nGPTNEOX_RULES = (\n    # embeddings\n    (\"gpt_neox\\\\.embed_in\", (\"mp\", \"fsdp\")),\n    # atention\n    (\"attention\\\\.query_key_value$\", (\"fsdp\", \"mp\")),\n    (\"attention\\\\.dense$\", (\"mp\", \"fsdp\")),\n    # mlp\n    (\"mlp\\\\.dense_h_to_4h$\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.dense_4h_to_h$\", (\"mp\", \"fsdp\")),\n    # output\n    (\"embed_out\", (\"fsdp\", \"mp\")),\n)\n\n\n\nMIXTRAL_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"w1\", (\"fsdp\", \"mp\")),\n    (\"w2\", (\"mp\", \"fsdp\")),\n    (\"w3\", (\"fsdp\", \"mp\")),\n    (\"gate\", (\"mp\", \"fsdp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\n\nGEMMA_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n)\n    \nALL_RULES = [\n    (GPTNeoXConfig, GPTNEOX_RULES),\n    (T5Config, T5_RULES),\n    (LlamaConfig, LLAMA_RULES),\n    (GPT2Config, GPT2_RULES),\n    (MistralConfig, MISTRAL_RULES),\n    (Qwen2Config, QWEN_RULES),\n    (MixtralConfig, MIXTRAL_RULES),\n    (PhiConfig,PHI_RULES),\n    (GemmaConfig,GEMMA_RULES),\n]\n\ndef find_rule(model):\n    for config, rule in ALL_RULES:\n        if model.config.__class__ == config:\n            return rule\n    raise Exception(\"unsupported model to partitioning\")\n\nstrkey2id = {\n    \"dp\": 0,\n    \"fsdp\": 1,\n    \"mp\": 2\n}\n\ndef partition_module(model, mesh, device=xm.xla_device(), verbose=False):\n    partition_specs = find_rule(model)\n    rule = [(k, tuple([strkey2id[x] for x in v])) for k, v in partition_specs]\n        \n    # print(rule)\n\n    for name, module in model.named_modules():\n        module.to(device)\n        # print(name, module.__class__.__name__)\n        if isinstance(module, (nn.Embedding, nn.Linear)):\n            for rule_pattern, spec in rule:\n                if re.findall(rule_pattern, name):\n                    if verbose:\n                        print(\"match\", rule_pattern, name)\n                    \n                    xs.mark_sharding(module.weight, mesh, spec)\n                    break\n        \ndef partition_module_dp(model, mesh, device=xm.xla_device(), verbose=True):\n    spec = (1, 2)\n\n    for name, module in model.named_modules():\n        module.to(device)\n        if isinstance(module, (nn.Embedding, nn.Linear)):\n            xs.mark_sharding(module.weight, mesh, spec)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:43:54.449856Z","iopub.execute_input":"2024-02-29T08:43:54.450080Z","iopub.status.idle":"2024-02-29T08:43:54.461349Z","shell.execute_reply.started":"2024-02-29T08:43:54.450051Z","shell.execute_reply":"2024-02-29T08:43:54.460731Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Overwriting spmd_util.py\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Required Libs**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport datasets\nimport torch.optim as optim\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.test.test_utils as test_utils\nimport torch\nimport torch.nn as nn\nimport re\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n    GPTNeoXConfig, T5Config, LlamaConfig, AutoTokenizer, AutoModelForCausalLM, MistralConfig, Qwen2Config, GPT2Config, DataCollatorWithPadding, AutoConfig, AutoModelForSequenceClassification\n) # You can use any of models with those configs (even flan T5 xxl!). Other models are not supported.\n\nfrom transformers import logging as hf_logging\nimport torch.nn.functional as F\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nimport torch_xla.experimental.xla_sharding as xs # \"experimental\" prefix always means you're gonna have a good time LMAO\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\n\nfrom peft import LoraConfig, TaskType, get_peft_model # If we wanna use peft. Quantazation requiers GPU though. You'll have to download already quantazed models\nfrom spmd_util import partition_module                # You could experiment with using already quantazed models like 4bit/Llama-2-7b-Chat-GPTQ if you're feeling funny\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\n\nimport transformers\nimport datasets\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom torch.utils.data import Dataset as TorchDataset\nimport torch.utils\ntry:\n    !export USE_TORCH=True #If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\n    os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n    os.environ.pop('TPU_PROCESS_ADDRESSES')\n    os.environ.pop('CLOUD_TPU_TASK_ID')\n    hf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\nexcept:\n    pass\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:44:04.153497Z","iopub.execute_input":"2024-02-29T08:44:04.153806Z","iopub.status.idle":"2024-02-29T08:44:09.108646Z","shell.execute_reply.started":"2024-02-29T08:44:04.153780Z","shell.execute_reply":"2024-02-29T08:44:09.107014Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Configuration**","metadata":{}},{"cell_type":"code","source":"MAX_INPUT=1024\nMODEL = \"abideen/gemma-7b-openhermes\" #You should be able to use 7B model with no changes! There should be enough HBM\nSAVED_MODEL = \"fhai50032/Gemma-Unaligned\"\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:44:09.110728Z","iopub.execute_input":"2024-02-29T08:44:09.111498Z","iopub.status.idle":"2024-02-29T08:44:09.116089Z","shell.execute_reply.started":"2024-02-29T08:44:09.111464Z","shell.execute_reply":"2024-02-29T08:44:09.115239Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nif 'pad_token' not in tokenizer.special_tokens_map:\n  tokenizer.pad_token=tokenizer.eos_token\n\n\nprint(f\"Tokens :\\n {tokenizer.special_tokens_map} \\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:44:16.938665Z","iopub.execute_input":"2024-02-29T08:44:16.939058Z","iopub.status.idle":"2024-02-29T08:44:17.868368Z","shell.execute_reply.started":"2024-02-29T08:44:16.939026Z","shell.execute_reply":"2024-02-29T08:44:17.867377Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Tokens :\n {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']} \n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"class ConversationDataset(TorchDataset):\n    def __init__(self, tokenizer, max_length=1024, dataset=None):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        messages = self.dataset[idx][\"QAs\"]\n        text = \"\"\n        for message in messages:\n            role = message[\"from\"]\n            if role == \"system\":\n                text += f\"<|im_start|>system\\n{message['value']}<|im_end|>\\n\"\n            if role in [\"human\",\"user\"]:\n                text += f\"<|im_start|>user\\n{message['value']}<|im_end|>\\n\"\n            if role == \"function-call\":\n                text += f\"<|im_start|>call\\n{message['value']}<|im_end|>\\n\"\n            if role == \"function-response\":\n                text += f\"<|im_start|>function\\n{message['value']}<|im_end|>\\n\"\n            if role in [\"gpt\",\"assistant\"]:\n                text += f\"<|im_start|>assistant\\n{message['value']}{self.tokenizer.eos_token}\"\n        input_ids = self.tokenizer(text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding=\"max_length\", return_attention_mask=True, return_tensors=\"pt\")\n        return {\n            \"input_ids\": input_ids[\"input_ids\"].squeeze(0),\n            \"labels\": input_ids[\"input_ids\"].squeeze(0),\n            \"attention_mask\":input_ids[\"attention_mask\"].squeeze(0),\n        }","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:44:21.270023Z","iopub.execute_input":"2024-02-29T08:44:21.271079Z","iopub.status.idle":"2024-02-29T08:44:21.279047Z","shell.execute_reply.started":"2024-02-29T08:44:21.271039Z","shell.execute_reply":"2024-02-29T08:44:21.278123Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_dataset=\"NobodyExistsOnTheInternet/ToxicQAFinal\"\ntest_dataset=\"NobodyExistsOnTheInternet/ToxicQAFinal\"\n\ntrain_data = load_dataset(train_dataset, split=\"train\").shuffle(seed=69)\nval = (load_dataset(test_dataset, split=\"train[:640]\")).shuffle(seed=420)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:44:23.988526Z","iopub.execute_input":"2024-02-29T08:44:23.988961Z","iopub.status.idle":"2024-02-29T08:44:25.872432Z","shell.execute_reply.started":"2024-02-29T08:44:23.988927Z","shell.execute_reply":"2024-02-29T08:44:25.871750Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:44:27.104163Z","iopub.execute_input":"2024-02-29T08:44:27.104591Z","iopub.status.idle":"2024-02-29T08:44:27.112426Z","shell.execute_reply.started":"2024-02-29T08:44:27.104539Z","shell.execute_reply":"2024-02-29T08:44:27.111804Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"6866"},"metadata":{}}]},{"cell_type":"code","source":"FLAGS = {'MAX_INPUT': MAX_INPUT,\n         'LOGGING_STEPS': 1,\n         'NUM_EPOCHS': 1,\n         'PAUSE_STEPS':1000,\n         'MAX_STEPS': -1,#Ooverides num epochs\n         'BATCH_SIZE': 2, #Making batch_size lower then 8 will result in slower training, but will allow for larger models\\context. Fortunately, we have 128GBs. Setting higher batch_size doesn't seem to improve time.\n          'LEN_TRAIN_DATA': len(train_data),\n         'VAL_STEPS': 20,\n         'VAL_BATCH': 4,\n#         'GRAD_ACCUMULATION':2,\n#          'MAX_GRAD_CLIP':1.0,\n        'LEARNING_RATE':2e-5,\n         'WARMUP_RATIO':0.1,\n         'OPTIMIZER':'adamw', # default = 'adamw'  options->  ['adamw','adamw8bit','adafactor','lion']           \n         'SCHEDULAR':'linear', # default= 'cosine'     options:-> ['linear','cosine']\n         'WEIGHT_DECAY':0.01,\n         'TRAIN_DATASET':train_dataset,\n         \"TEST_DATASET\":test_dataset,\n         'WANDB':True,\n        'PROJECT':'Xlake-Coder',\n        } # Indian pun :) ","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:44:30.130506Z","iopub.execute_input":"2024-02-29T08:44:30.130826Z","iopub.status.idle":"2024-02-29T08:44:30.136084Z","shell.execute_reply.started":"2024-02-29T08:44:30.130799Z","shell.execute_reply":"2024-02-29T08:44:30.135490Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"FLAGS","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:44:34.121408Z","iopub.execute_input":"2024-02-29T08:44:34.121740Z","iopub.status.idle":"2024-02-29T08:44:34.126880Z","shell.execute_reply.started":"2024-02-29T08:44:34.121713Z","shell.execute_reply":"2024-02-29T08:44:34.126246Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'MAX_INPUT': 1024,\n 'LOGGING_STEPS': 1,\n 'NUM_EPOCHS': 1,\n 'PAUSE_STEPS': 100,\n 'MAX_STEPS': -1,\n 'BATCH_SIZE': 2,\n 'LEN_TRAIN_DATA': 6866,\n 'VAL_STEPS': 20,\n 'VAL_BATCH': 4,\n 'LEARNING_RATE': 2e-05,\n 'WARMUP_RATIO': 0.1,\n 'OPTIMIZER': 'adamw',\n 'SCHEDULAR': 'linear',\n 'WEIGHT_DECAY': 0.01,\n 'TRAIN_DATASET': 'NobodyExistsOnTheInternet/ToxicQAFinal',\n 'TEST_DATASET': 'NobodyExistsOnTheInternet/ToxicQAFinal',\n 'WANDB': True,\n 'PROJECT': 'Xlake-Coder'}"},"metadata":{}}]},{"cell_type":"markdown","source":"**Quantization When??**","metadata":{}},{"cell_type":"code","source":"# from transformers import BitsAndBytesConfig\n\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_use_double_quant=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.bfloat16,\n#     llm_int8_has_fp16_weight=False,\n        \n# )\n# model = AutoModelForCausalLM.from_pretrained(MODEL,torch_dtype=torch.bfloat16,quantization_config=bnb_config,\n#     trust_remote_code=True,\n#     low_cpu_mem_usage=True) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(MODEL,torch_dtype=torch.bfloat16) \n### use only bf16 or atleast set compute type to bf16 ","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:44:37.568732Z","iopub.execute_input":"2024-02-29T08:44:37.569081Z","iopub.status.idle":"2024-02-29T08:44:42.171091Z","shell.execute_reply.started":"2024-02-29T08:44:37.569051Z","shell.execute_reply":"2024-02-29T08:44:42.170417Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**LoRA Applicable**","metadata":{}},{"cell_type":"code","source":"ls=LoraConfig(\n    r = 48, # Lora Rank ,I would prefer 8-32 for smaller models like 7b\n    target_modules = ['q_proj', 'down_proj', 'up_proj', 'o_proj', 'v_proj', 'gate_proj', 'k_proj'],\n    lora_alpha = 16, #weight_scaling\n    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimize\n    # modules_to_save = [\"lm_head\", \"embed_tokens\"] ## if you use new chat formats or embedding tokens\n)\nmodel = get_peft_model(model, ls)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:44:44.631468Z","iopub.execute_input":"2024-02-29T08:44:44.631788Z","iopub.status.idle":"2024-02-29T08:44:49.349528Z","shell.execute_reply.started":"2024-02-29T08:44:44.631761Z","shell.execute_reply":"2024-02-29T08:44:49.348777Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n","output_type":"stream"},{"name":"stdout","text":"/usr/local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\ntrainable params: 150,011,904 || all params: 8,687,692,800 || trainable%: 1.7267174087923551\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Data-Distributer**","metadata":{}},{"cell_type":"code","source":"train_data = ConversationDataset(tokenizer, dataset=train_data, max_length=1024)\nval = ConversationDataset(tokenizer, dataset=val)\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(\n    train_data, num_replicas=8, rank=xm.get_ordinal(), shuffle=True)\ntraining_loader = torch.utils.data.DataLoader(train_data, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=train_sampler)\nval_sampler = torch.utils.data.distributed.DistributedSampler(\n    val, num_replicas=8, rank=xm.get_ordinal(), shuffle=True)\ntesting_loader = torch.utils.data.DataLoader(val, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=val_sampler)\n\nprint(f\"Max Steps:- {len(training_loader)}  , Each Step has {8*FLAGS['BATCH_SIZE']} inputs\")\n\nFLAGS['STEPS']=len(training_loader)\nFLAGS['BATCH_DATA']=FLAGS['BATCH_SIZE']*8 ## 8 CORES ON TPU \n# print(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:44:52.906017Z","iopub.execute_input":"2024-02-29T08:44:52.906598Z","iopub.status.idle":"2024-02-29T08:44:52.913549Z","shell.execute_reply.started":"2024-02-29T08:44:52.906562Z","shell.execute_reply":"2024-02-29T08:44:52.912939Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Max Steps:- 430  , Each Step has 16 inputs\n","output_type":"stream"}]},{"cell_type":"code","source":"print(val[0]['input_ids'])\nfor i in testing_loader:\n    print(i['input_ids'])\n    break\nprint(tokenizer.decode(val[0]['input_ids']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_nb_trainable_parameters(model):\n        r\"\"\"\n        Returns the number of trainable parameters and number of all parameters in the model.\n        \"\"\"\n        trainable_params = 0\n        all_param = 0\n        for _, param in model.named_parameters():\n            num_params = param.numel()\n            # if using DS Zero 3 and the weights are initialized empty\n            if num_params == 0 and hasattr(param, \"ds_numel\"):\n                num_params = param.ds_numel\n\n            # Due to the design of 4bit linear layers from bitsandbytes\n            # one needs to multiply the number of parameters by 2 to get\n            # the correct number of parameters\n            if param.__class__.__name__ == \"Params4bit\":\n                num_params = num_params * 2\n\n            all_param += num_params\n            if param.requires_grad:\n                trainable_params += num_params\n\n        return trainable_params, all_param\ndef print_trainable_parameters(model):\n        \"\"\"\n        Prints the number of trainable parameters in the model.\n        \"\"\"\n        trainable_params, all_param = get_nb_trainable_parameters(model)\n        \n        print(\n            f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} || trainable%: {100 * trainable_params / all_param}\"\n        )","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:44:57.462566Z","iopub.execute_input":"2024-02-29T08:44:57.462890Z","iopub.status.idle":"2024-02-29T08:44:57.468968Z","shell.execute_reply.started":"2024-02-29T08:44:57.462863Z","shell.execute_reply":"2024-02-29T08:44:57.468232Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print_trainable_parameters(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(MODEL)\nnum_devices = xr.global_runtime_device_count()\nmesh_shape = (1, num_devices, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\npartition_module(model, mesh) # After this, the model is sharded between cores but still has the same API as if it was on single device. Neat.","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:00.968642Z","iopub.execute_input":"2024-02-29T08:45:00.968977Z","iopub.status.idle":"2024-02-29T08:45:26.316300Z","shell.execute_reply.started":"2024-02-29T08:45:00.968948Z","shell.execute_reply":"2024-02-29T08:45:26.315190Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"FLAGS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export XLA_USE_BF16=1\nimport torch.nn as nn\nimport wandb\n__wandb__=FLAGS['WANDB']\nfrom random import randrange\nfrom transformers import AdamW,Adafactor\nfrom lion_pytorch import Lion #LION best used for large batch size ~ 4096+ similar convergence as adam but faster\nfrom transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\n# from bitsandbytes.optim import AdamW8bit \nval_step=0\ndevice = xm.xla_device()\n\n\ndef evaluate_loss(outputs,labels,pad_id=tokenizer.pad_token_id):\n  epsilon=1e-8\n  logits=outputs.logits\n  logits = logits[..., :-1, :].contiguous()\n  labels = labels[..., 1:].contiguous()\n  log_probs = -nn.functional.log_softmax(logits, dim=-1)\n  if labels.dim() == log_probs.dim() - 1:\n    labels = labels.unsqueeze(-1)\n  padding_mask = labels.eq(pad_id)\n  labels = torch.clamp(labels, min=0)\n  nll_loss = log_probs.gather(dim=-1, index=labels)\n  smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)\n  nll_loss.masked_fill_(padding_mask, 0.0)\n  smoothed_loss.masked_fill_(padding_mask, 0.0)\n  num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n  nll_loss = nll_loss.sum() / num_active_elements\n  smoothed_loss = smoothed_loss.sum() / (num_active_elements * log_probs.shape[-1])\n  return (1-epsilon)*nll_loss + epsilon*smoothed_loss\n\n\n\ndef train(FLAGS):\n\n    \n    ### Configuring Training\n    global val_step\n    update_params= filter(lambda p: p.requires_grad, model.parameters())\n    num_iterations = FLAGS[\"NUM_EPOCHS\"] * FLAGS['STEPS']  #    // FLAGS['GRAD_ACCUMULATION'])\n    warmup_steps = int(num_iterations * FLAGS['WARMUP_RATIO'])\n    \n    if __wandb__:\n        wandb.init(project=FLAGS['PROJECT'],config=FLAGS)\n        wandb.define_metric(\"Validation_loss\", step_metric=\"val_step\")\n        wandb.define_metric(\"Learning_rate\",step_metric=\"train_step\")\n        wandb.define_metric(\"train_loss\",step_metric=\"train_step\")\n    \n    ### Optimizers\n    \n    if (FLAGS['OPTIMIZER']).lower()=='adamw':\n        optimizer = AdamW(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'],no_deprecation_warning=True)\n    elif (FLAGS['OPTIMIZER']).lower()=='lion':\n        optimizer = Lion(update_params, lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'])\n    elif (FLAGS['OPTIMIZER']).lower()=='adafactor':\n        optimizer = Adafactor(update_params,lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'],scale_parameter=True,relative_step=False)\n    else:\n#         optimizer = AdamW8bit(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'])\n        optimizer = AdamW(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'],no_deprecation_warning=True)\n\n    for param_group in optimizer.param_groups:\n        if len(param_group[\"params\"]) > 0:\n            print(param_group[\"params\"][0].device)\n            break\n    \n    \n    ### Schedulars\n    \n    if (FLAGS['SCHEDULAR']).lower()=='linear':\n        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n    else:\n        scheduler = get_cosine_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n        \n        \n    \n    \n    ### Training Loop\n    val_step=0\n    check=False #for brakes\n    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n        if check:\n            break\n        model.train()\n        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n        for step, batch in enumerate(training_loader):\n            \n            input_ids, labels,attention_mask = batch[\"input_ids\"].to(device),  batch[\"labels\"].to(device),batch['attention_mask'].to(device)\n            xs.mark_sharding(input_ids, mesh, (0, 1))  ### earlier:-> (0,1) according to pytorch-xla , input/dataloaders must be sharded across ('data',None) \n            xs.mark_sharding( labels,   mesh, (0, 1))  ###\n            xs.mark_sharding(  attention_mask,    mesh, (0, 1))###\n            outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n            loss = evaluate_loss(outputs,labels)\n            \n#           loss = loss / (FLAGS['GRAD_ACCUMULATION'] + scheduler.get_last_lr()[0]) # my touch for grad_norm\n\n\n\n            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n                xm.master_print(f'loss: {loss.item()}, time: {test_utils.now()}, step: {step+1}')\n            if __wandb__:\n                wandb.log({\n                'Learning_rate': optimizer.param_groups[0]['lr'],\n                'train_loss': loss.item(),\n                'train_step': step + 1 + ((epoch-1) * FLAGS[\"STEPS\"]),\n                        })\n            del input_ids , attention_mask \n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            xm.mark_step()\n            optimizer.zero_grad()\n            del loss \n                            \n            if (step+1)% FLAGS['VAL_STEPS'] == 0:\n                end_index=FLAGS[\"VAL_BATCH\"]\n                with torch.no_grad():\n                    total_loss = 0\n                    total_step = 0\n                    for stepx, batchx in enumerate(testing_loader):\n                        input_ids = batchx[\"input_ids\"].to(device)\n                        labels = batchx[\"labels\"].to(device)\n                        attention_mask = batchx[\"attention_mask\"].to(device)\n                        xs.mark_sharding(input_ids, mesh, (0, 1))\n                        xs.mark_sharding(labels, mesh, (0, 1))\n                        xs.mark_sharding( attention_mask,    mesh, (0, 1))\n                        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n                        loss = evaluate_loss(outputs,labels)\n                        total_loss += loss.item()\n                        total_step +=1\n                        xm.master_print('----- Time -> {} ----- Validation Batch -> {} ----  Validation Loss -> {:.4f}'.format(test_utils.now(), total_step , loss.item()))\n                        if __wandb__:\n                            val_step+=1\n                            wandb.log({\n                                'Validation_loss': loss.item(),\n                                'val_step':val_step,\n                                    })\n                        if (stepx+1)%end_index==0:\n                            break\n                        \n                    average_loss=total_loss/total_step\n                    xm.master_print('----- Time -> {} ----- Validation Batch Size -> {} ----  Validation Loss -> {:.7f}'.format(test_utils.now(), total_step , average_loss))\n\n            if (step+1)% FLAGS['PAUSE_STEPS']==0:\n                inp=input('want to continue training after {} steps'.format(step+1))\n                check = bool(\"no\" in inp.lower())\n                if check:\n                    break\n                else:\n                    pass\n            \n        \n        \n        \n        \n          ","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:34.901138Z","iopub.execute_input":"2024-02-29T08:45:34.901746Z","iopub.status.idle":"2024-02-29T08:45:36.417079Z","shell.execute_reply.started":"2024-02-29T08:45:34.901690Z","shell.execute_reply":"2024-02-29T08:45:36.415928Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**12 Mins to Train on 4k**","metadata":{}},{"cell_type":"code","source":"train(FLAGS)\nif FLAGS['WANDB']:\n    wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:42.554111Z","iopub.execute_input":"2024-02-29T08:45:42.554589Z","iopub.status.idle":"2024-02-29T09:09:54.002353Z","shell.execute_reply.started":"2024-02-29T08:45:42.554550Z","shell.execute_reply":"2024-02-29T09:09:54.001612Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240229_084547-bhjgc3g4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/fhai50032/Xlake-Coder/runs/bhjgc3g4' target=\"_blank\">apricot-cloud-8</a></strong> to <a href='https://wandb.ai/fhai50032/Xlake-Coder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/fhai50032/Xlake-Coder' target=\"_blank\">https://wandb.ai/fhai50032/Xlake-Coder</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/fhai50032/Xlake-Coder/runs/bhjgc3g4' target=\"_blank\">https://wandb.ai/fhai50032/Xlake-Coder/runs/bhjgc3g4</a>"},"metadata":{}},{"name":"stdout","text":"xla:0\nEpoch 1 train begin 08:45:48\nloss: 3.703125238418579, time: 08:46:33, step: 1\nloss: 3.781250238418579, time: 08:48:48, step: 2\nloss: 3.750000238418579, time: 08:50:31, step: 3\nloss: 3.250000238418579, time: 08:50:32, step: 4\nloss: 3.343750238418579, time: 08:50:33, step: 5\nloss: 3.218750238418579, time: 08:50:35, step: 6\nloss: 3.593750238418579, time: 08:50:36, step: 7\nloss: 3.625000238418579, time: 08:50:38, step: 8\nloss: 3.328125238418579, time: 08:50:39, step: 9\nloss: 3.281250238418579, time: 08:50:41, step: 10\nloss: 3.500000238418579, time: 08:50:42, step: 11\nloss: 3.046875238418579, time: 08:50:44, step: 12\nloss: 3.296875238418579, time: 08:50:45, step: 13\nloss: 3.421875238418579, time: 08:50:47, step: 14\nloss: 2.937500238418579, time: 08:50:48, step: 15\nloss: 2.968750238418579, time: 08:50:50, step: 16\nloss: 2.859375238418579, time: 08:50:51, step: 17\nloss: 3.156250238418579, time: 08:50:53, step: 18\nloss: 3.343750238418579, time: 08:50:54, step: 19\nloss: 3.000000238418579, time: 08:50:56, step: 20\n----- Time -> 08:50:57 ----- Validation Batch -> 1 ----  Validation Loss -> 3.2344\n----- Time -> 08:51:36 ----- Validation Batch -> 2 ----  Validation Loss -> 3.1719\n----- Time -> 08:52:22 ----- Validation Batch -> 3 ----  Validation Loss -> 3.0781\n----- Time -> 08:53:14 ----- Validation Batch -> 4 ----  Validation Loss -> 3.3438\n----- Time -> 08:53:14 ----- Validation Batch Size -> 4 ----  Validation Loss -> 3.2070315\nloss: 3.046875238418579, time: 08:54:13, step: 21\nloss: 2.781250238418579, time: 08:56:26, step: 22\nloss: 3.156250238418579, time: 08:56:30, step: 23\nloss: 3.234375238418579, time: 08:56:31, step: 24\nloss: 2.843750238418579, time: 08:56:33, step: 25\nloss: 3.296875238418579, time: 08:56:34, step: 26\nloss: 3.031250238418579, time: 08:56:36, step: 27\nloss: 2.796875238418579, time: 08:56:37, step: 28\nloss: 2.734375238418579, time: 08:56:38, step: 29\nloss: 2.703125238418579, time: 08:56:40, step: 30\nloss: 2.703125238418579, time: 08:56:41, step: 31\nloss: 2.921875238418579, time: 08:56:43, step: 32\nloss: 2.593750238418579, time: 08:56:44, step: 33\nloss: 2.593750238418579, time: 08:56:46, step: 34\nloss: 2.468750238418579, time: 08:56:47, step: 35\nloss: 2.937500238418579, time: 08:56:49, step: 36\nloss: 2.437500238418579, time: 08:56:50, step: 37\nloss: 2.515625238418579, time: 08:56:52, step: 38\nloss: 2.734375238418579, time: 08:56:53, step: 39\nloss: 2.296875238418579, time: 08:56:55, step: 40\n----- Time -> 08:56:56 ----- Validation Batch -> 1 ----  Validation Loss -> 2.4219\n----- Time -> 08:56:58 ----- Validation Batch -> 2 ----  Validation Loss -> 2.4063\n----- Time -> 08:56:59 ----- Validation Batch -> 3 ----  Validation Loss -> 2.5000\n----- Time -> 08:57:00 ----- Validation Batch -> 4 ----  Validation Loss -> 2.5156\n----- Time -> 08:57:00 ----- Validation Batch Size -> 4 ----  Validation Loss -> 2.4609377\nloss: 2.500000238418579, time: 08:57:01, step: 41\nloss: 2.437500238418579, time: 08:57:05, step: 42\nloss: 2.140625238418579, time: 08:57:09, step: 43\nloss: 2.296875238418579, time: 08:57:11, step: 44\nloss: 2.265625238418579, time: 08:57:12, step: 45\nloss: 2.046875238418579, time: 08:57:14, step: 46\nloss: 2.046875238418579, time: 08:57:15, step: 47\nloss: 2.187500238418579, time: 08:57:17, step: 48\nloss: 1.953125238418579, time: 08:57:18, step: 49\nloss: 2.062500238418579, time: 08:57:20, step: 50\nloss: 2.109375238418579, time: 08:57:21, step: 51\nloss: 1.937500238418579, time: 08:57:23, step: 52\nloss: 1.968750238418579, time: 08:57:24, step: 53\nloss: 2.140625238418579, time: 08:57:26, step: 54\nloss: 2.109375238418579, time: 08:57:27, step: 55\nloss: 1.914062738418579, time: 08:57:29, step: 56\nloss: 1.968750238418579, time: 08:57:30, step: 57\nloss: 2.187500238418579, time: 08:57:32, step: 58\nloss: 2.046875238418579, time: 08:57:33, step: 59\nloss: 2.125000238418579, time: 08:57:35, step: 60\n----- Time -> 08:57:36 ----- Validation Batch -> 1 ----  Validation Loss -> 2.0000\n----- Time -> 08:57:38 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9688\n----- Time -> 08:57:39 ----- Validation Batch -> 3 ----  Validation Loss -> 2.0469\n----- Time -> 08:57:40 ----- Validation Batch -> 4 ----  Validation Loss -> 1.9844\n----- Time -> 08:57:40 ----- Validation Batch Size -> 4 ----  Validation Loss -> 2.0000002\nloss: 2.093750238418579, time: 08:57:42, step: 61\nloss: 1.984375238418579, time: 08:57:46, step: 62\nloss: 1.945312738418579, time: 08:57:50, step: 63\nloss: 1.929687738418579, time: 08:57:51, step: 64\nloss: 1.859375238418579, time: 08:57:53, step: 65\nloss: 1.750000238418579, time: 08:57:54, step: 66\nloss: 1.984375238418579, time: 08:57:56, step: 67\nloss: 1.984375238418579, time: 08:57:57, step: 68\nloss: 1.7500003576278687, time: 08:57:59, step: 69\nloss: 1.859375238418579, time: 08:58:00, step: 70\nloss: 1.804687738418579, time: 08:58:02, step: 71\nloss: 1.710937738418579, time: 08:58:03, step: 72\nloss: 1.734375238418579, time: 08:58:05, step: 73\nloss: 1.929687738418579, time: 08:58:06, step: 74\nloss: 1.929687738418579, time: 08:58:08, step: 75\nloss: 1.648437738418579, time: 08:58:09, step: 76\nloss: 1.664062738418579, time: 08:58:11, step: 77\nloss: 2.140625238418579, time: 08:58:12, step: 78\nloss: 1.679687738418579, time: 08:58:14, step: 79\nloss: 1.593750238418579, time: 08:58:15, step: 80\n----- Time -> 08:58:17 ----- Validation Batch -> 1 ----  Validation Loss -> 1.7656\n----- Time -> 08:58:18 ----- Validation Batch -> 2 ----  Validation Loss -> 1.7813\n----- Time -> 08:58:19 ----- Validation Batch -> 3 ----  Validation Loss -> 1.7969\n----- Time -> 08:58:20 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7031\n----- Time -> 08:58:20 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.7617190\nloss: 1.718750238418579, time: 08:58:22, step: 81\nloss: 1.742187738418579, time: 08:58:26, step: 82\nloss: 1.781250238418579, time: 08:58:30, step: 83\nloss: 1.812500238418579, time: 08:58:31, step: 84\nloss: 1.546875238418579, time: 08:58:33, step: 85\nloss: 1.664062738418579, time: 08:58:34, step: 86\nloss: 1.671875238418579, time: 08:58:36, step: 87\nloss: 1.593750238418579, time: 08:58:37, step: 88\nloss: 1.531250238418579, time: 08:58:39, step: 89\nloss: 1.687500238418579, time: 08:58:40, step: 90\nloss: 1.4296878576278687, time: 08:58:42, step: 91\nloss: 1.671875238418579, time: 08:58:43, step: 92\nloss: 1.6250003576278687, time: 08:58:45, step: 93\nloss: 1.585937738418579, time: 08:58:46, step: 94\nloss: 1.609375238418579, time: 08:58:48, step: 95\nloss: 1.726562738418579, time: 08:58:49, step: 96\nloss: 1.593750238418579, time: 08:58:51, step: 97\nloss: 1.617187738418579, time: 08:58:52, step: 98\nloss: 1.828125238418579, time: 08:58:54, step: 99\nloss: 1.4843753576278687, time: 08:58:55, step: 100\n----- Time -> 08:58:57 ----- Validation Batch -> 1 ----  Validation Loss -> 1.6406\n----- Time -> 08:58:58 ----- Validation Batch -> 2 ----  Validation Loss -> 1.6484\n----- Time -> 08:59:00 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6953\n----- Time -> 08:59:01 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5938\n----- Time -> 08:59:01 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.6445315\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"want to continue training after 100 steps yes\n"},{"name":"stdout","text":"loss: 1.562500238418579, time: 08:59:11, step: 101\nloss: 1.710937738418579, time: 08:59:15, step: 102\nloss: 1.4843753576278687, time: 08:59:19, step: 103\nloss: 1.6562503576278687, time: 08:59:20, step: 104\nloss: 1.648437738418579, time: 08:59:22, step: 105\nloss: 1.562500238418579, time: 08:59:23, step: 106\nloss: 1.625000238418579, time: 08:59:25, step: 107\nloss: 1.476562738418579, time: 08:59:26, step: 108\nloss: 1.523437738418579, time: 08:59:28, step: 109\nloss: 1.484375238418579, time: 08:59:29, step: 110\nloss: 1.585937738418579, time: 08:59:31, step: 111\nloss: 1.6328128576278687, time: 08:59:32, step: 112\nloss: 1.6171878576278687, time: 08:59:34, step: 113\nloss: 1.6328128576278687, time: 08:59:35, step: 114\nloss: 1.632812738418579, time: 08:59:37, step: 115\nloss: 1.5156253576278687, time: 08:59:38, step: 116\nloss: 1.6718753576278687, time: 08:59:40, step: 117\nloss: 1.4687503576278687, time: 08:59:41, step: 118\nloss: 1.5625003576278687, time: 08:59:43, step: 119\nloss: 1.539062738418579, time: 08:59:44, step: 120\n----- Time -> 08:59:46 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5781\n----- Time -> 08:59:47 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5859\n----- Time -> 08:59:49 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6250\n----- Time -> 08:59:50 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5234\n----- Time -> 08:59:50 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.5781253\nloss: 1.5546878576278687, time: 08:59:51, step: 121\nloss: 1.5156253576278687, time: 08:59:56, step: 122\nloss: 1.4062503576278687, time: 09:00:00, step: 123\nloss: 1.6328128576278687, time: 09:00:01, step: 124\nloss: 1.4531253576278687, time: 09:00:02, step: 125\nloss: 1.4453128576278687, time: 09:00:04, step: 126\nloss: 1.5000003576278687, time: 09:00:06, step: 127\nloss: 1.4453128576278687, time: 09:00:07, step: 128\nloss: 1.6093753576278687, time: 09:00:08, step: 129\nloss: 1.3750003576278687, time: 09:00:10, step: 130\nloss: 1.335937738418579, time: 09:00:12, step: 131\nloss: 1.625000238418579, time: 09:00:13, step: 132\nloss: 1.3906253576278687, time: 09:00:14, step: 133\nloss: 1.4843753576278687, time: 09:00:16, step: 134\nloss: 1.507812738418579, time: 09:00:17, step: 135\nloss: 1.4375003576278687, time: 09:00:19, step: 136\nloss: 1.4531253576278687, time: 09:00:20, step: 137\nloss: 1.585937738418579, time: 09:00:22, step: 138\nloss: 1.5000003576278687, time: 09:00:23, step: 139\nloss: 1.3281253576278687, time: 09:00:25, step: 140\n----- Time -> 09:00:26 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5391\n----- Time -> 09:00:28 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5625\n----- Time -> 09:00:29 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5781\n----- Time -> 09:00:31 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5000\n----- Time -> 09:00:31 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.5449221\nloss: 1.4375003576278687, time: 09:00:32, step: 141\nloss: 1.5390628576278687, time: 09:00:36, step: 142\nloss: 1.539062738418579, time: 09:00:40, step: 143\nloss: 1.3046878576278687, time: 09:00:42, step: 144\nloss: 1.5625003576278687, time: 09:00:43, step: 145\nloss: 1.4531253576278687, time: 09:00:45, step: 146\nloss: 1.5312503576278687, time: 09:00:46, step: 147\nloss: 1.4687503576278687, time: 09:00:48, step: 148\nloss: 1.593750238418579, time: 09:00:49, step: 149\nloss: 1.4843753576278687, time: 09:00:51, step: 150\nloss: 1.429687738418579, time: 09:00:52, step: 151\nloss: 1.4218753576278687, time: 09:00:54, step: 152\nloss: 1.4453128576278687, time: 09:00:55, step: 153\nloss: 1.3437503576278687, time: 09:00:57, step: 154\nloss: 1.5156253576278687, time: 09:00:58, step: 155\nloss: 1.476562738418579, time: 09:01:00, step: 156\nloss: 1.593750238418579, time: 09:01:01, step: 157\nloss: 1.3906253576278687, time: 09:01:03, step: 158\nloss: 1.3593753576278687, time: 09:01:04, step: 159\nloss: 1.4921878576278687, time: 09:01:06, step: 160\n----- Time -> 09:01:07 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5156\n----- Time -> 09:01:08 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5156\n----- Time -> 09:01:10 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5391\n----- Time -> 09:01:11 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4766\n----- Time -> 09:01:11 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.5117190\nloss: 1.4218753576278687, time: 09:01:13, step: 161\nloss: 1.3593753576278687, time: 09:01:17, step: 162\nloss: 1.382812738418579, time: 09:01:21, step: 163\nloss: 1.406250238418579, time: 09:01:22, step: 164\nloss: 1.593750238418579, time: 09:01:24, step: 165\nloss: 1.3750003576278687, time: 09:01:25, step: 166\nloss: 1.4843753576278687, time: 09:01:27, step: 167\nloss: 1.2656253576278687, time: 09:01:28, step: 168\nloss: 1.4453128576278687, time: 09:01:30, step: 169\nloss: 1.4609378576278687, time: 09:01:31, step: 170\nloss: 1.4609378576278687, time: 09:01:33, step: 171\nloss: 1.507812738418579, time: 09:01:34, step: 172\nloss: 1.398437738418579, time: 09:01:36, step: 173\nloss: 1.593750238418579, time: 09:01:37, step: 174\nloss: 1.585937738418579, time: 09:01:39, step: 175\nloss: 1.4765628576278687, time: 09:01:40, step: 176\nloss: 1.312500238418579, time: 09:01:42, step: 177\nloss: 1.554687738418579, time: 09:01:43, step: 178\nloss: 1.4218753576278687, time: 09:01:45, step: 179\nloss: 1.640625238418579, time: 09:01:46, step: 180\n----- Time -> 09:01:48 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4922\n----- Time -> 09:01:49 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5156\n----- Time -> 09:01:50 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5156\n----- Time -> 09:01:52 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4688\n----- Time -> 09:01:52 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4980472\nloss: 1.5703128576278687, time: 09:01:53, step: 181\nloss: 1.3984378576278687, time: 09:01:57, step: 182\nloss: 1.570312738418579, time: 09:02:01, step: 183\nloss: 1.2578128576278687, time: 09:02:03, step: 184\nloss: 1.4062503576278687, time: 09:02:04, step: 185\nloss: 1.4453128576278687, time: 09:02:06, step: 186\nloss: 1.523437738418579, time: 09:02:07, step: 187\nloss: 1.500000238418579, time: 09:02:09, step: 188\nloss: 1.414062738418579, time: 09:02:10, step: 189\nloss: 1.281250238418579, time: 09:02:12, step: 190\nloss: 1.382812738418579, time: 09:02:13, step: 191\nloss: 1.500000238418579, time: 09:02:15, step: 192\nloss: 1.648437738418579, time: 09:02:16, step: 193\nloss: 1.531250238418579, time: 09:02:18, step: 194\nloss: 1.375000238418579, time: 09:02:19, step: 195\nloss: 1.3359378576278687, time: 09:02:21, step: 196\nloss: 1.335937738418579, time: 09:02:22, step: 197\nloss: 1.4062503576278687, time: 09:02:24, step: 198\nloss: 1.437500238418579, time: 09:02:25, step: 199\nloss: 1.382812738418579, time: 09:02:27, step: 200\n----- Time -> 09:02:28 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4688\n----- Time -> 09:02:30 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5000\n----- Time -> 09:02:31 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5234\n----- Time -> 09:02:32 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4375\n----- Time -> 09:02:32 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4824221\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"want to continue training after 200 steps yes\n"},{"name":"stdout","text":"loss: 1.492187738418579, time: 09:02:38, step: 201\nloss: 1.429687738418579, time: 09:02:42, step: 202\nloss: 1.476562738418579, time: 09:02:46, step: 203\nloss: 1.468750238418579, time: 09:02:48, step: 204\nloss: 1.468750238418579, time: 09:02:49, step: 205\nloss: 1.390625238418579, time: 09:02:50, step: 206\nloss: 1.453125238418579, time: 09:02:52, step: 207\nloss: 1.359375238418579, time: 09:02:54, step: 208\nloss: 1.351562738418579, time: 09:02:55, step: 209\nloss: 1.390625238418579, time: 09:02:56, step: 210\nloss: 1.476562738418579, time: 09:02:58, step: 211\nloss: 1.3984378576278687, time: 09:03:00, step: 212\nloss: 1.2578128576278687, time: 09:03:01, step: 213\nloss: 1.4296878576278687, time: 09:03:02, step: 214\nloss: 1.421875238418579, time: 09:03:04, step: 215\nloss: 1.406250238418579, time: 09:03:06, step: 216\nloss: 1.617187738418579, time: 09:03:07, step: 217\nloss: 1.546875238418579, time: 09:03:09, step: 218\nloss: 1.445312738418579, time: 09:03:10, step: 219\nloss: 1.453125238418579, time: 09:03:12, step: 220\n----- Time -> 09:03:13 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4609\n----- Time -> 09:03:14 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4766\n----- Time -> 09:03:16 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5000\n----- Time -> 09:03:17 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4141\n----- Time -> 09:03:17 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4628909\nloss: 1.500000238418579, time: 09:03:18, step: 221\nloss: 1.429687738418579, time: 09:03:23, step: 222\nloss: 1.2500003576278687, time: 09:03:27, step: 223\nloss: 1.265625238418579, time: 09:03:28, step: 224\nloss: 1.398437738418579, time: 09:03:30, step: 225\nloss: 1.453125238418579, time: 09:03:31, step: 226\nloss: 1.531250238418579, time: 09:03:33, step: 227\nloss: 1.273437738418579, time: 09:03:34, step: 228\nloss: 1.351562738418579, time: 09:03:36, step: 229\nloss: 1.367187738418579, time: 09:03:37, step: 230\nloss: 1.320312738418579, time: 09:03:39, step: 231\nloss: 1.429687738418579, time: 09:03:40, step: 232\nloss: 1.382812738418579, time: 09:03:42, step: 233\nloss: 1.335937738418579, time: 09:03:43, step: 234\nloss: 1.453125238418579, time: 09:03:45, step: 235\nloss: 1.523437738418579, time: 09:03:46, step: 236\nloss: 1.421875238418579, time: 09:03:48, step: 237\nloss: 1.335937738418579, time: 09:03:49, step: 238\nloss: 1.437500238418579, time: 09:03:51, step: 239\nloss: 1.437500238418579, time: 09:03:52, step: 240\n----- Time -> 09:03:54 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n----- Time -> 09:03:55 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4688\n----- Time -> 09:03:56 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4922\n----- Time -> 09:03:58 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4063\n----- Time -> 09:03:58 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4550784\nloss: 1.460937738418579, time: 09:03:59, step: 241\nloss: 1.414062738418579, time: 09:04:03, step: 242\nloss: 1.382812738418579, time: 09:04:07, step: 243\nloss: 1.546875238418579, time: 09:04:09, step: 244\nloss: 1.406250238418579, time: 09:04:10, step: 245\nloss: 1.335937738418579, time: 09:04:12, step: 246\nloss: 1.203125238418579, time: 09:04:13, step: 247\nloss: 1.375000238418579, time: 09:04:15, step: 248\nloss: 1.367187738418579, time: 09:04:16, step: 249\nloss: 1.601562738418579, time: 09:04:18, step: 250\nloss: 1.328125238418579, time: 09:04:19, step: 251\nloss: 1.414062738418579, time: 09:04:21, step: 252\nloss: 1.382812738418579, time: 09:04:22, step: 253\nloss: 1.500000238418579, time: 09:04:24, step: 254\nloss: 1.437500238418579, time: 09:04:25, step: 255\nloss: 1.523437738418579, time: 09:04:27, step: 256\nloss: 1.507812738418579, time: 09:04:28, step: 257\nloss: 1.226562738418579, time: 09:04:30, step: 258\nloss: 1.156250238418579, time: 09:04:31, step: 259\nloss: 1.390625238418579, time: 09:04:33, step: 260\n----- Time -> 09:04:34 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n----- Time -> 09:04:36 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4609\n----- Time -> 09:04:37 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4844\n----- Time -> 09:04:38 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3984\n----- Time -> 09:04:38 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4492190\nloss: 1.320312738418579, time: 09:04:40, step: 261\nloss: 1.343750238418579, time: 09:04:44, step: 262\nloss: 1.328125238418579, time: 09:04:48, step: 263\nloss: 1.257812738418579, time: 09:04:50, step: 264\nloss: 1.406250238418579, time: 09:04:51, step: 265\nloss: 1.468750238418579, time: 09:04:53, step: 266\nloss: 1.414062738418579, time: 09:04:54, step: 267\nloss: 1.375000238418579, time: 09:04:56, step: 268\nloss: 1.429687738418579, time: 09:04:57, step: 269\nloss: 1.429687738418579, time: 09:04:59, step: 270\nloss: 1.406250238418579, time: 09:05:00, step: 271\nloss: 1.273437738418579, time: 09:05:02, step: 272\nloss: 1.453125238418579, time: 09:05:03, step: 273\nloss: 1.351562738418579, time: 09:05:05, step: 274\nloss: 1.351562738418579, time: 09:05:06, step: 275\nloss: 1.453125238418579, time: 09:05:07, step: 276\nloss: 1.445312738418579, time: 09:05:09, step: 277\nloss: 1.507812738418579, time: 09:05:10, step: 278\nloss: 1.414062738418579, time: 09:05:12, step: 279\nloss: 1.437500238418579, time: 09:05:13, step: 280\n----- Time -> 09:05:15 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n----- Time -> 09:05:16 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4453\n----- Time -> 09:05:18 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4766\n----- Time -> 09:05:19 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3828\n----- Time -> 09:05:19 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4394534\nloss: 1.437500238418579, time: 09:05:20, step: 281\nloss: 1.539062738418579, time: 09:05:25, step: 282\nloss: 1.585937738418579, time: 09:05:29, step: 283\nloss: 1.484375238418579, time: 09:05:30, step: 284\nloss: 1.328125238418579, time: 09:05:32, step: 285\nloss: 1.421875238418579, time: 09:05:33, step: 286\nloss: 1.351562738418579, time: 09:05:35, step: 287\nloss: 1.335937738418579, time: 09:05:36, step: 288\nloss: 1.296875238418579, time: 09:05:38, step: 289\nloss: 1.328125238418579, time: 09:05:39, step: 290\nloss: 1.429687738418579, time: 09:05:41, step: 291\nloss: 1.382812738418579, time: 09:05:42, step: 292\nloss: 1.304687738418579, time: 09:05:43, step: 293\nloss: 1.367187738418579, time: 09:05:45, step: 294\nloss: 1.375000238418579, time: 09:05:46, step: 295\nloss: 1.507812738418579, time: 09:05:48, step: 296\nloss: 1.367187738418579, time: 09:05:49, step: 297\nloss: 1.304687738418579, time: 09:05:51, step: 298\nloss: 1.320312738418579, time: 09:05:52, step: 299\nloss: 1.375000238418579, time: 09:05:54, step: 300\n----- Time -> 09:05:55 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4609\n----- Time -> 09:05:57 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4453\n----- Time -> 09:05:58 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4844\n----- Time -> 09:06:00 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3906\n----- Time -> 09:06:00 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4453127\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"want to continue training after 300 steps yes\n"},{"name":"stdout","text":"loss: 1.375000238418579, time: 09:06:21, step: 301\nloss: 1.335937738418579, time: 09:06:25, step: 302\nloss: 1.421875238418579, time: 09:06:29, step: 303\nloss: 1.304687738418579, time: 09:06:30, step: 304\nloss: 1.460937738418579, time: 09:06:32, step: 305\nloss: 1.500000238418579, time: 09:06:33, step: 306\nloss: 1.351562738418579, time: 09:06:35, step: 307\nloss: 1.414062738418579, time: 09:06:37, step: 308\nloss: 1.492187738418579, time: 09:06:38, step: 309\nloss: 1.406250238418579, time: 09:06:40, step: 310\nloss: 1.312500238418579, time: 09:06:41, step: 311\nloss: 1.414062738418579, time: 09:06:43, step: 312\nloss: 1.335937738418579, time: 09:06:44, step: 313\nloss: 1.328125238418579, time: 09:06:46, step: 314\nloss: 1.437500238418579, time: 09:06:47, step: 315\nloss: 1.351562738418579, time: 09:06:49, step: 316\nloss: 1.578125238418579, time: 09:06:50, step: 317\nloss: 1.367187738418579, time: 09:06:52, step: 318\nloss: 1.382812738418579, time: 09:06:53, step: 319\nloss: 1.445312738418579, time: 09:06:55, step: 320\n----- Time -> 09:06:56 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n----- Time -> 09:06:58 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4453\n----- Time -> 09:06:59 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4766\n----- Time -> 09:07:00 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3984\n----- Time -> 09:07:00 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4433596\nloss: 1.382812738418579, time: 09:07:02, step: 321\nloss: 1.367187738418579, time: 09:07:06, step: 322\nloss: 1.296875238418579, time: 09:07:10, step: 323\nloss: 1.242187738418579, time: 09:07:12, step: 324\nloss: 1.351562738418579, time: 09:07:13, step: 325\nloss: 1.398437738418579, time: 09:07:15, step: 326\nloss: 1.320312738418579, time: 09:07:16, step: 327\nloss: 1.343750238418579, time: 09:07:18, step: 328\nloss: 1.343750238418579, time: 09:07:19, step: 329\nloss: 1.351562738418579, time: 09:07:21, step: 330\nloss: 1.468750238418579, time: 09:07:22, step: 331\nloss: 1.296875238418579, time: 09:07:24, step: 332\nloss: 1.257812738418579, time: 09:07:25, step: 333\nloss: 1.546875238418579, time: 09:07:27, step: 334\nloss: 1.398437738418579, time: 09:07:28, step: 335\nloss: 1.593750238418579, time: 09:07:30, step: 336\nloss: 1.445312738418579, time: 09:07:31, step: 337\nloss: 1.367187738418579, time: 09:07:33, step: 338\nloss: 1.375000238418579, time: 09:07:34, step: 339\nloss: 1.359375238418579, time: 09:07:36, step: 340\n----- Time -> 09:07:37 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n----- Time -> 09:07:38 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4609\n----- Time -> 09:07:40 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4766\n----- Time -> 09:07:41 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3984\n----- Time -> 09:07:41 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4472659\nloss: 1.164062738418579, time: 09:07:43, step: 341\nloss: 1.476562738418579, time: 09:07:47, step: 342\nloss: 1.210937738418579, time: 09:07:51, step: 343\nloss: 1.351562738418579, time: 09:07:52, step: 344\nloss: 1.375000238418579, time: 09:07:54, step: 345\nloss: 1.398437738418579, time: 09:07:55, step: 346\nloss: 1.281250238418579, time: 09:07:57, step: 347\nloss: 1.289062738418579, time: 09:07:58, step: 348\nloss: 1.328125238418579, time: 09:08:00, step: 349\nloss: 1.187500238418579, time: 09:08:01, step: 350\nloss: 1.289062738418579, time: 09:08:03, step: 351\nloss: 1.476562738418579, time: 09:08:04, step: 352\nloss: 1.296875238418579, time: 09:08:06, step: 353\nloss: 1.367187738418579, time: 09:08:07, step: 354\nloss: 1.359375238418579, time: 09:08:09, step: 355\nloss: 1.406250238418579, time: 09:08:10, step: 356\nloss: 1.382812738418579, time: 09:08:12, step: 357\nloss: 1.421875238418579, time: 09:08:13, step: 358\nloss: 1.320312738418579, time: 09:08:15, step: 359\nloss: 1.335937738418579, time: 09:08:16, step: 360\n----- Time -> 09:08:18 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n----- Time -> 09:08:19 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4453\n----- Time -> 09:08:20 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4688\n----- Time -> 09:08:22 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3984\n----- Time -> 09:08:22 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4414065\nloss: 1.210937738418579, time: 09:08:23, step: 361\nloss: 1.312500238418579, time: 09:08:27, step: 362\nloss: 1.414062738418579, time: 09:08:31, step: 363\nloss: 1.398437738418579, time: 09:08:33, step: 364\nloss: 1.406250238418579, time: 09:08:34, step: 365\nloss: 1.320312738418579, time: 09:08:36, step: 366\nloss: 1.406250238418579, time: 09:08:37, step: 367\nloss: 1.359375238418579, time: 09:08:39, step: 368\nloss: 1.281250238418579, time: 09:08:40, step: 369\nloss: 1.328125238418579, time: 09:08:42, step: 370\nloss: 1.195312738418579, time: 09:08:43, step: 371\nloss: 1.281250238418579, time: 09:08:45, step: 372\nloss: 1.164062738418579, time: 09:08:46, step: 373\nloss: 1.375000238418579, time: 09:08:48, step: 374\nloss: 1.281250238418579, time: 09:08:49, step: 375\nloss: 1.570312738418579, time: 09:08:51, step: 376\nloss: 1.359375238418579, time: 09:08:52, step: 377\nloss: 1.445312738418579, time: 09:08:54, step: 378\nloss: 1.312500238418579, time: 09:08:55, step: 379\nloss: 1.265625238418579, time: 09:08:57, step: 380\n----- Time -> 09:08:58 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4609\n----- Time -> 09:08:59 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4297\n----- Time -> 09:09:01 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4766\n----- Time -> 09:09:02 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3906\n----- Time -> 09:09:02 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4394534\nloss: 1.437500238418579, time: 09:09:04, step: 381\nloss: 1.296875238418579, time: 09:09:08, step: 382\nloss: 1.382812738418579, time: 09:09:12, step: 383\nloss: 1.515625238418579, time: 09:09:13, step: 384\nloss: 1.398437738418579, time: 09:09:15, step: 385\nloss: 1.359375238418579, time: 09:09:16, step: 386\nloss: 1.367187738418579, time: 09:09:18, step: 387\nloss: 1.304687738418579, time: 09:09:19, step: 388\nloss: 1.367187738418579, time: 09:09:21, step: 389\nloss: 1.406250238418579, time: 09:09:22, step: 390\nloss: 1.445312738418579, time: 09:09:24, step: 391\nloss: 1.304687738418579, time: 09:09:25, step: 392\nloss: 1.382812738418579, time: 09:09:27, step: 393\nloss: 1.250000238418579, time: 09:09:28, step: 394\nloss: 1.375000238418579, time: 09:09:30, step: 395\nloss: 1.289062738418579, time: 09:09:31, step: 396\nloss: 1.429687738418579, time: 09:09:33, step: 397\nloss: 1.460937738418579, time: 09:09:34, step: 398\nloss: 1.343750238418579, time: 09:09:36, step: 399\nloss: 1.242187738418579, time: 09:09:37, step: 400\n----- Time -> 09:09:39 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4531\n----- Time -> 09:09:40 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4531\n----- Time -> 09:09:42 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4688\n----- Time -> 09:09:43 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3828\n----- Time -> 09:09:43 ----- Validation Batch Size -> 4 ----  Validation Loss -> 1.4394534\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"want to continue training after 400 steps no\n"},{"name":"stderr","text":"wandb: WARNING No program path found when generating artifact job source for a non-colab notebook run. See https://docs.wandb.ai/guides/launch/create-job\nwandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning_rate</td><td>▁▂▄▆███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>Validation_loss</td><td>█▇▅▅▃▄▂▃▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇█▇▅▃▃▃▃▂▂▂▂▂▁▂▂▂▁▁▂▂▁▂▂▂▁▂▁▂▂▁▁▂▁▂▁▂▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning_rate</td><td>0.0</td></tr><tr><td>Validation_loss</td><td>1.38281</td></tr><tr><td>train_loss</td><td>1.24219</td></tr><tr><td>train_step</td><td>400</td></tr><tr><td>val_step</td><td>80</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">apricot-cloud-8</strong> at: <a href='https://wandb.ai/fhai50032/Xlake-Coder/runs/bhjgc3g4' target=\"_blank\">https://wandb.ai/fhai50032/Xlake-Coder/runs/bhjgc3g4</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240229_084547-bhjgc3g4/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nprint('Loading the model on CPU')\nSTART=time.time()\nmodel = model.cpu()\nprint(f\"Loaded model on cpu in {time.time()-START} seconds \")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:09:58.342138Z","iopub.execute_input":"2024-02-29T09:09:58.342510Z","iopub.status.idle":"2024-02-29T09:12:07.731514Z","shell.execute_reply.started":"2024-02-29T09:09:58.342478Z","shell.execute_reply":"2024-02-29T09:12:07.730373Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Loading the model on CPU\nLoaded model on cpu in 129.38400077819824 seconds \n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"hf_token\") ##\nmodel.push_to_hub(\n    SAVED_MODEL, \n    tokenizer=tokenizer,\n    safe_serialization=True,\n    private=True,\n    create_pr=True,\n    max_shard_size=\"3GB\", \n    )\ntokenizer.push_to_hub(\n    SAVED_MODEL,\n    private=True, \n    \n    )","metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:12:12.417594Z","iopub.execute_input":"2024-02-29T09:12:12.417989Z","iopub.status.idle":"2024-02-29T09:12:22.131094Z","shell.execute_reply.started":"2024-02-29T09:12:12.417958Z","shell.execute_reply":"2024-02-29T09:12:22.130135Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"README.md: 100%|██████████| 5.18k/5.18k [00:00<00:00, 18.4MB/s]\nadapter_model.safetensors: 100%|██████████| 300M/300M [00:05<00:00, 54.2MB/s] \ntokenizer.json: 100%|██████████| 17.5M/17.5M [00:00<00:00, 42.9MB/s]\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/fhai50032/Gemma-Unaligned/commit/6df55c71cc91b149a6a7da601c23e01672715801', commit_message='Upload tokenizer', commit_description='', oid='6df55c71cc91b149a6a7da601c23e01672715801', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}
