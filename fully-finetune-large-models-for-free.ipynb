{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":121.512757,"end_time":"2023-11-04T12:34:07.401258","exception":false,"start_time":"2023-11-04T12:32:05.888501","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip3 install --upgrade transformers datasets sentencepiece peft pandas pyarrow -q\n","!pip install torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q # Updating torch since we need the latest version\n","!pip install torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n","!pip uninstall tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n","!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":13.608021,"end_time":"2023-11-04T12:34:21.013846","exception":false,"start_time":"2023-11-04T12:34:07.405825","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import datasets\n","import torch.optim as optim\n","import torch_xla.debug.profiler as xp\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\n","import torch_xla.distributed.parallel_loader as pl\n","import torch_xla.test.test_utils as test_utils\n","import torch\n","import torch.nn as nn\n","import re\n","import torch_xla.experimental.xla_sharding as xs\n","import torch_xla.core.xla_model as xm\n","from transformers import (\n","    GPTNeoXConfig, T5Config, LlamaConfig, AutoTokenizer, AutoModelForCausalLM, MistralConfig, Qwen2Config, GPT2Config, DataCollatorWithPadding, AutoConfig, AutoModelForSequenceClassification\n",") # You can use any of models with those configs (even flan T5 xxl!). Other models are not supported.\n","\n","from transformers import logging as hf_logging\n","import torch.nn.functional as F\n","import torch_xla.runtime as xr\n","\n","xr.use_spmd()\n","\n","import torch_xla.experimental.xla_sharding as xs # \"experimental\" prefix always means you're gonna have a good time LMAO\n","from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n","from torch_xla.experimental.xla_sharding import Mesh\n","\n","from peft import LoraConfig, TaskType, get_peft_model # If we wanna use peft. Quantazation requiers GPU though. You'll have to download already quantazed models\n","from spmd_util import partition_module                # You could experiment with using already quantazed models like 4bit/Llama-2-7b-Chat-GPTQ if you're feeling funny\n","from datasets import Dataset, load_dataset, concatenate_datasets\n","from dataclasses import dataclass\n","from tqdm import tqdm\n","\n","import transformers\n","import datasets\n","import pandas as pd\n","import numpy as np\n","from datasets import Dataset\n","from torch.utils.data import Dataset as TorchDataset\n","import torch.utils\n","try:\n","    !export USE_TORCH=True #If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\n","    os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n","    os.environ.pop('TPU_PROCESS_ADDRESSES')\n","    os.environ.pop('CLOUD_TPU_TASK_ID')\n","    hf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\n","except:\n","    pass\n","\n","\n","MAX_INPUT=2048\n","MODEL = \"mistralai/Mistral-7B-v0.1\" #You should be able to use 7B model with no changes! There should be enough HBM\n","SAVED_MODEL = \"Locutusque/Hercules-2.0-Mistral-7B\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ConversationDataset(TorchDataset):\n","    def __init__(self, tokenizer, max_length=512, dataset=None):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        messages = self.dataset[idx][\"conversations\"]\n","        text = \"\"\n","        for message in messages:\n","            role = message[\"from\"]\n","            if role == \"system\":\n","                text += f\"<|im_start|>system\\n{message['value']}<|im_end|>\\n\"\n","            if role == \"human\":\n","                text += f\"<|im_start|>user\\n{message['value']}<|im_end|>\\n\"\n","            if role == \"function-call\":\n","                text += f\"<|im_start|>call\\n{message['value']}<|im_end|>\\n\"\n","            if role == \"function-response\":\n","                text += f\"<|im_start|>function\\n{message['value']}<|im_end|>\\n\"\n","            if role ==\"gpt\":\n","                text += f\"<|im_start|>assistant\\n{message['value']}{self.tokenizer.eos_token}\"\n","        input_ids = self.tokenizer(text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding=\"max_length\", return_attention_mask=True, return_tensors=\"pt\")\n","        return {\n","            \"input_ids\": input_ids[\"input_ids\"].squeeze(0),\n","            \"labels\": input_ids[\"input_ids\"].squeeze(0),\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.547357,"end_time":"2023-11-04T12:34:26.034497","exception":false,"start_time":"2023-11-04T12:34:25.48714","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data = load_dataset(\"Locutusque/hercules-v2.0\", split=\"train[:100000]\")\n","val = load_dataset('Locutusque/hercules-v2.0', split=\"train[:100]\")"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":300.034782,"end_time":"2023-11-04T12:40:23.192281","exception":false,"start_time":"2023-11-04T12:35:23.157499","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained(MODEL, torch_dtype=torch.bfloat16, token=\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.027467,"end_time":"2023-11-04T12:40:23.239228","exception":false,"start_time":"2023-11-04T12:40:23.211761","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["FLAGS = {'MAX_INPUT': 512,\n","         'LOGGING_STEPS': 1,\n","         'NUM_EPOCHS': 1,\n","         'BATCH_SIZE': 8, #Making batch_size lower then 8 will result in slower training, but will allow for larger models\\context. Fortunately, we have 128GBs. Setting higher batch_size doesn't seem to improve time.\n","          'NUM_STEPS': len(train_data)} "]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.027766,"end_time":"2023-11-04T12:40:23.286172","exception":false,"start_time":"2023-11-04T12:40:23.258406","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_data = ConversationDataset(tokenizer, dataset=train_data, max_length=512)\n","val = ConversationDataset(tokenizer, dataset=val)\n","training_loader = torch.utils.data.DataLoader(train_data, batch_size=FLAGS[\"BATCH_SIZE\"], shuffle=True)\n","testing_loader = torch.utils.data.DataLoader(val, batch_size=FLAGS[\"BATCH_SIZE\"], shuffle=True)\n","\n","device = xm.xla_device()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":29.992732,"end_time":"2023-11-04T12:40:53.298154","exception":false,"start_time":"2023-11-04T12:40:23.305422","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def get_nb_trainable_parameters(model):\n","        r\"\"\"\n","        Returns the number of trainable parameters and number of all parameters in the model.\n","        \"\"\"\n","        trainable_params = 0\n","        all_param = 0\n","        for _, param in model.named_parameters():\n","            num_params = param.numel()\n","            # if using DS Zero 3 and the weights are initialized empty\n","            if num_params == 0 and hasattr(param, \"ds_numel\"):\n","                num_params = param.ds_numel\n","\n","            # Due to the design of 4bit linear layers from bitsandbytes\n","            # one needs to multiply the number of parameters by 2 to get\n","            # the correct number of parameters\n","            if param.__class__.__name__ == \"Params4bit\":\n","                num_params = num_params * 2\n","\n","            all_param += num_params\n","            if param.requires_grad:\n","                trainable_params += num_params\n","\n","        return trainable_params, all_param\n","def print_trainable_parameters(model):\n","        \"\"\"\n","        Prints the number of trainable parameters in the model.\n","        \"\"\"\n","        trainable_params, all_param = get_nb_trainable_parameters(model)\n","\n","        print(\n","            f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} || trainable%: {100 * trainable_params / all_param}\"\n","        )\n","\n","cnt = 0\n","for param in model.parameters():\n","    cnt += 1\n","    param.requires_grad = False\n","    if cnt > 0: # You can set this to a higher value to freeze parameters if your running out of memory.\n","        param.requires_grad = True\n","print_trainable_parameters(model)\n","config = AutoConfig.from_pretrained(MODEL)\n","num_devices = xr.global_runtime_device_count()\n","mesh_shape = (1, num_devices, 1)\n","device_ids = np.array(range(num_devices))\n","mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n","partition_module(model, mesh) # After this, the model is sharded between cores but still has the same API as if it was on single device. Neat."]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.912133,"end_time":"2023-11-04T12:40:55.230144","exception":false,"start_time":"2023-11-04T12:40:53.318011","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!export XLA_USE_BF16=1\n","\n","def train(FLAGS):\n","    num_iterations = int(FLAGS['NUM_STEPS'] / FLAGS['BATCH_SIZE'])\n","    lr = 2e-5\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=FLAGS['NUM_STEPS'] * FLAGS['BATCH_SIZE'])\n","    i = 0\n","    total_loss = 0\n","    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n","        model.train()\n","        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n","        for step, batch in enumerate(training_loader):\n","            optimizer.zero_grad()\n","            input_ids = batch[\"input_ids\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","            xs.mark_sharding(input_ids, mesh, (0, 1)) # Sharding inputs\n","            xs.mark_sharding(labels, mesh, (0, 1))\n","            outputs = model(input_ids=input_ids, labels=labels)\n","            logits = outputs.logits\n","            loss = outputs.loss\n","            loss.requires_grad_()\n","            loss.backward()\n","            optimizer.step()\n","            xm.mark_step()\n","            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n","                print(f'loss: {loss.item()}, time: {test_utils.now()}, step: {step}')\n"," \n","            scheduler.step()\n","            i += 1\n","\n","        model.eval()\n","        total_loss = 0.0\n","        total_steps = 0\n","\n","        with torch.no_grad():\n","            for step, batch in enumerate(testing_loader):\n","                input_ids = batch[\"input_ids\"].to(device)\n","                labels = batch[\"labels\"].to(device)\n","                xs.mark_sharding(input_ids, mesh, (0, 1))\n","                xs.mark_sharding(labels, mesh, (0, 1))\n","                outputs = model(input_ids=input_ids, labels=labels)\n","                loss = outputs.loss\n","                total_loss += loss.item()\n","                total_steps += 1\n","\n","        average_loss = total_loss / total_steps\n","        xm.master_print('Epoch {} test end {}, test loss={:.2f}'.format(epoch, test_utils.now(), average_loss))\n","        xm.master_print('Epoch {} train end {}'.format(epoch, test_utils.now()))"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"papermill":{"duration":2070.358248,"end_time":"2023-11-04T13:15:25.607979","exception":false,"start_time":"2023-11-04T12:40:55.249731","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train(FLAGS) # I haven't tested the evaluation part in this notebook so hopefully it works. It really should"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":388.410448,"end_time":"2023-11-04T13:21:54.038795","exception":false,"start_time":"2023-11-04T13:15:25.628347","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","from huggingface_hub import login\n","\n","user_secrets = UserSecretsClient()\n","hf_token = user_secrets.get_secret(\"hf_write\") # Provide your own HF API token with write access\n","login(hf_token)\n","model = model.cpu()\n","print('now saving the model')\n","model.push_to_hub(\n","    SAVED_MODEL, \n","    tokenizer=tokenizer,\n","    safe_serialization=True,\n","    private=True,\n","    create_pr=True,\n","    max_shard_size=\"2GB\", # Sharding isn't as important as before since hardware is better now but who cares anyway\n","    )# We have to push the model to HF since there is not enough memory on disk. Download weights from there\n","tokenizer.push_to_hub(\n","    SAVED_MODEL,\n","    private=True, \n","    \n","    )"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"datasetId":4050072,"sourceId":7566468,"sourceType":"datasetVersion"}],"dockerImageVersionId":30529,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.17"}},"nbformat":4,"nbformat_minor":4}
