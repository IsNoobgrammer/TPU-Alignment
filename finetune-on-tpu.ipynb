{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30647,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install transformers zstandard jsonlines peft wandb bitsandbytes -q\n!pip3 install datasets sentencepiece langchain torch_xla[tpuvm] -q\n!pip uninstall tensorflow -y #that's the meme part","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For Logging (Wandb)**","metadata":{}},{"cell_type":"code","source":"!huggingface-cli login --token \nimport wandb\nwandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sharding Module for different Architechture**","metadata":{}},{"cell_type":"code","source":"%%writefile spmd_util.py\nimport torch\nimport torch.nn as nn\nimport re\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n    GPTNeoXConfig, T5Config, LlamaConfig, GPT2Config, MistralConfig, Qwen2Config\n)\n\n# ends with $ to prevent sharding lora parameters\nGPTNEOX_RULES = (\n    # embeddings\n    (\"gpt_neox\\\\.embed_in\", (\"mp\", \"fsdp\")),\n    # atention\n    (\"attention\\\\.query_key_value$\", (\"fsdp\", \"mp\")),\n    (\"attention\\\\.dense$\", (\"mp\", \"fsdp\")),\n    # mlp\n    (\"mlp\\\\.dense_h_to_4h$\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.dense_4h_to_h$\", (\"mp\", \"fsdp\")),\n    # output\n    (\"embed_out\", (\"fsdp\", \"mp\")),\n)\n\nT5_RULES = (\n    # embeddings\n    (\"shared$\", (\"mp\", \"fsdp\")),\n    (\"embed_tokens$\", (\"mp\", \"fsdp\")),\n    \n    # attention\n    (\"q$\", (\"fsdp\", \"mp\")),\n    (\"k$\", (\"fsdp\", \"mp\")),\n    (\"v$\", (\"fsdp\", \"mp\")),\n    (\"o$\", (\"mp\", \"fsdp\")),\n\n    # mlp\n    (\"w$\", (\"fsdp\", \"mp\")),\n    (\"wi_0$\", (\"fsdp\", \"mp\")),\n    (\"wi_1$\", (\"fsdp\", \"mp\")),\n    (\"wo$\", (\"mp\", \"fsdp\")),\n\n    # seq2seq lm head\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n)\n\nLLAMA_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\nQWEN_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\nGPT2_RULES = (\n    # embeddings\n    (\"wte\", (\"mp\", \"fsdp\")), \n    (\"wpe\", (\"mp\", \"fsdp\")),\n    \n    # attention\n    (\"c_attn\", (\"fsdp\", \"mp\")),\n    (\"c_proj\", (\"mp\", \"fsdp\")),\n    \n    # mlp\n    (\"c_fc\", (\"fsdp\", \"mp\")), \n    (\"c_proj\", (\"mp\", \"fsdp\")),\n    \n    # output \n    (\"ln_f\", (\"fsdp\", \"mp\")),\n)\nMISTRAL_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\n    \nALL_RULES = [\n    (GPTNeoXConfig, GPTNEOX_RULES),\n    (T5Config, T5_RULES),\n    (LlamaConfig, LLAMA_RULES),\n    (GPT2Config, GPT2_RULES),\n    (MistralConfig, MISTRAL_RULES),\n    (Qwen2Config, QWEN_RULES)\n]\n\ndef find_rule(model):\n    for config, rule in ALL_RULES:\n        if model.config.__class__ == config:\n            return rule\n    raise Exception(\"unsupported model to partitioning\")\n\nstrkey2id = {\n    \"dp\": 0,\n    \"fsdp\": 1,\n    \"mp\": 2\n}\n\ndef partition_module(model, mesh, device=xm.xla_device(), verbose=False):\n    partition_specs = find_rule(model)\n    rule = [(k, tuple([strkey2id[x] for x in v])) for k, v in partition_specs]\n        \n    # print(rule)\n\n    for name, module in model.named_modules():\n        module.to(device)\n        # print(name, module.__class__.__name__)\n        if isinstance(module, (nn.Embedding, nn.Linear)):\n            for rule_pattern, spec in rule:\n                if re.findall(rule_pattern, name):\n                    if verbose:\n                        print(\"match\", rule_pattern, name)\n                    \n                    xs.mark_sharding(module.weight, mesh, spec)\n                    break\n        \ndef partition_module_dp(model, mesh, device=xm.xla_device(), verbose=False):\n    spec = (1, 2)\n\n    for name, module in model.named_modules():\n        module.to(device)\n        if isinstance(module, (nn.Embedding, nn.Linear)):\n            xs.mark_sharding(module.weight, mesh, spec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Required Libs**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport datasets\nimport torch.optim as optim\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.test.test_utils as test_utils\nimport torch\nimport torch.nn as nn\nimport re\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n    GPTNeoXConfig, T5Config, LlamaConfig, AutoTokenizer, AutoModelForCausalLM, MistralConfig, Qwen2Config, GPT2Config, DataCollatorWithPadding, AutoConfig, AutoModelForSequenceClassification\n) # You can use any of models with those configs (even flan T5 xxl!). Other models are not supported.\n\nfrom transformers import logging as hf_logging\nimport torch.nn.functional as F\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nimport torch_xla.experimental.xla_sharding as xs # \"experimental\" prefix always means you're gonna have a good time LMAO\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\n\nfrom peft import LoraConfig, TaskType, get_peft_model # If we wanna use peft. Quantazation requiers GPU though. You'll have to download already quantazed models\nfrom spmd_util import partition_module                # You could experiment with using already quantazed models like 4bit/Llama-2-7b-Chat-GPTQ if you're feeling funny\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\n\nimport transformers\nimport datasets\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom torch.utils.data import Dataset as TorchDataset\nimport torch.utils\ntry:\n    !export USE_TORCH=True #If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\n    os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n    os.environ.pop('TPU_PROCESS_ADDRESSES')\n    os.environ.pop('CLOUD_TPU_TASK_ID')\n    hf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\nexcept:\n    pass\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Configuration**","metadata":{}},{"cell_type":"code","source":"MAX_INPUT=1024\nMODEL = \"fhai50032/xLakeChat\" #You should be able to use 7B model with no changes! There should be enough HBM\nSAVED_MODEL = \"fhai50032/XLake-Coder\"\n# !export XLA_TENSOR_ALLOCATOR_MAXSIZE=1000000\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n# tokenizer.pad_token = tokenizer.eos_token","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConversationDataset(TorchDataset):\n    def __init__(self, tokenizer, max_length=1024, dataset=None):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        messages = self.dataset[idx][\"conversations\"]\n        text = \"\"\n        for message in messages:\n            role = message[\"from\"]\n            if role == \"system\":\n                text += f\"<|im_start|>system\\n{message['value']}<|im_end|>\\n\"\n            if role == \"human\":\n                text += f\"<|im_start|>user\\n{message['value']}<|im_end|>\\n\"\n            if role == \"function-call\":\n                text += f\"<|im_start|>call\\n{message['value']}<|im_end|>\\n\"\n            if role == \"function-response\":\n                text += f\"<|im_start|>function\\n{message['value']}<|im_end|>\\n\"\n            if role ==\"gpt\":\n                text += f\"<|im_start|>assistant\\n{message['value']}{self.tokenizer.eos_token}\"\n        input_ids = self.tokenizer(text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding=\"max_length\", return_attention_mask=True, return_tensors=\"pt\")\n        return {\n            \"input_ids\": input_ids[\"input_ids\"].squeeze(0),\n            \"labels\": input_ids[\"input_ids\"].squeeze(0),\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = load_dataset(\"fhai50032/magicoder-oss-instruct-sharegpt-75k\", split=\"train[:600]\").shuffle()\nval = (load_dataset('fhai50032/magicoder-oss-instruct-sharegpt-75k', split=\"train[:24]\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FLAGS = {'MAX_INPUT': MAX_INPUT,\n         'LOGGING_STEPS': 6,\n         'NUM_EPOCHS': 1,\n         'BATCH_SIZE': 6, #Making batch_size lower then 8 will result in slower training, but will allow for larger models\\context. Fortunately, we have 128GBs. Setting higher batch_size doesn't seem to improve time.\n          'NUM_STEPS': len(train_data)//3,\n        'GRAD_ACCUMALATION':2,\n        'LEARNING_RATE':2e-5,\n         'WARMUP_RATIO':0.1,\n         'OPTIMIZER':'AdamW',\n         'SCHEDULAR':'LINEAR',# or cosine \n         'WEIGHT_DECAY':0.0,\n        'PROJECT':'TPU-Training',\n        'RUN':'Silver-Pulse'} # Indian Pun ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FLAGS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(MODEL, torch_dtype=torch.bfloat16, token=\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## for debugging No-use\nprint(model)\nfor _, param in model.named_parameters():\n#     print(_)\n    if \"down_proj\" in str(_):\n        print(param)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LoRA Applicable**","metadata":{}},{"cell_type":"code","source":"ls=LoraConfig(\n    r = 32, # Lora Rank ,I would prefer 8-32 for smaller models like 7b\n    target_modules = ['v_proj', 'down_proj', 'up_proj', 'o_proj', 'q_proj', 'gate_proj', 'k_proj'],\n    lora_alpha = 48, #weight_scaling\n    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimize\n    # modules_to_save = [\"lm_head\", \"embed_tokens\"] ## if you use new chat formats or embedding tokens\n)\nmodel = get_peft_model(model, ls)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data-Distributer**","metadata":{}},{"cell_type":"code","source":"train_data = ConversationDataset(tokenizer, dataset=train_data, max_length=1024)\nval = ConversationDataset(tokenizer, dataset=val)\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(\n    train_data, num_replicas=8, rank=xm.get_ordinal(), shuffle=True)\ntraining_loader = torch.utils.data.DataLoader(train_data, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=train_sampler)\nval_sampler = torch.utils.data.distributed.DistributedSampler(\n    val, num_replicas=8, rank=xm.get_ordinal(), shuffle=True)\ntesting_loader = torch.utils.data.DataLoader(val, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=val_sampler)\n\ndevice = xm.xla_device()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_nb_trainable_parameters(model):\n        r\"\"\"\n        Returns the number of trainable parameters and number of all parameters in the model.\n        \"\"\"\n        trainable_params = 0\n        all_param = 0\n        for _, param in model.named_parameters():\n            num_params = param.numel()\n            # if using DS Zero 3 and the weights are initialized empty\n            if num_params == 0 and hasattr(param, \"ds_numel\"):\n                num_params = param.ds_numel\n\n            # Due to the design of 4bit linear layers from bitsandbytes\n            # one needs to multiply the number of parameters by 2 to get\n            # the correct number of parameters\n            if param.__class__.__name__ == \"Params4bit\":\n                num_params = num_params * 2\n\n            all_param += num_params\n            if param.requires_grad:\n                trainable_params += num_params\n\n        return trainable_params, all_param\ndef print_trainable_parameters(model):\n        \"\"\"\n        Prints the number of trainable parameters in the model.\n        \"\"\"\n        trainable_params, all_param = get_nb_trainable_parameters(model)\n\n        print(\n            f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} || trainable%: {100 * trainable_params / all_param}\"\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnt = 0\nfor param in model.parameters():\n    cnt += 1\n    param.requires_grad = False\n    if cnt > 0: # You can set this to a higher value to freeze parameters if your running out of memory.\n        param.requires_grad = True\nprint_trainable_parameters(model)\nconfig = AutoConfig.from_pretrained(MODEL)\nnum_devices = xr.global_runtime_device_count()\nmesh_shape = (1, num_devices, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\npartition_module(model, mesh) # After this, the model is sharded between cores but still has the same API as if it was on single device. Neat.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export XLA_USE_BF16=1\n\ndef train(FLAGS):\n#     num_iterations = int(FLAGS['NUM_STEPS'] / 8)\n    lr = FLAGS['LEARNING_RATE']\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=FLAGS['NUM_STEPS'] * FLAGS['BATCH_SIZE'])\n    i = 0\n    total_loss = 0\n    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n        model.train()\n        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n        for step, batch in enumerate(training_loader):\n            optimizer.zero_grad()\n            input_ids = batch[\"input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            xs.mark_sharding(input_ids, mesh, (0, 1)) # Sharding inputs\n            xs.mark_sharding(labels, mesh, (0, 1))\n            outputs = model(input_ids=input_ids, labels=labels)\n            logits = outputs.logits\n            loss = outputs.loss\n            loss.requires_grad_()\n            loss.backward()\n            optimizer.step()\n            xm.mark_step()\n            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n                print(f'loss: {loss.item()}, time: {test_utils.now()}, step: {step+1}')\n \n            scheduler.step()\n            i += 1\n\n        model.eval()\n        total_loss = 0.0\n        total_steps = 0\n\n        with torch.no_grad():\n            for step, batch in enumerate(testing_loader):\n                input_ids = batch[\"input_ids\"].to(device)\n                labels = batch[\"labels\"].to(device)\n                xs.mark_sharding(input_ids, mesh, (0, 1))\n                xs.mark_sharding(labels, mesh, (0, 1))\n                outputs = model(input_ids=input_ids, labels=labels)\n                loss = outputs.loss\n                total_loss += loss.item()\n                total_steps += 1\n\n        average_loss = total_loss / total_steps\n        xm.master_print('Epoch {} test end {}, test loss={:.4f}'.format(epoch, test_utils.now(), average_loss))\n        xm.master_print('Epoch {} train end {}'.format(epoch, test_utils.now()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(FLAGS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export XLA_USE_BF16=1\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\ndef train(FLAGS):\n#     num_iterations = int(FLAGS['NUM_STEPS'] / 8)\n\n\n    num_iterations = int(FLAGS['NUM_STEPS'] /  FLAGS['GRAD_ACCUMALATION'])\n    warmup_steps = int(num_iterations * FLAGS['WARMUP_RATIO'])\n    if (FLAGS['OPTIMIZER']).lower()=='adamw':\n        optimizer = AdamW(model.parameters(), eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'])\n    else:\n        pass #TODO\n    \n    \n    \n    if (FLAGS['SCHEDULAR']).lower()=='linear':\n        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n    else:\n        schedular = get_cosine_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n        \n        \n        \n    global_step = 0\n    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n        model.train()\n        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n        for step, batch in enumerate(training_loader):\n            input_ids, labels = batch[\"input_ids\"].to(device),  batch[\"labels\"].to(device)\n            xs.mark_sharding(input_ids, mesh, (0, 1))\n            xs.mark_sharding(labels, mesh, (0, 1))\n            outputs = model(input_ids=input_ids, labels=labels)\n            loss = outputs.loss\n\n            if (global_step + 1) % FLAGS['GRAD_ACCUMALATION'] != 0:\n                loss = loss / (FLAGS['GRAD_ACCUMALATION'] + scheduler.get_last_lr()[0]) # my touch for grad_norm\n                \n            loss.backward()\n            \n            \n            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n                print(f'loss: {loss.item()}, time: {test_utils.now()}, step: {step+1}')\n            \n            \n            if (global_step + 1) % FLAGS['GRAD_ACCUMALATION'] == 0 or (global_step + 1 == len(training_loader)): # makes sure that data from the last batches are not discarded\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                global_step += 1\n                \n                \n            xm.mark_step()\n            \n        \n        xm.master_print('Epoch {} train end {}'.format(epoch, test_utils.now()))\n            \n\n        \n        \n        \n        \n\n        model.eval()\n        total_loss = 0.0\n        total_steps = 0\n        with torch.no_grad():\n            for step, batch in enumerate(testing_loader):\n                input_ids = batch[\"input_ids\"].to(device)\n                labels = batch[\"labels\"].to(device)\n                xs.mark_sharding(input_ids, mesh, (0, 1))\n                xs.mark_sharding(labels, mesh, (0, 1))\n                outputs = model(input_ids=input_ids, labels=labels)\n                loss = outputs.loss\n                total_loss += loss.item()\n                total_steps += 1\n\n        average_loss = total_loss / total_steps\n        xm.master_print('----- Time -> {} ----- Validation Steps -> {} ----  Validation Loss -> {:.4f}'.format(test_utils.now(), total_steps , average_loss))\n#         ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(FLAGS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_wandb(FLAGS,loss,steps,batch_step):\n    wandb.log({\"Train Loss\": loss.item(), \"Global Step\": global_step})\n    wandb.log({\"Epoch Number\": epoch, \"Time End Epoch\": test_utils.now()})\n    \n    return \"Humpe toh hai hi naw\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from kaggle_secrets import UserSecretsClient\n\n\n# user_secrets = UserSecretsClient()\n# hf_token = user_secrets.get_secret(\"hf_write\") # Provide your own HF API token with write access\n\nmodel = model.cpu()\nprint('now saving the model')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"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\")\nmodel.push_to_hub(\n    SAVED_MODEL, \n    tokenizer=tokenizer,\n    safe_serialization=True,\n    private=True,\n    create_pr=True,\n    max_shard_size=\"3GB\", # Sharding isn't as important as before since hardware is better now but who cares anyway\n    )# We have to push the model to HF since there is not enough memory on disk. Download weights from there\ntokenizer.push_to_hub(\n    SAVED_MODEL,\n    private=True, \n    \n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}