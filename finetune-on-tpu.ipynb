{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30647,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install transformers zstandard jsonlines peft wandb bitsandbytes lion-pytorch -q\n!pip3 install accelerate datasets sentencepiece langchain torch_xla[tpuvm] -q\n!pip uninstall tensorflow -y #that's the meme part","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For Logging (Wandb)**","metadata":{}},{"cell_type":"code","source":"!huggingface-cli login --token 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\nimport wandb\nwandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sharding Module for different Architechture**","metadata":{}},{"cell_type":"code","source":"%%writefile spmd_util.py\nimport torch\nimport torch.nn as nn\nimport re\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n    GPTNeoXConfig, T5Config, LlamaConfig, GPT2Config, MistralConfig, Qwen2Config\n)\n\n# ends with $ to prevent sharding lora parameters\nGPTNEOX_RULES = (\n    # embeddings\n    (\"gpt_neox\\\\.embed_in\", (\"mp\", \"fsdp\")),\n    # atention\n    (\"attention\\\\.query_key_value$\", (\"fsdp\", \"mp\")),\n    (\"attention\\\\.dense$\", (\"mp\", \"fsdp\")),\n    # mlp\n    (\"mlp\\\\.dense_h_to_4h$\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.dense_4h_to_h$\", (\"mp\", \"fsdp\")),\n    # output\n    (\"embed_out\", (\"fsdp\", \"mp\")),\n)\n\nT5_RULES = (\n    # embeddings\n    (\"shared$\", (\"mp\", \"fsdp\")),\n    (\"embed_tokens$\", (\"mp\", \"fsdp\")),\n    \n    # attention\n    (\"q$\", (\"fsdp\", \"mp\")),\n    (\"k$\", (\"fsdp\", \"mp\")),\n    (\"v$\", (\"fsdp\", \"mp\")),\n    (\"o$\", (\"mp\", \"fsdp\")),\n\n    # mlp\n    (\"w$\", (\"fsdp\", \"mp\")),\n    (\"wi_0$\", (\"fsdp\", \"mp\")),\n    (\"wi_1$\", (\"fsdp\", \"mp\")),\n    (\"wo$\", (\"mp\", \"fsdp\")),\n\n    # seq2seq lm head\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n)\n\nLLAMA_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\nQWEN_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\nGPT2_RULES = (\n    # embeddings\n    (\"wte\", (\"mp\", \"fsdp\")), \n    (\"wpe\", (\"mp\", \"fsdp\")),\n    \n    # attention\n    (\"c_attn\", (\"fsdp\", \"mp\")),\n    (\"c_proj\", (\"mp\", \"fsdp\")),\n    \n    # mlp\n    (\"c_fc\", (\"fsdp\", \"mp\")), \n    (\"c_proj\", (\"mp\", \"fsdp\")),\n    \n    # output \n    (\"ln_f\", (\"fsdp\", \"mp\")),\n)\nMISTRAL_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\n    \nALL_RULES = [\n    (GPTNeoXConfig, GPTNEOX_RULES),\n    (T5Config, T5_RULES),\n    (LlamaConfig, LLAMA_RULES),\n    (GPT2Config, GPT2_RULES),\n    (MistralConfig, MISTRAL_RULES),\n    (Qwen2Config, QWEN_RULES)\n]\n\ndef find_rule(model):\n    for config, rule in ALL_RULES:\n        if model.config.__class__ == config:\n            return rule\n    raise Exception(\"unsupported model to partitioning\")\n\nstrkey2id = {\n    \"dp\": 0,\n    \"fsdp\": 1,\n    \"mp\": 2\n}\n\ndef partition_module(model, mesh, device=xm.xla_device(), verbose=False):\n    partition_specs = find_rule(model)\n    rule = [(k, tuple([strkey2id[x] for x in v])) for k, v in partition_specs]\n        \n    # print(rule)\n\n    for name, module in model.named_modules():\n        module.to(device)\n        # print(name, module.__class__.__name__)\n        if isinstance(module, (nn.Embedding, nn.Linear)):\n            for rule_pattern, spec in rule:\n                if re.findall(rule_pattern, name):\n                    if verbose:\n                        print(\"match\", rule_pattern, name)\n                    \n                    xs.mark_sharding(module.weight, mesh, spec)\n                    break\n        \ndef partition_module_dp(model, mesh, device=xm.xla_device(), verbose=False):\n    spec = (1, 2)\n\n    for name, module in model.named_modules():\n        module.to(device)\n        if isinstance(module, (nn.Embedding, nn.Linear)):\n            xs.mark_sharding(module.weight, mesh, spec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Required Libs**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport datasets\nimport torch.optim as optim\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.test.test_utils as test_utils\nimport torch\nimport torch.nn as nn\nimport re\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n    GPTNeoXConfig, T5Config, LlamaConfig, AutoTokenizer, AutoModelForCausalLM, MistralConfig, Qwen2Config, GPT2Config, DataCollatorWithPadding, AutoConfig, AutoModelForSequenceClassification\n) # You can use any of models with those configs (even flan T5 xxl!). Other models are not supported.\n\nfrom transformers import logging as hf_logging\nimport torch.nn.functional as F\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nimport torch_xla.experimental.xla_sharding as xs # \"experimental\" prefix always means you're gonna have a good time LMAO\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\n\nfrom peft import LoraConfig, TaskType, get_peft_model # If we wanna use peft. Quantazation requiers GPU though. You'll have to download already quantazed models\nfrom spmd_util import partition_module                # You could experiment with using already quantazed models like 4bit/Llama-2-7b-Chat-GPTQ if you're feeling funny\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\n\nimport transformers\nimport datasets\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom torch.utils.data import Dataset as TorchDataset\nimport torch.utils\ntry:\n    !export USE_TORCH=True #If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\n    os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n    os.environ.pop('TPU_PROCESS_ADDRESSES')\n    os.environ.pop('CLOUD_TPU_TASK_ID')\n    hf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\nexcept:\n    pass\n","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:05:42.051373Z","iopub.execute_input":"2024-02-14T13:05:42.052310Z","iopub.status.idle":"2024-02-14T13:05:47.054656Z","shell.execute_reply.started":"2024-02-14T13:05:42.052261Z","shell.execute_reply":"2024-02-14T13:05:47.053408Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Configuration**","metadata":{}},{"cell_type":"code","source":"MAX_INPUT=1024\nMODEL = \"fhai50032/BeagleLake-7B\" #You should be able to use 7B model with no changes! There should be enough HBM\nSAVED_MODEL = \"fhai50032/BeagleLake-Coder\"\n# !export XLA_TENSOR_ALLOCATOR_MAXSIZE=1000000\n","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:17:59.438942Z","iopub.execute_input":"2024-02-14T13:17:59.439327Z","iopub.status.idle":"2024-02-14T13:17:59.443976Z","shell.execute_reply.started":"2024-02-14T13:17:59.439295Z","shell.execute_reply":"2024-02-14T13:17:59.443142Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nif 'pad_token' not in tokenizer.special_tokens_map:\n  tokenizer.pad_token=tokenizer.eos_token\n\n\nprint(f\"Tokens :\\n {tokenizer.special_tokens_map} \\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:17:56.706457Z","iopub.execute_input":"2024-02-14T13:17:56.706840Z","iopub.status.idle":"2024-02-14T13:17:56.891093Z","shell.execute_reply.started":"2024-02-14T13:17:56.706809Z","shell.execute_reply":"2024-02-14T13:17:56.890012Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Tokens :\n {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<unk>', '<s>', '</s>']} \n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"class ConversationDataset(TorchDataset):\n    def __init__(self, tokenizer, max_length=1024, dataset=None):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        messages = self.dataset[idx][\"conversations\"]\n        text = \"\"\n        for message in messages:\n            role = message[\"from\"]\n            if role == \"system\":\n                text += f\"<|im_start|>system\\n{message['value']}<|im_end|>\\n\"\n            if role == \"human\":\n                text += f\"<|im_start|>user\\n{message['value']}<|im_end|>\\n\"\n            if role == \"function-call\":\n                text += f\"<|im_start|>call\\n{message['value']}<|im_end|>\\n\"\n            if role == \"function-response\":\n                text += f\"<|im_start|>function\\n{message['value']}<|im_end|>\\n\"\n            if role ==\"gpt\":\n                text += f\"<|im_start|>assistant\\n{message['value']}{self.tokenizer.eos_token}\"\n        input_ids = self.tokenizer(text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding=\"max_length\", return_attention_mask=True, return_tensors=\"pt\")\n        return {\n            \"input_ids\": input_ids[\"input_ids\"].squeeze(0),\n            \"labels\": input_ids[\"input_ids\"].squeeze(0),\n        }","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:17:53.393787Z","iopub.execute_input":"2024-02-14T13:17:53.394176Z","iopub.status.idle":"2024-02-14T13:17:53.402275Z","shell.execute_reply.started":"2024-02-14T13:17:53.394143Z","shell.execute_reply":"2024-02-14T13:17:53.401260Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"train_data = load_dataset(\"fhai50032/magicoder-oss-instruct-sharegpt-75k\", split=\"train[1000:2600]\").shuffle()\nval = (load_dataset('fhai50032/magicoder-oss-instruct-sharegpt-75k', split=\"train[:24]\"))","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:28:49.317073Z","iopub.execute_input":"2024-02-14T13:28:49.317448Z","iopub.status.idle":"2024-02-14T13:28:51.301049Z","shell.execute_reply.started":"2024-02-14T13:28:49.317417Z","shell.execute_reply":"2024-02-14T13:28:51.300238Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:28:51.806771Z","iopub.execute_input":"2024-02-14T13:28:51.807148Z","iopub.status.idle":"2024-02-14T13:28:51.812496Z","shell.execute_reply.started":"2024-02-14T13:28:51.807116Z","shell.execute_reply":"2024-02-14T13:28:51.811609Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"1600"},"metadata":{}}]},{"cell_type":"code","source":"FLAGS = {'MAX_INPUT': MAX_INPUT,\n         'LOGGING_STEPS': 1,\n         'NUM_EPOCHS': 1,\n         'BATCH_SIZE': 8, #Making batch_size lower then 8 will result in slower training, but will allow for larger models\\context. Fortunately, we have 128GBs. Setting higher batch_size doesn't seem to improve time.\n          'NUM_STEPS': len(train_data),\n        'GRAD_ACCUMULATION':1,\n        'LEARNING_RATE':2e-5,\n         'WARMUP_RATIO':0.1,\n         'OPTIMIZER':'adamw', # default = 'adamw8bit'  options->  ['adamw','adamw8bit','adafactor','lion']           \n         'SCHEDULAR':'LINEAR', # default= 'cosine'     options:-> ['linear','cosine']\n         'WEIGHT_DECAY':0.0,\n         'MAX_GRAD_CLIP':1.0,\n        'PROJECT':'TPU-Training',\n        'RUN':'Silver-Pulse'} # Indian pun :) ","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:28:55.373949Z","iopub.execute_input":"2024-02-14T13:28:55.374315Z","iopub.status.idle":"2024-02-14T13:28:55.379250Z","shell.execute_reply.started":"2024-02-14T13:28:55.374287Z","shell.execute_reply":"2024-02-14T13:28:55.378442Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"FLAGS","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:28:57.517802Z","iopub.execute_input":"2024-02-14T13:28:57.518153Z","iopub.status.idle":"2024-02-14T13:28:57.523311Z","shell.execute_reply.started":"2024-02-14T13:28:57.518125Z","shell.execute_reply":"2024-02-14T13:28:57.522501Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"{'MAX_INPUT': 1024,\n 'LOGGING_STEPS': 1,\n 'NUM_EPOCHS': 1,\n 'BATCH_SIZE': 8,\n 'NUM_STEPS': 1600,\n 'GRAD_ACCUMALATION': 1,\n 'LEARNING_RATE': 2e-05,\n 'WARMUP_RATIO': 0.1,\n 'OPTIMIZER': 'adamw',\n 'SCHEDULAR': 'LINEAR',\n 'WEIGHT_DECAY': 0.0,\n 'MAX_GRAD_CLIP': 1.0,\n 'PROJECT': 'TPU-Training',\n 'RUN': 'Silver-Pulse'}"},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(MODEL, torch_dtype=torch.bfloat16, token=\"\")","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:16:31.609937Z","iopub.execute_input":"2024-02-14T13:16:31.610302Z","iopub.status.idle":"2024-02-14T13:16:36.210035Z","shell.execute_reply.started":"2024-02-14T13:16:31.610271Z","shell.execute_reply":"2024-02-14T13:16:36.209220Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  2.74it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"## for debugging No-use\nprint(model)\nfor _, param in model.named_parameters():\n#     print(_)\n    if \"down_proj\" in str(_):\n        print(param)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LoRA Applicable**","metadata":{}},{"cell_type":"code","source":"ls=LoraConfig(\n    r = 32, # Lora Rank ,I would prefer 8-32 for smaller models like 7b\n    target_modules = ['v_proj', 'down_proj', 'up_proj', 'o_proj', 'q_proj', 'gate_proj', 'k_proj'],\n    lora_alpha = 48, #weight_scaling\n    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimize\n    # modules_to_save = [\"lm_head\", \"embed_tokens\"] ## if you use new chat formats or embedding tokens\n)\nmodel = get_peft_model(model, ls)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:16:41.609652Z","iopub.execute_input":"2024-02-14T13:16:41.610037Z","iopub.status.idle":"2024-02-14T13:16:43.430140Z","shell.execute_reply.started":"2024-02-14T13:16:41.610009Z","shell.execute_reply":"2024-02-14T13:16:43.429358Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n","output_type":"stream"},{"name":"stdout","text":"/usr/local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\ntrainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451058188485088\n","output_type":"stream"}]},{"cell_type":"markdown","source":"***'USE ONLY IF NOT USING LoRA'*** *(SETS ALL PARAMS TO BE TRAINABLE)*","metadata":{}},{"cell_type":"code","source":"cnt = 0\nfor param in model.parameters():\n    cnt += 1\n    param.requires_grad = False\n    if cnt > 0: # You can set this to a higher value to freeze parameters if your running out of memory.\n        param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:06:49.291011Z","iopub.execute_input":"2024-02-14T13:06:49.291500Z","iopub.status.idle":"2024-02-14T13:06:49.297807Z","shell.execute_reply.started":"2024-02-14T13:06:49.291462Z","shell.execute_reply":"2024-02-14T13:06:49.296994Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Data-Distributer**","metadata":{}},{"cell_type":"code","source":"train_data = ConversationDataset(tokenizer, dataset=train_data, max_length=1024)\nval = ConversationDataset(tokenizer, dataset=val)\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(\n    train_data, num_replicas=8, rank=xm.get_ordinal(), shuffle=True)\ntraining_loader = torch.utils.data.DataLoader(train_data, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=train_sampler)\nval_sampler = torch.utils.data.distributed.DistributedSampler(\n    val, num_replicas=8, rank=xm.get_ordinal(), shuffle=True)\ntesting_loader = torch.utils.data.DataLoader(val, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=val_sampler)\n\nprint(len(training_loader))","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:29:05.174725Z","iopub.execute_input":"2024-02-14T13:29:05.175114Z","iopub.status.idle":"2024-02-14T13:29:05.182841Z","shell.execute_reply.started":"2024-02-14T13:29:05.175084Z","shell.execute_reply":"2024-02-14T13:29:05.182012Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"25\n","output_type":"stream"}]},{"cell_type":"code","source":"device = xm.xla_device()\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:16:53.535098Z","iopub.execute_input":"2024-02-14T13:16:53.535485Z","iopub.status.idle":"2024-02-14T13:16:53.539798Z","shell.execute_reply.started":"2024-02-14T13:16:53.535452Z","shell.execute_reply":"2024-02-14T13:16:53.539034Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"xla:0\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_nb_trainable_parameters(model):\n        r\"\"\"\n        Returns the number of trainable parameters and number of all parameters in the model.\n        \"\"\"\n        trainable_params = 0\n        all_param = 0\n        for _, param in model.named_parameters():\n            num_params = param.numel()\n            # if using DS Zero 3 and the weights are initialized empty\n            if num_params == 0 and hasattr(param, \"ds_numel\"):\n                num_params = param.ds_numel\n\n            # Due to the design of 4bit linear layers from bitsandbytes\n            # one needs to multiply the number of parameters by 2 to get\n            # the correct number of parameters\n            if param.__class__.__name__ == \"Params4bit\":\n                num_params = num_params * 2\n\n            all_param += num_params\n            if param.requires_grad:\n                trainable_params += num_params\n\n        return trainable_params, all_param\ndef print_trainable_parameters(model):\n        \"\"\"\n        Prints the number of trainable parameters in the model.\n        \"\"\"\n        trainable_params, all_param = get_nb_trainable_parameters(model)\n\n        print(\n            f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} || trainable%: {100 * trainable_params / all_param}\"\n        )","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:16:56.405940Z","iopub.execute_input":"2024-02-14T13:16:56.406344Z","iopub.status.idle":"2024-02-14T13:16:56.412968Z","shell.execute_reply.started":"2024-02-14T13:16:56.406312Z","shell.execute_reply":"2024-02-14T13:16:56.412079Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:16:59.548106Z","iopub.execute_input":"2024-02-14T13:16:59.548949Z","iopub.status.idle":"2024-02-14T13:16:59.560464Z","shell.execute_reply.started":"2024-02-14T13:16:59.548916Z","shell.execute_reply":"2024-02-14T13:16:59.559593Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451058188485088\n","output_type":"stream"}]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(MODEL)\nnum_devices = xr.global_runtime_device_count()\nmesh_shape = (1, num_devices, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\npartition_module(model, mesh) # After this, the model is sharded between cores but still has the same API as if it was on single device. Neat.","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:17:02.164663Z","iopub.execute_input":"2024-02-14T13:17:02.165554Z","iopub.status.idle":"2024-02-14T13:17:12.854581Z","shell.execute_reply.started":"2024-02-14T13:17:02.165514Z","shell.execute_reply":"2024-02-14T13:17:12.853453Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# help(mesh)\nmesh.shape()","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:17:15.783355Z","iopub.execute_input":"2024-02-14T13:17:15.783761Z","iopub.status.idle":"2024-02-14T13:17:15.789656Z","shell.execute_reply.started":"2024-02-14T13:17:15.783730Z","shell.execute_reply":"2024-02-14T13:17:15.788682Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"OrderedDict([('dp', 1), ('fsdp', 8), ('mp', 1)])"},"metadata":{}}]},{"cell_type":"code","source":"!export XLA_USE_BF16=1\n\ndef train(FLAGS):\n#     num_iterations = int(FLAGS['NUM_STEPS'] / 8)\n    lr = FLAGS['LEARNING_RATE']\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=FLAGS['NUM_STEPS'] * FLAGS['BATCH_SIZE'])\n    i = 0\n    total_loss = 0\n    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n        model.train()\n        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n        for step, batch in enumerate(training_loader):\n            optimizer.zero_grad()\n            input_ids = batch[\"input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            xs.mark_sharding(input_ids, mesh, (0, 1)) # Sharding inputs\n            xs.mark_sharding(labels, mesh, (0, 1))\n            outputs = model(input_ids=input_ids, labels=labels)\n            logits = outputs.logits\n            loss = outputs.loss\n            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n                print(f'loss: {loss.item()}, time: {test_utils.now()}, step: {step+1}')\n            loss.requires_grad_()\n            loss.backward()\n            optimizer.step()\n            xm.mark_step()\n            \n \n            scheduler.step()\n            i += 1\n\n        model.eval()\n        total_loss = 0.0\n        total_steps = 0\n\n        with torch.no_grad():\n            for step, batch in enumerate(testing_loader):\n                input_ids = batch[\"input_ids\"].to(device)\n                labels = batch[\"labels\"].to(device)\n                xs.mark_sharding(input_ids, mesh, (0, 1))\n                xs.mark_sharding(labels, mesh, (0, 1))\n                outputs = model(input_ids=input_ids, labels=labels)\n                loss = outputs.loss\n                total_loss += loss.item()\n                total_steps += 1\n\n        average_loss = total_loss / total_steps\n        xm.master_print('Epoch {} test end {}, test loss={:.4f}'.format(epoch, test_utils.now(), average_loss))\n        xm.master_print('Epoch {} train end {}'.format(epoch, test_utils.now()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(FLAGS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export XLA_USE_BF16=1\nfrom transformers import AdamW,Adafactor\nfrom lion_pytorch import Lion #optimizer best used for large batch size ~ 4096+\nfrom transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\nfrom bitsandbytes import AdamW8bit \n\ndef train(FLAGS):\n\n    \n    ### Configuring Training\n    \n    update_params= filter(lambda p: p.requires_grad, model.parameters())\n    num_iterations = int(FLAGS['NUM_STEPS'] /  FLAGS['BATCH_SIZE'] * FLAGS['GRAD_ACCUMULATION'])\n    warmup_steps = int(num_iterations * FLAGS['WARMUP_RATIO'])\n    \n    \n    \n    ### Optimizers\n    \n    if (FLAGS['OPTIMIZER']).lower()=='adamw':\n        optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'])\n    elif (FLAGS['OPTIMIZER']).lower()=='lion':\n        optimizer = Lion(filter(lambda p: p.requires_grad, model.parameters()), lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'])\n    elif (FLAGS['OPTIMIZER']).lower()=='adafactor':\n        optimizer = Adafactor(filter(lambda p: p.requires_grad, model.parameters()),lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'],scale_parameter=True,relative_step=False)\n    else:\n        optimizer = AdamW8bit(filter(lambda p: p.requires_grad, model.parameters()), eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'])\n\n    \n    \n    ### Schedulars\n    \n    if (FLAGS['SCHEDULAR']).lower()=='linear':\n        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n    else:\n        schedular = get_cosine_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n        \n        \n    \n    \n    ### Training Loop\n    \n    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n        model.train()\n        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n        for step, batch in enumerate(training_loader):\n            input_ids, labels = batch[\"input_ids\"].to(device),  batch[\"labels\"].to(device)\n            xs.mark_sharding(input_ids, mesh, (0, 1))\n            xs.mark_sharding(labels, mesh, (0, 1))\n            outputs = model(input_ids=input_ids, labels=labels)\n            \n            \n            optimizer.zero_grad()\n            loss = outputs.loss\n            \n#           loss = loss / (FLAGS['GRAD_ACCUMULATION'] + scheduler.get_last_lr()[0]) # my touch for grad_norm\n\n\n\n            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n                print(f'loss: {loss.item()}, time: {test_utils.now()}, step: {step+1}')\n            \n            loss.requires_grad_()\n            loss.backward()\n            optimizer.step()\n            xm.mark_step()\n            scheduler.step()\n            xm.master_print('Epoch {} train end {}'.format(epoch, test_utils.now()))\n            \n            \n            \n            ###TODO Gradient Accumulation with model syncing\n#             if (step + 1) % FLAGS['GRAD_ACCUMULATION'] == 0 or (step + 1 == len(training_loader)): # makes sure that data from the last batches are not discarded\n#                 torch.nn.utils.clip_grad_norm_(update_params, max_norm=1.0)\n# #                 optimizer.step()\n# #                 optimizer.zero_grad()\n# #                 xm.mark_step()\n#                 xm.optimizer_step(optimizer)\n#                 scheduler.step()\n# #                 global_step += 1\n                \n        \n        \n            \n\n        \n        \n        \n        \n        ### Validation - Test\n        model.eval()\n        total_loss = 0.0\n        total_steps = 0\n        with torch.no_grad():\n            for step, batch in enumerate(testing_loader):\n                input_ids = batch[\"input_ids\"].to(device)\n                labels = batch[\"labels\"].to(device)\n                xs.mark_sharding(input_ids, mesh, (0, 1))\n                xs.mark_sharding(labels, mesh, (0, 1))\n                outputs = model(input_ids=input_ids, labels=labels)\n                loss = outputs.loss\n                total_loss += loss.item()\n                total_steps += 1\n\n        average_loss = total_loss / total_steps\n        xm.master_print('----- Time -> {} ----- Validation Steps -> {} ----  Validation Loss -> {:.4f}'.format(test_utils.now(), total_steps , average_loss))\n#         ","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:17:32.389098Z","iopub.execute_input":"2024-02-14T13:17:32.389546Z","iopub.status.idle":"2024-02-14T13:17:33.696624Z","shell.execute_reply.started":"2024-02-14T13:17:32.389508Z","shell.execute_reply":"2024-02-14T13:17:33.695187Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"train(FLAGS)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T13:29:22.812881Z","iopub.execute_input":"2024-02-14T13:29:22.813758Z","iopub.status.idle":"2024-02-14T13:36:35.057083Z","shell.execute_reply.started":"2024-02-14T13:29:22.813723Z","shell.execute_reply":"2024-02-14T13:36:35.056242Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Epoch 1 train begin 13:29:22\nloss: 1.1611374616622925, time: 13:30:13, step: 1\nloss: 0.9305022358894348, time: 13:32:38, step: 2\nloss: 1.1327972412109375, time: 13:35:04, step: 3\nloss: 0.8886520862579346, time: 13:35:08, step: 4\nloss: 1.029287338256836, time: 13:35:12, step: 5\nloss: 0.8203051686286926, time: 13:35:16, step: 6\nloss: 0.869084894657135, time: 13:35:20, step: 7\nloss: 0.7779562473297119, time: 13:35:24, step: 8\nloss: 0.6987584829330444, time: 13:35:28, step: 9\nloss: 0.6810611486434937, time: 13:35:32, step: 10\nloss: 0.670748770236969, time: 13:35:36, step: 11\nloss: 0.5769106149673462, time: 13:35:40, step: 12\nloss: 0.537781298160553, time: 13:35:44, step: 13\nloss: 0.48798009753227234, time: 13:35:48, step: 14\nloss: 0.4467232823371887, time: 13:35:52, step: 15\nloss: 0.4357216954231262, time: 13:35:56, step: 16\nloss: 0.37424397468566895, time: 13:35:59, step: 17\nloss: 0.3310996890068054, time: 13:36:03, step: 18\nloss: 0.3002502918243408, time: 13:36:07, step: 19\nloss: 0.26418328285217285, time: 13:36:11, step: 20\nloss: 0.23024801909923553, time: 13:36:15, step: 21\nloss: 0.19637088477611542, time: 13:36:19, step: 22\nloss: 0.17096219956874847, time: 13:36:23, step: 23\nloss: 0.14054463803768158, time: 13:36:27, step: 24\nloss: 0.11318252980709076, time: 13:36:31, step: 25\nEpoch 1 train end 13:36:32\n----- Time -> 13:36:35 ----- Validation Steps -> 1 ----  Validation Loss -> 0.3989\n","output_type":"stream"}]},{"cell_type":"code","source":"def log_wandb(FLAGS,loss,steps,batch_step):\n    wandb.log({\"Train Loss\": loss.item(), \"Global Step\": global_step})\n    wandb.log({\"Epoch Number\": epoch, \"Time End Epoch\": test_utils.now()})\n    \n    return \"Humpe toh hai hi naw\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Interference \n\n# text=\"\"\"\n\n# \"\"\"\n\n# input_ids = tokenizer(text, add_special_tokens=True, max_length=MAX_INPUT, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n# ids=(input_ids[\"input_ids\"].squeeze(0)).to(device)\nprompt_2 = \"\"\"\n<im_start>user\nHow to fine Tune Mistral 7B Using pytorch lightning\n<im_end>\n<im_start>assistant\"\"\"\ninputs = tokenizer([prompt_2],add_special_tokens=True, max_length=MAX_INPUT, truncation=True, return_tensors=\"pt\").to(device)\nids=(inputs[\"input_ids\"].squeeze(0)).to(device)\nmask=(inputs[\"attention_mask\"].squeeze(0)).to(device)\nxs.mark_sharding(ids, mesh, (0,))\nxs.mark_sharding(mask,mesh,(0,))\n# print(inputs)\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(ids, streamer = text_streamer, max_new_tokens = 9000)\n\n# xs.mark_sharding(ids, mesh, (0, 1))\n# outputs = model(input_ids=ids)\n# tokenizer.decode(outputs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.cpu()\nprint('now saving the model')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"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\")\nmodel.push_to_hub(\n    SAVED_MODEL, \n    tokenizer=tokenizer,\n    safe_serialization=True,\n    private=True,\n    create_pr=True,\n    max_shard_size=\"3GB\", # Sharding isn't as important as before since hardware is better now but who cares anyway\n    )# We have to push the model to HF since there is not enough memory on disk. Download weights from there\ntokenizer.push_to_hub(\n    SAVED_MODEL,\n    private=True, \n    \n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
